\documentclass[10pt,openany]{book}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

%\input{c:/aaaWork/zGnrlLatex/BookPreamble_HC}   % use for the hard-copy version
\input{c:/aaaWork/zGnrlLatex/BookPreamble}
\hypersetup{pdftitle = MTH107 Notes,bookmarksdepth=0}
\input{c:/aaaWork/zGnrlLatex/JustRPreamble}
\usepackage{animate}
\usepackage{titlesec}
\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsection{-3pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{-3pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\renewcommand{\chaptername}{Module}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}




  \frontmatter
    \include{FBMatter/FMatter}

  \mainmatter



\chapter{Why Statistics is Important} \label{chap:WhyStatsImportant}

\minitoc

\section{Realities}\label{sect:Realities}
\lettrine{T}{he city of Ashland} performed an investigation in the area of Kreher Park \figrefp{fig:KreherParkMap} when considering the possible expansion of an existing wastewater treatment facility in 1989. The discovery of contamination from creosote waste in the subsoils and ground water at Kreher Park prompted the city to abandon the project. A subsequent assessment by the Wisconsin Department of Natural Resources (WDNR) indicated elevated levels of hazardous substances in soil borings, ground water samples, and in the sediments of Chequamegon Bay directly offshore of Kreher Park. In 1995 and 1999, the Northern States Power Company conducted investigations that further defined the area of contamination and confirmed the presence of specific contaminants associated with coal tar wastes. This site is now listed as a superfund site and is being given considerably more attention.\footnote{More information at the \href{https://cumulis.epa.gov/supercpad/cursites/csitinfo.cfm?id=0507952}{EPA} and the \href{http://dnr.wi.gov/topic/brownfields/ashland.html}{WDNR} websites.}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=6in]{Figs/Kreher_Park_Map.png}
  \caption{Location of the Ashland superfund site (left) with the location of 119 historical sediment sampling sites (right).}
  \label{fig:KreherParkMap}
\end{figure}

The WDNR wants to study elements in the sediment (among other things) in the entire 3000 m$^2$ area shaded in \figref{fig:KreherParkMap}. Is it physically possible to examine every square meter of that area?  Is it prudent, ecologically and economically, to examine every square meter of this area?  The answer, of course, is ``no.''  How then will the WDNR be able to make conclusions about this entire area if they cannot reasonably examine the whole area?  The most reasonable solution is to sample a subset of the area and use the results from this sample to make inferences about the entire area.

Methods for properly selecting a sample that fairly represents a larger collection of individuals are an important area of study in statistics. For example, the WDNR would not want to sample areas that are only conveniently near shore because this will likely not be an accurate representation of the entire area. In this example, it appears that the WDNR used a grid to assure a relatively even dispersal of samples throughout the study area \figrefp{fig:KreherParkMap}. Methods for choosing the number of individuals to select and how to select those individuals are discussed in \modref{chap:DataProd}.


Suppose that the WDNR measured the concentration of lead at each of the 119 locations shown in \figref{fig:KreherParkMap}. Further suppose that they presented their results at a public meeting by simply showing the list of lead concentration measurements \tabrefp{tab:KreherParkPbconc}.\footnote{These are hypothetical data for this site.}  Is it easy to make conclusions about what these data mean from this type of presentation?

\begin{table}[htbp]   \label{tab:KreherParkPbconc}
  \caption{Lead concentration ($\mu g \cdot m^{-3}$) from 119 sites in Kreher Park superfund site.}
  \begin{center}
% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:25 2017
\begin{tabular}{rrrrrrrrrrrrrrr}
  \hline
  \hline
0.91 & 1.09 & 1.00 & 1.09 & 1.06 & 0.98 & 0.98 & 0.94 & 0.89 & 1.09 & 0.91 & 1.06 & 0.81 & 0.90 & 1.21 \\ 
  1.03 & 0.95 & 1.14 & 0.99 & 0.99 & 0.96 & 1.13 & 0.84 & 1.03 & 0.86 & 0.98 & 1.04 & 0.91 & 1.27 & 0.90 \\ 
  0.87 & 1.23 & 1.12 & 0.98 & 0.79 & 1.10 & 1.06 & 1.09 & 0.73 & 0.81 & 1.18 & 0.92 & 0.82 & 1.11 & 0.97 \\ 
  1.24 & 1.06 & 1.09 & 0.78 & 0.94 & 1.08 & 0.91 & 0.98 & 1.22 & 1.04 & 0.77 & 1.18 & 0.93 & 1.14 & 0.94 \\ 
  1.05 & 0.91 & 1.14 & 0.93 & 0.94 & 0.90 & 1.05 & 1.36 & 1.02 & 0.93 & 1.09 & 1.17 & 0.91 & 1.06 & 0.95 \\ 
  0.88 & 0.67 & 1.12 & 1.06 & 0.99 & 0.89 & 0.83 & 0.99 & 1.33 & 1.00 & 1.05 & 1.11 & 1.01 & 1.25 & 0.96 \\ 
  1.07 & 1.17 & 1.01 & 1.20 & 1.17 & 1.05 & 1.21 & 1.10 & 1.07 & 1.01 & 1.16 & 1.24 & 0.86 & 0.90 & 1.07 \\ 
  1.11 & 0.99 & 0.70 & 0.98 & 1.11 & 1.12 & 1.30 & 1.00 & 0.89 & 0.91 & 0.95 & 1.08 & 1.02 & 0.93 &  \\ 
   \hline
\end{tabular}

  \end{center}
\end{table}

\vspace{-12pt}
Instead, suppose that the scientists brought a simple plot of the frequency of observed lead concentrations and brief numerical summaries \figrefp{fig:KreherParkPbhist} to the meeting. With these one can easily see that the measurements were fairly symmetric with no obviously ``weird'' values. The lead concentrations ranged from as low as 0.67 $\mu g \cdot m^{-3}$ to as high as 1.36 $\mu g \cdot m^{-3}$ with the measurements centered on approximately 1.0 $\mu g \cdot m^{-3}$. These summaries are discussed in detail in \modref{chap:UnivEDAQuant}. However, at this point, note that summarizing large quantities of data with few graphical or numerical summaries makes it is easier to identify meaning from data.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/KreherParkPbhist-1} 
\includegraphics[width=.4\linewidth]{Figs/KreherParkPbhist-2} 

}

\caption[Histogram and summary statistics of lead concentration measurements ($\mu g \cdot m^{-3}$) at each of 119 sites in Kreher Park superfund site]{Histogram and summary statistics of lead concentration measurements ($\mu g \cdot m^{-3}$) at each of 119 sites in Kreher Park superfund site.}\label{fig:KreherParkPbhist}
\end{figure}


\end{knitrout}

A critical question at this point is whether or not the results from the one sample of 119 sites perfectly represents the results for the entire area. One way to consider this question is to examine the results obtained from another sample of 119 sites. The results from this second sample \figrefp{fig:KreherParkPbhist1} are clearly, though not radically, different from the results of the first sample. Thus, it is seen that any one sample from a larger whole will not perfectly represent the large whole. This will lead to some uncertainty in our summaries of the larger whole.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/KreherParkPbhist1-1} 
\includegraphics[width=.4\linewidth]{Figs/KreherParkPbhist1-2} 

}

\caption[Kreher Park 1]{Histogram and summary statistics of lead concentration measurements ($\mu g \cdot m^{-3}$) at each of 119 sites (different from the sites shown in \figref{fig:KreherParkPbhist}) in Kreher Park superfund site.}\label{fig:KreherParkPbhist1}
\end{figure}


\end{knitrout}

The results from two different samples do not perfectly agree because each sample contains different individuals (sites in this example), and no two individuals are exactly alike. The fact that no two individuals are exactly alike is \textbf{natural variability}\index{Natural Variability!Definition}, because of the ``natural'' differences that occur among individuals. The fact that the results from different samples are different is called \textbf{sampling variability}\index{Sampling Variability!Definition}. If there was no natural variability, then there would be no sampling variability. If there was no sampling variability, then the field of statistics would not be needed because a sample (even of one individual) would perfectly represent the larger group of individuals. Thus, understanding variability is at the core of statistical practice. Natural and sampling variability will be revisited continuously throughout this course.

This may be unsettling! First, it was shown that an entire area or all of the individuals of interest cannot be examined. It was then shown that a sample of individuals from the larger whole did not perfectly represent the larger whole. Furthermore, each sample is unique and will likely lead to a (slightly) different conclusion. These are all real and difficult issues faced by the practicing scientist and considered by the informed consumer. However, the field of statistics is designed to ``deal with'' these issues such that the results from a relatively small subset of measurements can be used to make conclusions about the entire collection of measurements.

\warn{Statistics provides methods for overcoming the difficulties caused by the requirement of sampling and the presence of sampling variability.}


\section{Major Goals of Statistics}
As seen in the Kreher Park example, the field of statistics has two primary purposes. First, statistics provides methods to summarize large quantities of data into concise and informative numerical or graphical summaries. For example, it was easier to discern the general underlying structure of the lead measurements from the statistics and histograms presented in Figures \ref{fig:KreherParkPbhist} and \ref{fig:KreherParkPbhist1}, than it was from the full list of lead measurements in \tabref{tab:KreherParkPbconc}. Second, statistical methods allow inferences to be made about all individuals (i.e., a population) from a few individuals (i.e., a sample).\footnote{Population and sample are defined more completely in \sectref{sect:IVPPSS}.}


\section{Definition of Statistics}
Statistics is the science of collecting, organizing, and interpreting numerical information or data \citep{MooreMcCabe1998}\index{Statistics, Field of!Definition}. People study statistics for a variety of reasons, including \citep{Bluman2000}:
\begin{Enumerate}
  \item To understand the statistical studies performed in their field (i.e., be knowledgeable about the vocabulary, symbols, concepts, and statistical procedures used in those studies).
  \item To conduct research in their field (i.e., be able to design experiments and samples; collect, organize, analyze, and summarize data; make reliable predictions or forecasts for future use; and communicate statistical results).
  \item To be better consumers of statistical information.
\end{Enumerate}

Statistics permeates a wide variety of disciplines. \cite{MooreMcCabe1998} state:
\begin{quote}
The study and collection of data are important in the work of many professions, so that training in the science of statistics is valuable preparation for a variety of careers. Each month, for example, government statistical offices release the latest numerical information on unemployment and inflation. Economists and financial advisers, as well as policy makers in government and business study these data in order to make informed decisions. Doctors must understand the origin and trustworthiness of the data that appear in medical journals if they are to offer their patients the most effective treatments. Politicians rely on data from polls of public opinion. Business decisions are based on market research data that reveal customer tastes. Farmers study data from field trials of new crop varieties. Engineers gather data on the quality and reliability of manufactured products. Most areas of academic study make use of numbers, and therefore also make use of the methods of statistics.
\end{quote}



\chapter{Foundational Definitions} \label{chap:FoundationalDefinitions}

\minitoc
\vspace{18pt}

\lettrine{S}{tatistical inference is the process} of forming conclusions about a parameter of a population from statistics computed from individuals in a sample.\index{Inference!Definition}\footnote{Formal methods of inference are discussed beginning with \modref{chap:ProbIntro}.} Thus, understanding statistical inference requires understanding the difference between a population and a sample and a parameter and a statistic. And, to properly describe those items, the individual and variable(s) of interest must be identified. Understanding and identifying these six items is the focus of this module.

The following hypothetical example is used throughout this module. Assume that we are interested in the average length of 1015 fish in Square Lake. To illustrate important concepts in this module, assume that all information for all 1015 fish in this lake is known \figrefp{fig:SquareLakePopn}. In ``real life'' this complete information would not be known.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/SquareLakePopn-1} 
\includegraphics[width=.4\linewidth]{Figs/SquareLakePopn-2} 

}

\caption[Schematic representation of individual fish (i.e., dots]{Schematic representation of individual fish (i.e., dots; \textbf{Left}) and histogram (\textbf{Right}) of the total length of the 1015 fish in Square Lake.}\label{fig:SquareLakePopn}
\end{figure}


\end{knitrout}

\section{Definitions} \label{sect:IVPPSS}
The \textbf{individual} in a statistical analysis is one of the ``items'' examined by the researcher.\index{Individual, Definition}  Sometimes the individual is a person, but it may be an animal, a piece of wood, a location, a particular time, or an event. It is extremely important that you don't always visualize a person when considering an individual in a statistical sense. Synonyms for individual are unit, experimental unit (usually used in experiments), sampling unit (usually used in observational studies), case, and subject (usually used in studies involving humans). An individual in the Square Lake example is a fish, because the researcher will collect a set of fish and examine each individual fish.

The \textbf{variable} is the characteristic recorded about each individual.\index{Variable!Definition} The variable in the Square Lake example is the length of each fish. In most studies, the researcher will record more than one variable. For example, the researcher may also record the fish's weight, sex, age, time of capture, and location of capture. In this module, only one variable is considered. In other modules, two variables will be considered.

A \textbf{population} is ALL individuals of interest.\index{Population} In the Square Lake example, the population is all 1015 fish in the lake. The population should be defined as thoroughly as possible including qualifiers, especially those related to time and space, as necessary. This example is simple because Square Lake is so well defined; however, as you will see in the review exercises, the population is often only well-defined by your choice of descriptors.

A \textbf{parameter} is a summary computed from ALL individuals in a population.\index{Parameter!Definition}  The term for the particular summary is usually preceded by the word ``population.'' For example, the population average length of all 1015 fish in Square Lake is 98.06 mm and the population standard deviation is 31.49 mm \tabrefp{tab:SquareLakePopn}.\footnote{We will discuss how to compute and interpret each of these values in later modules.} Parameters are ultimately what is of interest, because interest is in all individuals in the population. However, in practice, parameters cannot be computed because the entire population cannot usually be ``seen.''
% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:26 2017
\begin{table}[ht]
\centering
\caption{Parameters for the total length of ALL 1015 fish in the Square Lake population.} 
\label{tab:SquareLakePopn}
\begin{tabular}{cccccccc}
 n & mean & sd & min & Q1 & median & Q3 & max \\ 
  \hline
1015 & 98.06 & 31.49 & 39 & 72 & 93 & 117 & 203 \\ 
   \hline
\end{tabular}
\end{table}


The entire population cannot be ``seen'' in real life. Thus, to learn something about the population, a subset of the population is usually examined. This subset is called a \textbf{sample}.\index{Sample!Definition} The red dots in \figref{fig:SquareLakeSample1} represent a random sample of n=50 fish from Square Lake (note that the sample size is usually denoted by n).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/SquareLakeSample1-1} 
\includegraphics[width=.4\linewidth]{Figs/SquareLakeSample1-2} 

}

\caption{Schematic representation (\textbf{Left}) of a sample of 50 fish (i.e., red dots) from Square Lake and histogram (\textbf{Right}) of the total length of the 50 fish in this sample.}\label{fig:SquareLakeSample1}
\end{figure}


\end{knitrout}

Summaries computed from individuals in a sample are called \textbf{statistics}.\index{Statistic!Definition}  Specific names of statistics are preceded by ``sample.''  The statistic of interest is always the same as the parameter of interest; i.e., the statistic describes the sample in the same way that the parameter describes the population. For example, if interest is in the population mean, then the sample mean would be computed.

Some statistics computed from the sample from Square Lake are shown in \tabref{tab:SquareLakeSample1} and \figref{fig:SquareLakeSample1}. The sample mean of 100.04 mm is the best ``guess'' at the population mean. Not surprisingly from the discussion in \modref{chap:WhyStatsImportant}, the sample mean does not perfectly equal the population mean.

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:26 2017
\begin{table}[ht]
\centering
\caption{Summary statistics for the total length of a sample of 50 fish from the Square Lake population.} 
\label{tab:SquareLakeSample1}
\begin{tabular}{cccccccc}
 n & mean & sd & min & Q1 & median & Q3 & max \\ 
  \hline
50 & 100.04 & 31.94 & 49 & 81 & 91 & 118 & 203 \\ 
   \hline
\end{tabular}
\end{table}


\warn{An individual is not necessarily a person.}

\vspace{-12pt}
\warn{Populations and parameters can generally not be ``seen.''}


\section{Performing an IVPPSS}
In each statistical analysis it is important that you determine the Individual, Variable, Population, Parameter, Sample, and Statistic (\textbf{IVPPSS}). First, determine what items you are actually going to look at; those are the individuals. Second, determine what is recorded about each individual; that is the variable. Third, ALL individuals is the population. Fourth, the summary (e.g., mean or proportion) of the variable recorded from ALL individuals in the population is the parameter.\footnote{Again, parameters generally cannot be computed because all of the individuals in the population can not be seen. Thus, the parameter is largely conceptual.} Fifth, the population usually cannot be seen, so only a few individuals are examined; those few individuals are the sample. Finally, the summary of the individuals in the sample is the statistic.

When performing an IVPPSS, keep in mind that parameters describe populations (note that they both start with ``p'') and statistics describe samples (note that they both start with ``s''). This can also be looked at from another perspective. A sample is an estimate of the population and a statistic is an estimate of a parameter. Thus, the statistic has to be the same summary (mean or proportion) of the sample as the parameter is of the population.

The IVPPSS process is illustrated for the following situation:
\vspace{-6pt}
\begin{quote}
\textit{A University of New Hampshire graduate student (and Northland College alum) investigated habitat utilization by New England (Sylvilagus transitionalis) and Eastern (Sylvilagus floridanus) cottontail rabbits in eastern Maine in 2007. In a preliminary portion of his research he determined the proportion of ``rabbit patches'' that were inhabited by New England cottontails. He examined 70 ``patches'' and found that 53 showed evidence of inhabitance by New England cottontails.}
\end{quote}
\vspace{-6pt}

\begin{Itemize}
  \item An individual is a rabbit patch in eastern Maine in 2007 (i.e., a rabbit patch is the ``item'' being sampled and examined).
  \item The variable is ``evidence for New England cottontails or not (yes or no)'' (i.e., the characteristic of each rabbit patch that was recorded).
  \item The population is ALL rabbit patches in eastern Maine in 2007.
  \item The parameter is the proportion of ALL rabbit patches in eastern Maine in 2007 that showed evidence for New England cottontails.\footnote{Note that this population and parameter cannot actually be calculated but it is what the researcher wants to know.}
  \item The sample is the 70 rabbit patches from eastern Maine in 2007 that were actually examined by the researcher.
  \item The statistic is the proportion of the 70 rabbit patches from eastern Maine in 2007 actually examined that showed evidence for New England cottontails. [In this case, the statistic would be 53/70 or 0.757.]
\end{Itemize}

In the descriptions above, take note that the individual is very carefully defined (including stating a specific time (2007) and place (eastern Maine)), the population and parameter both use the word ``ALL'', the sample and statistic both use the specific sample size (70 rabbits), and that the parameter and statistics both use the same summary (i.e., proportion of patches that showed evidence of New England cottontails).

In some situations it may be easier to identify the sample first. From this, and realizing that a sample is always ``of the individuals,'' it may be easier to identify the individual. This process is illustrated in the following example, with the items listed in the order identified rather than in the traditional IVPPSS order.

\vspace{-6pt}
\begin{quote}
\textit{The Duluth, MN Touristry Board is interested in the average number of raptors seen per year at Hawk Ridge.\footnote{Information about Hawk Ridge is found \href{http://www.hawkridge.org/}{here}.}  To determine this value, they collected the total number of raptors seen in a sample of years from 1971-2003.}
\end{quote}
\vspace{-6pt}

\begin{Itemize}
  \item The sample is the 32 years between 1971 and 2003 at Hawk Ridge.
  \item An individual is a year (because a ``sample of \emph{years}'' was taken) at Hawk Ridge.
  \item The variable recorded was the number of raptors seen in one year at Hawk Ridge.
  \item The population is ALL years at Hawk Ridge (this is a bit ambiguous but may be thought of as all years that Hawk Ridge has existed).
  \item The parameter is the average number of raptors seen per year in ALL years at Hawk Ridge.
  \item The statistic is the average number of raptors seen in the 1971-2003 sample of years at Hawk Ridge.
\end{Itemize}

Again, note that the individual is very carefully defined (including stating a specific time and place), the population and parameter both use the word ``ALL'', the sample and statistic both use the specific sample size (32 years), and that the parameter and statistics both use the same summary (i.e., average number of raptors).

\warn{An individual is usually defined by a specific time and place.}

\vspace{-12pt}
\warn{Descriptions for population and parameter will always include the word ``All.''}

\vspace{-12pt}
\warn{Descriptions for sample and statistic will contain the specific sample size.}

\vspace{-12pt}
\warn{Descriptions for parameter and statistic will contain the same summary (usually average/mean or proportion/percentage). Howeve the summary is for a different set of individuals -- the population for the parameter and the sample for the statistic.}


\subsection{Sampling Variability (Revisited)}
It is instructive to once again (see \modref{chap:WhyStatsImportant}) consider how statistics differ among samples. \tabref{tab:SquareLakeSample234} and \figref{fig:SquareLakeSample234} show results from three more samples of n=50 fish from the Square Lake population. The means from all four samples (including the sample in \tabref{tab:SquareLakeSample1} and \figref{fig:SquareLakeSample1}) were quite different from the known population mean of 98.06 mm. Similarly, all four histograms were similar in appearance but slightly different in actual values. These results illustrate that a statistic (or sample) will only approximate the parameter (or population) and that statistics vary among samples. This \textbf{sampling variability} is one of the most important concepts in statistics and is discussed in great detail beginning in \modref{chap:SamplingDist}.\index{Sampling Variability!Definition}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.7\linewidth]{Figs/SquareLakeSample234-1} 

}

\caption{Schematic representation (\textbf{Left}) of three samples of 50 fish (i.e., red dots) from Square Lake and histograms (\textbf{Right}) of the total length of the 50 fish in each sample.}\label{fig:SquareLakeSample234}
\end{figure}


\end{knitrout}

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:26 2017
\begin{table}[htbp]
\centering
\caption{Summary statistics for the total length in three samples of 50 fish from the Square Lake population.} 
\label{tab:SquareLakeSample234}
\begin{tabular}{cccccccc}
 n & mean & sd & min & Q1 & median & Q3 & max \\ 
  \hline
50 & 99.56 & 32.47 & 57 & 69 & 91 & 123 & 167 \\ 
  50 & 88.64 & 24.52 & 53 & 68 & 86 & 106 & 166 \\ 
  50 & 112.74 & 35.86 & 61 & 84 & 108 & 147 & 174 \\ 
   \hline
\end{tabular}
\end{table}


%\defn{Sampling Variability}{The realization that no two samples are exactly alike. Thus, statistics computed from different samples will likely vary.}

This example also illustrates that parameters are fixed values because populations don't change. If a population does change, then it is considered a different population. In the Square Lake example, if a fish is removed from the lake, then the fish in the lake would be considered a different population. Statistics, on the other hand, vary depending on the sample because each sample consists of different individuals that vary (i.e., sampling variability exists because natural variability exists).

\warn{Parameters are fixed in value, while statistics vary in value.}


\clearpage
\section{Variable Types}\label{sect:VarTypes}
The type of statistic that can be calculated is dictated by the type of variable recorded. For example, an average can only be calculated for quantitative variables (defined below). Thus, the type of variable should be identified immediately after performing an IVPPSS.

\subsection{Variable Definitions}
There are two main groups of variable types -- quantitative and categorical \figrefp{fig:VarTypes}. \textbf{Quantitative} variables are variables with numerical values for which it makes sense to do arithmetic operations (like adding or averaging).\index{Quantitative Variable}  Synonyms for quantitative are measurement or numerical. \textbf{Categorical} variables are variables that record which group or category an individual belongs.\index{Categorical Variable}  Synonyms for categorical are qualitative or attribute. Within each main type of variable are two subgroups \figrefp{fig:VarTypes}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.5\linewidth]{Figs/VarTypes-1} 

}

\caption[Schematic representation of the four types of variables]{Schematic representation of the four types of variables.}\label{fig:VarTypes}
\end{figure}


\end{knitrout}
\vspace{9pt} % added because of paragraph compressions following R code

The two types of quantitative variables are continuous and discrete variables. \textbf{Continuous} variables are quantitative variables that have an uncountable number of values.\index{Continuous Variable}  In other words, a potential value \textsc{does} exist between every pair of values of a continuous variable. \textbf{Discrete} variables are quantitative variables that have a countable number of values.\index{Discrete Variable}  Stated differently, a potential value \textsc{does not} exist between every pair of values for a discrete variable. Typically, but not always, discrete variables are counts of items.

Continuous and discrete variables are easily distinguished by determining if it is possible for a value to exist between every two values of the variable. For example, can there be between 2 and 3 ducks on a pond?  No!  Thus, the number of ducks is a discrete variable. Alternatively, can a duck weigh between 2 and 3 kg?  Yes!  Can it weigh between 2 and 2.1 kg?  Yes!  Can it weigh between 2 and 2.01 kg?  Yes!  You can see that this line of questions could continue forever; thus, duck weight is a continuous variable.

\warn{A quantitative variable is continuous if a possible value exists between every two values of the variable; otherwise, it is discrete.}

The two types of categorical variables are ordinal and nominal. \textbf{Ordinal} variables are categorical variables where a natural order or ranking exists among the categories.\index{Ordinal Variable}  \textbf{Nominal} variables are categorical variables where no order or ranking exists among the categories.\index{Nominal Variable}

Ordinal and nominal variables are easily distinguished by determining if the order of the categories matters. For example, suppose that a researcher recorded a subjective measure of condition (i.e., poor, average, excellent) and the species of each duck. Order matters with the condition variable -- i.e., condition improves from the first (poor) to the last category (excellent) -- and some reorderings of the categories would not make sense -- i.e., average, poor, excellent does not make sense. Thus, condition is an ordinal variable. In contrast, species (e.g., mallard, redhead, canvasback, and wood duck) is a nominal variable because there is no inherent order among the categories (i.e., any reordering of the categories also ``makes sense'').

\warn{\textbf{Ord}inal means that an \textbf{ord}er among the categories exists (note ``ord'' in both ordinal and order).}

The following are some issues to consider when identifying the type of a variable:
\begin{Enumerate}
  \item The categories of a categorical variable are sometimes labeled with numbers. For example, 1=``Poor'', 3=``Fair'', and 5=``Good''. Don't let this fool you into calling the variable quantitative.
  \item Rankings, ratings, and preferences are ordinal (categorical) variables.
  \item Counts of numbers are discrete (quantitative) variables.
  \item Measurements are typically continuous (quantitative) variables.
  \item It does not matter how precisely quantiative variables are recorded when deciding if the variable is continuous or discrete. For example, the weight of the duck might have been recorded to the nearest kg. However, this was just a choice that was made, the actual values can be continuously finer than kg and, thus, weight is a continuous variable.
  \item Categorical variables that consist of only two levels or categories will be labeled as a nominal variable (because any order of the groups makes sense). This type of variable is also often called ``binomial.''
  \item Do not confuse ``what type of variable'' (answer is one of ``continuous'', ``discrete'', ``nominal'', or ``ordinal'')  with ``what type of variability'' (answer is ``natural'' or ``sampling'') questions.
\end{Enumerate}

\warn{``What type of variable is ...?'' is a different question than ``what type of variability is ...?''  Be careful to note the word difference (i.e., ``variable'' versus ``variability'') when answering these questions.}

\vspace{-12pt}
\warn{The precision to which a quantitative variable was recorded does not determine whether it is continuous or discrete. How precisely the variable COULD have been recorded is the important consideration.}



\chapter{Data Production} \label{chap:DataProd}

\minitoc
\vspace{24pt}

\lettrine{S}{tatistical inference is the process} of making conclusions about a population from the results of a single sample.\index{Inference!Definition} To make conclusions about the larger population, the sample must fairly represent the larger population. Thus, the proper collection (or production) of data is critical to statistics (and science in general). In this module, two ways of producing data -- (1) Experiments and (2) Observational Studies -- are described.

\warn{Inferences cannot be made if data are not properly collected.}


\section{Experiments}
An experiment deliberately imposes a \textit{condition} on individuals to observe the effect on the \textbf{response variable}.\index{Experiment!Definition}\index{Response Variable!Experiment} In a properly designed experiment, all variables that are not of interest are held constant, whereas the variable(s) that is (are) of interest are changed among treatments. As long as the experiment is designed properly (see below), differences among treatments are either due to the variable(s) that were deliberately changed or randomness (chance). Methods to determine if differences were likely due to randomness are developed in later modules. Because we can determine if differences most likely occurred o randomness or changes in the variales, strong \textit{cause-and-effect conclusions} can be made from data collected from carefully designed experiments.

\subsection{Single-factor Experiments}\index{Experiment!Single-Factor}
A \textbf{factor} is a variable that is deliberately manipulated to determine its effect on the response variable.\index{Factor!Experiment} A factor is sometimes called an \textbf{explanatory variable} because we are attempting to determine how it affects (or ``explains'') the response variable. The simplest experiment is a single-factor experiment where the individuals are split into groups defined by the categories of a single factor.

For example, suppose that a researcher wants to examine the effect of temperature on the total number of bacterial cells after two weeks. They have inoculated 120 agars\footnote{An agar, in this case, is a petri dish with a growth medium for the bacteria.} with the bacteria and placed them in a chamber where all environmental conditions (e.g., temperature, humidity, light) are controlled exactly. The researchers will use only two temperatures in this simple experiment -- $10^{o}$C and $15^{o}$C. All other variables are maintained at constant levels. Thus, temperature is the only factor in this simple experiement because it is the only variable manipulated to different values to determine its impact on the number of bacterial cells.

\warn{In a single-factor experiment only one explanatory variable (i.e., factor) is allowed to vary; all other explanatory variables are held constant.}

\textbf{Levels} are the number of categories of the factor variable.\index{Level!Experimental} In this example, there are two levels -- $10^{o}$C and $15^{o}$C. \textbf{Treatments} are the number of unique conditions that individuals in the experiment are exposed to. In a single-factor experiment, the number of treatments is the same as the number of levels of the single factor.\index{Treatment, Experimental} Thus, in this simple experiment, there are two treatments -- $10^{o}$C and $15^{o}$C. Treatments are discussed more thoroughly in the next section.

The \textbf{number of replicates} in an experiment is the number of individuals that will receive each treatment.\index{Replicates} In this example, a replicate is an inoculated agar. The number of replicates is the number of inoculated agars that will receive each of the two temperature treatments. The number of replicates is determined by dividing the total number of available individuals (120) by the number of treatments (2). Thus, in this example, the number of replicates is 60 inoculated agars.

The agars used in this experiment will be randomly allocated to the two temperature treatments. All other variables -- humidity, light, etc. -- are kept the same for each treatment. At the end of two weeks, the total number of bacterial cells on each agar (i.e., the response variable) will be recorded and compared between the agars kept at both temperatures.\footnote{Methods for making this comparison are in \modref{chap:tTest2}.} Any difference in mean number of bacterial cells will be due to either different temperature treatments or randomness, because all other variables were the same between the two treatments.

\warn{Differences among treatments are either caused by randomness (chance) or the factor.}

The single factor is not restricted to just two levels. For example, more than two temperatures, say $10^{o}$C, $12.5^{o}$C, $15^{o}$C, and $17.5^{o}$C, could have been tested. With this modification, there is still only one factor -- temperature -- but there are now four levels (and only four treatments).

\subsection{Multi-factor Experiments -- Design and Definitions}
More than one factor can be tested in an experiment.\index{Experiment!Multi-Factor} In fact, it is more efficient to have a properly designed experiment where more than one factor is varied at a time than it is to use separate experiments in which only one factor is varied in each. However, before showing this benefit, let's examine the definitions from the previous section in a multi-factor experiment.

Suppose that the previous experiment was modified to also examine the effect of relative humidity on the number of bacteria cells. This modified experiment has two factors -- temperature (with two levels of $10^{o}$C or $15^{o}$C) and relative humidity (with four levels of 20\%, 40\%, 60\%, and 80\%). The number of treatments, or combinations of all factors, in this experiment is found by multiplying the levels of all factors (i.e., 2$\times$4=8 in this case).\index{Treatment, Experimental} The number of replicates in this experiment is now 15 (i.e., total number of available agars divided by the number of treatments; 120/8).

\warn{The number of treatments is determined for the overall experiment, whereas the number of levels is determined for each factor.}

A drawing of the experimental design can be instructive (below). The drawing is a grid where the levels of one factor are the rows and the levels of the other factor are the columns. The number of rows and columns correspond to the levels of the two factors, respectively, whereas the number of cells in the grid is the number of treatments (numbered in this table to show eight treatments).

\begin{center}
\begin{tabular}{cc|c|c|c}
 & \multicolumn{4}{c}{Relative Humidity} \\
\cline{2-5}
 & 20\% & 40\% & 60\% & 80\% \\
\cline{2-5}
\multicolumn{1}{c|}{$10^{o}$C} & 1 & 2 & 3 & \multicolumn{1}{c|}{4} \\
\hline
\multicolumn{1}{c|}{$15^{o}$C} & 5 & 6 & 7 & \multicolumn{1}{c|}{8} \\
\cline{2-5}
\end{tabular}
\end{center}


\subsection{Multi-factor Experiments -- Benefits}
The analysis of a multi-factor experimental design is more involved than what will be shown in this course. However, multi-factor experiments have many benefits, which can be illustrated by comparing a multi-factor experiment to separate single-factor experiments. For example, in addition to the two factor experiment in the previous section, consider separate single-factor experiments to determine the effect of each factor separately (further assume that individuals (i.e., agars) can be used in only one of these separate experiments).

To conduct the two separate experiments, randomly split the 120 available agars into two equally-sized groups of 60. The first 60 will be split into two groups of 30 for the first experiment with two temperatures. The second 60 will be split into four groups of 15 for the second experiment with four relative humidities. These separate single-factor experiments are summarized in the following tables (where the numbers in the cells represent the number of replicates in each treatment).

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\multicolumn{2}{c}{Temperature} & \multicolumn{1}{c}{} & \multicolumn{4}{c}{Relative Humidity} \\
\cline{1-2}\cline{4-7}
$10^{o}$C & $15^{o}$C & & 20\% & 40\% & 60\% & 80\% \\
\cline{1-2}\cline{4-7}
30 & 30 & & 15 & 15 & 15 & 15 \\
\cline{1-2}\cline{4-7}
\end{tabular}
\end{center}

The tabel below was modified from the previous section to show the number of replicates in each treatment of the experiment where both factors were simultaneously manipulated.

\begin{center}
\begin{tabular}{cc|c|c|c}
 & \multicolumn{4}{c}{Relative Humidity} \\
\cline{2-5}
 & 20\% & 40\% & 60\% & 80\% \\
\cline{2-5}
\multicolumn{1}{c|}{$10^{o}$C} & 15 & 15 & 15 & \multicolumn{1}{c|}{15} \\
\hline
\multicolumn{1}{c|}{$15^{o}$C} & 15 & 15 & 15 & \multicolumn{1}{c|}{15} \\
\cline{2-5}
\end{tabular}
\end{center}

The key to examining the benefits of the multi-factor experiment is to determine the number of individuals that give ``information'' about (i.e., are exposed to) each factor. From the last table it is seen that all 120 individuals were exposed to one of the temperature levels with 60 individuals exposed to each level. In contrast, only 30 individuals were exposed to these levels in the single-factor experiment. In addition, all 120 individuals were exposed to one of the relative humidity levels with 30 individuals exposed to each level. Again, this is in contrast to the single-factor experiment where only 15 individuals were exposed to these levels. Thus, the first advantage of multi-factor experiments is that the available individuals are used more efficiently. In other words, more ``information'' (i.e., the responses of more individuals) is obtained from a multi-factor experiment than from combinations of single-factor experiments.\footnote{The real importance of this advantage will become apparent when statistical power is introduced in \modref{chap:HypothesisTests}.}

A properly designed multi-factor experiment also allows researchers to determine if multiple factors interact to impact an individual's response.\index{Interaction effect} For example, consider the hypothetical results from this experiment in \figref{fig:ExpDInt}.\footnote{The means of each treatment are plotted and connected with lines in this plot.} The effect of relative humidity is to increase the growth rate for those individuals at $10^{o}$C (black line) but to decrease the growth rate for those individuals at $15^{o}$C (blue line). That is, the effect of relative humidity differs depending on the level of temperature. When the effect of one factor differs depending on the level of the other factor, then the two factors are said to \textit{interact}. Interactions cannot be determined from the two single-factor experiments because the same individuals are not exposed to levels of the two factors at the same time.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/ExpDInt-1} 

}

\caption[Mean growth rates in a two-factor experiment that depict an interaction effect]{Mean growth rates in a two-factor experiment that depict an interaction effect.}\label{fig:ExpDInt}
\end{figure}


\end{knitrout}
\vspace{9pt} % added because of paragraph compressions following R code

Multi-factor experiments are used to detect the presence or absence of interaction, not just the presence of it. The hypothetical results in \figref{fig:ExpDNoInt} show that the growth rate increases with increasing relative humidity at about the same rate for both temperatures. Thus, because the effect of relative humidity is the same for each temperature (and vice versa), there does not appear to be an interaction between the two factors. Again, this could not be determined from the separate single-factor experiments.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/ExpDNoInt-1} 

}

\caption[Mean growth rates in a two-factor experiment that depict no interaction effect]{Mean growth rates in a two-factor experiment that depict no interaction effect.}\label{fig:ExpDNoInt}
\end{figure}


\end{knitrout}


\subsection{Allocating Individuals}
Individuals\footnote{When discussing experiments, an ``individual'' is often referred to as a ``replicate'' or an ``experimental unit.''} should be randomly allocated (i.e., placed into) to treatments.\index{Replicates} Randomization will tend to even out differences among groups for variables not considered in the experiment. In other words, randomization should help assure that all groups are similar before the treatments are imposed. Thus, randomly allocating individuals to treatments removes any bias (foreseen or unforeseen) from entering the experiment.

In the single-factor experiment above -- two treatments of temperature -- there were 120 agars. To randomly allocate these individuals to the treatments, 60 pieces of paper marked with ``10'' and 60 marked with ``15'' could be placed into a hat. One piece of paper would be drawn for each agar and the agar would receive the temperature found on the piece of paper. Alternatively, each agar could be assigned a unique number between 1 and 120 and pieces of paper with these numbers could be placed into the hat. Agars corresponding to the first 60 numbers drawn from the hat could then be placed into the first treatment. Agars for the next (or remaining) 60 numbers would be placed in the second treatment. This process is essentially the same as randomly ordering 120 numbers.

A random order of numbers is obtained with R by including the count of numbers as the only argument to \R{sample()}. For example, randomly ordering 1 through 120 is accomplished with

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> sample(120)
\end{verbatim}
\end{kframe}
\end{knitrout}
\vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
  [1]  80  30 100  90  21  68 104  79  64 106  98  16  73  91 107   1  60  54  26  99
 [21] 108 111  31  47  57  92   5  58  37  50  34  88  41  66  65  29 110 113   4  75
 [41]  93  23  49  97  35  84  74   7  15  39  70  94 114  14  71  20  33  67  86   8
 [61]   6  28  52  48  13  18  63  72  69 120  55  83  42   3  77  82  38  22  96  43
 [81]  56  89  78  17 112  44 103  46  59  85 109 115 118  87  32  62  51  95  24  40
[101] 119 102  19  27 116  36   2  12  45  53  11  76 117  61 105   9 101  25  81  10
\end{verbatim}
\end{kframe}
\end{knitrout}

Thus, the first five (of 60) agars in the 10$^{o}$C treatment are 80, 30, 100, 90, and 21. The first five (of 60) agars in the 15$^{o}$C treatment are 6, 28, 52, 48, and 13.

In the modified experiment with two factors -- temperature and relative humidity -- with eight treatments containing 15 agars each, it is more efficient to save the random numbers into an object and then select the numbers in the first 15 positions, then the second 15 positions, etc. Positions are selected from an object by putting the position numbers in square brackets following the object name. Additionally, a colon is used to make a sequence of integers from the number before to the number after the colon.\footnote{For example, \R{1:4} will make an object with the numbers 1, 2, 3, and 4 in it.}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ragars2 <- sample(120)
> ragars2[1:15]     # "grab" the first 15 numbers
 [1]  61  82 103  31  66  81 105  40 104 106   5   9  71  36   8
> ragars2[16:30]    # "grab" the second 15 numbers, and so on
 [1] 120   6  26  41  62 111  83  20  57   1  63  86  70  85  73
\end{verbatim}
\end{kframe}
\end{knitrout}

This design might be shown with the following table, where the numbers in each cell represent the first two agars selected to receive that treatment.\footnote{Only the first two numbers are shown because of space constraints.}

\begin{center}
\begin{tabular}{cc|c|c|c}
 & \multicolumn{4}{c}{Relative Humidity} \\
\cline{2-5}
 & 20\% & 40\% & 60\% & 80\% \\
\cline{2-5}
\multicolumn{1}{c|}{$10^{o}$C} & 61,82,$\cdots$ & 120,6,$\cdots$ & 60,72,$\cdots$ & \multicolumn{1}{c|}{89,49,$\cdots$} \\
\hline
\multicolumn{1}{c|}{$15^{o}$C} & 78,10,$\cdots$ & 109,101,$\cdots$ & 22,2,$\cdots$ & \multicolumn{1}{c|}{114,77,$\cdots$} \\
\cline{2-5}
\end{tabular}
\end{center}

\warn{Individuals should be randomly allocated to treatments to remove bias.}


\subsection{Design Principles}
There are many other methods of designing experiments and allocating individuals that are beyond the scope of this book.\footnote{Other common designs include blocked, Latin square, and nested designs.} However, all experimental designs contain the following three basic principles.\index{Experiment!Principles}
\begin{Enumerate}
  \item \textbf{Control} the effect of variables on the response variable by deliberately manipulating factors to certain levels and maintaining constancy among other variables.
  \item \textbf{Randomize} the allocation of individuals to treatments to eliminate bias.
  \item \textbf{Replicate individuals} (use many individuals) in the experiment to reduce chance variation in the results.
\end{Enumerate}

Proper control in an experiment allows for strong cause-and-effect conclusions to be made (i.e., to state that an observed difference in the response variable was due to the levels of the factor or chance variation rather than some other foreseen or unforeseen variable). Randomly allocating individuals to treatments removes any bias that may be included in the experiment. For example, if we do not randomly allocate the agars to the treatments, then it is possible that a set of all ``poor'' agars may end up in one treatment. In this case, any observed differences in the response may not be due to the levels of the factor but to the prior quality of the agars. Replication means that there should be more than one or a few individuals in each treatment. This reduces the effect of each individual on the overall results. For example, if there was one agar in each treatment, then, even with random allocation, the effect of that treatment may be due to some inherent properties of that agar rather than the levels of the factors. Replication, along with randomization, helps assure that the groups of individuals in each treatment are as alike as possible at the start of the experiment.


\section{Observational Studies -- Sampling}
In observational studies the researcher has no control over any of the variables observed for an individual.\index{Observational Study} The researcher simply observes individuals, disturbing them as little as possible, trying to get a ``picture'' of the population. Observational studies cannot be used to make cause-and-effect statements because all variables that may impact the outcome may not have been measured or specifically controlled. Thus, any observed difference among groups may be caused by the variables measured, some other unmeasured variables, or chance (randomness).

Consider the following as an example of the problems that can occur when all variables are not measured. For many years scientists thought that the brains of females weighed less than the brains of males. They used this finding to support all kinds of ideas about sex-based differences in learning ability. However, these earlier researchers failed to measure body weight, which is strongly related to brain weight in both males and females. After controlling for the effect of differences in body weights, there was no difference in brain weights between the sexes. Thus, many sexist ideas persisted for years because cause-and-effect statements were inferred from data where all variables were not considered.

\warn{Strong cause-and-effect statements CANNOT be made from observational studies.}

In observational studies, it is important to understand to which population inferences will refer.\footnote{Thus, it is very important to first perform an IVPPS as discussed in \modref{chap:FoundationalDefinitions}.} To make useful inferences from a sample, the sample must be an unbiased representation of the population. In other words, it must not systematically favor certain individuals or outcomes.

For example, consider that you want to determine the mean length of all fish in a particular lake (e.g., Square Lake from \modref{chap:FoundationalDefinitions}). Using a net with large mesh, such that only large fish are caught, would produce a biased sample because interest is in all fish not just the large fish. Setting the nets near spawning beds (i.e., only adult fish) would also produce a biased sample. In both instances, a sample would be collected from a population other than the population of interest. Thus it is important to select a sample from the specified population.

\warn{It is important to understand the population before considering how to take a sample.}

\subsection{Types of Sampling Designs}
Three common types of sampling designs -- voluntary response, convenience, and probability-based samples -- are considered in this section. Voluntary response and convenience samples tend to produce biased samples, whereas proper probability-based samples will produce an unbiased sample.

A \textbf{voluntary response} sample consists of individuals that have chosen themselves for the sample by responding to a general appeal.\index{Voluntary Response Sample} An example of a voluntary response sample would be the group of people that respond to a general appeal placed in the school newspaper. If the population of interest in this sample was all students at the school, then this type of general appeal would likely produce a biased sample of students that (i) read the school newspaper, (ii) feel strongly about the topic, or (iii) both.

A \textbf{convenience} sample consists of individuals who are easiest to reach for the researcher.\index{Convenience Sample} An example of a convenience sample is when a researcher queries only those students in a particular class. This sample is ``convenient'' because the individuals are easy to gather. However, if the population of interest was all students at the school, then this type of sample would likely produce a biased sample of students that is likely of (i) one major or another, (ii) one or a few ``years-in-school'' (e.g., Freshman or Sophomores), or (iii) both.

In probability-based sampling, each individual of the population has a known chance of being selected for the sample. The simplest probability-based sample is the \textbf{Simple Random Sample} (SRS) where each individual has the same chance of being selected.\index{Simple Random Sample} Proper selection of an SRS requires each individual to be assigned a unique number. The SRS is then formed by choosing random numbers and collecting the individuals that correspond to those numbers.

For example, an auditor may need to select a sample of 30 financial transactions from all transactions of a particular bank during the previous month. Because each transaction is numbered, the auditor may know that there were 1112 transactions during the previous month (i.e., the population). The auditor would then number each transaction from 1 to 1112, randomly select 30 numbers (with no repeats) from between 1 and 1112, and then physically locate the 30 transactions that correspond to the 30 selected numbers. Those 30 transactions are the SRS.

Random numbers are selected in R by including the population size as the first and sample size as the second argument to \R{sample()}. For example, 30 numbers from between 1 and 1112 is selected with

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> sample(1112,30)
\end{verbatim}
\end{kframe}
\end{knitrout}
\vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
 [1]   75  320  874  104  128  870  607 1091 1030 1053 1031  518  433  893  816  903
[17]  342 1016  136  580  670  376  576 1076 1034  365  492  189  409   66
\end{verbatim}
\end{kframe}
\end{knitrout}

Thus, accounts 75, 320, 874, 104, and 128 would be the first five (of 30) selected.

There are other more complex types of probability-based samples that are beyond the scope of this course.\footnote{For example, stratified samples, nested, and multistage samples.} However, the goal of these more complex types of samples is generally to impart more control into the sampling design.

\warn{A proper SRS requires each individual i the population to be assigned a unique number.}

If the population is such that a number cannot be assigned to each individual, then the researcher must try to use a method for which they feel each individual has an equal chance of being selected. Usually this means randomizing the technique rather than the individuals. In the fish example discussed on the previous page, the researcher may consider choosing random mesh sizes, random locations for placing the net, or random times for placing the net. Thus, in many real-life instances, the researcher simply tries to use a method that is likely to produce an SRS or something very close to it.

\warn{If a number cannot be assigned to each individual in the population, then the researcher should randomize the ``technique'' to assure as close to a random sample as possible.}

Polls, campaign or otherwise, are examples of observational studies that you are probably familiar with. The following are links where various aspects of polling are discussed.
\begin{Itemize}
  \item \href{http://media.gallup.com/PDF/FAQ/HowArePolls.pdf}{How Polls are Conducted by Frank Newport, Lydia Saad, and David Moore, The Gallup Organization}.
  \item \href{http://www2.psych.purdue.edu/~codelab/Invalid.Polls.html}{Why Do Campaign Polls Zigzag So Much? by G.S. Wasserman, Purdue U}.
\end{Itemize}


\subsection{Of What Value are Observational Studies?}
Properly designed experiments can lead to ``cause-and-effect'' statements, whereas observational studies (even properly designed) are unlikely to lead to such statements. Furthermore, in the last section, it was suggested that it is very difficult to take a proper probability-based sample because it is hard to assign a number to each individual in the population (precisely because entire populations are very difficult to ``see''). So, do observational studies have any value?  There are at least three reasons why observational studies are useful.

The scientific method begins with making an observation about a natural phenomenon. Observational studies may serve to provide such an observation. Alternatively, observational studies may be deployed after an observation has been made to see if that observation is ``prevalent'' and worthy of further investigation. Thus, observational studies may lead directly to hypotheses that form the basis of experiments.

Experiments are often conducted under very confined and controlled conditions so that the effect of one or more factors on the response variable can be identified. However, at the conclusion of an experiment it is often questioned whether a similar response would be observed ``in nature'' under much less controlled conditions. For example, one might determine that a certain fertilizer increases growth of a certain plant in the greenhouse, with consistent soil characteristics, temperatures, lighting, etc. However, it is a much different, and, perhaps, more interesting, question to determine if that fertilizer elicits the same response when applied to an actual field.

Finally, there are situations where conducting an experiment simply cannot be done, either for ethical, financial, size, or other constraints. For example, it is generally accepted that smoking causes cancer in humans even though an experiment where one group of people was forced to smoke while another was not allowed to smoke has not been conducted. Similarly, it is also very difficult to perform valid experiments on ``ecosystems.''  In these situations, an observational study is simply the best study allowable. Cause-and-effect statements are arrived at in these situations because observational studies can be conducted with some, though not absolute, control and control can be imparted mathematically into some analyses.\footnote{These analyses are beyond the scope of this book, though.} In addition, a ``preponderance of evidence'' may be arrived at if enough observational studies point to the same conclusion.



\chapter{Getting Data Into R} \label{chap:FoundationsR}

\vspace{-12pt}
\minitoc

\section{Setting Up R and Helpers} \label{sect:RSetup}
\vspace{-4pt}
R is a software environment for performing statistical analyses. RStudio is a helper program that makes it easier to use R. \R{NCStats} is a set of R functions that make the statistical methods used in this class easier. Herein, I will refer to R but you will interact with R through RStudio.

Detailed methods for downloading, installing, and configuring R, RStudio, and \R{NCStats} on your personal computer are given on the \href{http://derekogle.com/NCMTH107/resources#computer}{Resources page of the course website}.

\section{Working With R Basics} \label{sect:RBasics}
\vspace{-4pt}
\subsection{Saving Results} \label{sect:RSaving}
\vspace{-4pt}
Results are not saved in R. Rather, ``scripts'' of successful R commands are saved and, then, if the analysis needs to be re-done, the entire set of commands is opened and run again. When writing a report, all tabular and graphical output should be copied from R and pasted into your report document. This document will serve as your analysis report and can be modified to include answers to questions, references to the tables and graphs, etc.\footnote{Specifics for how to format homework assignments is on the course syllabus}  All data that is not a simple vector (see \sectref{sect:RVectors}) should be entered into R through ``comma-separated values'' text files (see \sectref{sect:REnterData}).

R does allow one to save a ``workspace'', though I urge you not to do that. Rather, save your ``good'' commands in a script and save your ``good'' results in a report document; do not save the workspace.


\subsection{Expressions and Assignments} \label{sect:RExprAssn}
Expressions in R are mathematical ``equations'' that are evaluated by R with a result seen immediately. An example of an expression in R is
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> 5+log(7)-pi
[1] 3.804317
\end{verbatim}
\end{kframe}
\end{knitrout}

where \R{log()} and \R{pi} are built-in functions used to compute the natural log and find the value of $\pi$, respectively. Expressions in R are like using a calculator where the result is shown, but not saved for subsequent analyses. In addition, expressions in R follow the same order of operations and use of parentheses as expressions entered into your calculator.

Results from an expression are typically saved for further computations by assigning the results to an object with the assignment operator (i.e., \R{<-}). The general form for saving the result of an expression into an object is \R{object <- expression}. The result of the expression will not be seen unless the object name is subsequently typed into R (but see below). For example, the result of the previous expression is saved into an object called \R{x} and then viewed with
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> x <- 5+log(7)-pi
> x
[1] 3.804317
\end{verbatim}
\end{kframe}
\end{knitrout}
The result of an expression can be both assigned and printed by surrounding the command in parentheses. For example, the following assigns the result of the expression to \R{y} and prints the result.\footnote{The spaces between the expression and the parentheses are only needed to increase legibility.}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( y <- 15*exp(2) )
[1] 110.8358
\end{verbatim}
\end{kframe}
\end{knitrout}

An object can be named whatever you want, with the exception that it cannot start with a number, contain a space, or be the name of a reserved word or function in R (e.g., \R{pi} or \R{log}). Furthermore, object names should be short and simple enough that you can remember what is contained in the object. It is also good practice to view the object immediately after making the assignment to make sure that it contains results that seem appropriate.



\subsection{Functions and Arguments}  \label{sect:RFunctions}
R contains many ``programs,'' or functions, to perform particular tasks. A function is ``called'' by typing the function name followed by open and closed parentheses. Arguments, which the function will use to perform its task, are contained within the parentheses. The \R{log()} function, used in the previous section, is an example of a function. The name of the function is \R{log} and the argument, the number for which to compute the natural log, is contained within the parentheses following the function name. Many other functions will be described below and in subsequent modules.

\warn{Regular curved parentheses have two primary uses in R: (1) to control order of operations in expressions (as with a calculator) and (2) to contain the arguments sent to a function.}



\section{Working With Data}
\subsection{Data Types}  \label{sect:RDataTypes}
Data in R will be designated as an integer (whole numbers), numeric (non-integer numeric values), character (strings), factor (group membership), or logical (TRUE/FALSE). The type of data largely dictates the type of analysis that can be performed. Data types will be discussed in more detail as needed. Note, however, that the \textbf{factor} data type is a special case of the character data type, where the specific items describe the group to which an individual belongs. This description allows for certain analyses in later modules.

\subsection{Entering Data}  \label{sect:REnterData}
For real data (i.e., several variables from many individuals) it is most efficient to enter data into a comma-separated values (CSV) file and then import that file into R. Creating a CSV file with Microsoft Excel is described below, though there are other ways to create CSV files (see \href{http://derekogle.com/NCMTH107/resources/FAQ/}{FAQs on class webpage}). This explanation assumes that you have a basic understanding of Excel (or other spreadsheet softwares).

The spreadsheet should be organized with variables in columns and individuals in rows, with the exception that the first row should contain variable names. The example spreadsheet below shows the length (cm), weight (kg), and capture location data for a small sample of Black Bears.

\begin{center}
  \includegraphics[width=1.5in]{Figs/Data_File_1.jpg}
\end{center}

Variable names must NOT contain spaces. For example, don't use \var{total length} or \var{length (cm)}. If you feel the need to have longer variable names, then separate the parts with a period (e.g., \var{length.cm}) or an underscore (e.g., \var{length\_cm}). Furthermore, numerical measurements should NOT include units (e.g., don't use \verb"7 cm"). Finally, for categorical data, make sure that all categories are consistent (e.g., do not have a column that contains both \verb"bayfield" and \verb"Bayfield").

The spreadsheet is saved as a CSV file by selecting the \verb"File..Save As" menu item, which will produce the dialog box below. In this dialog box, change \verb"Save as type" to \verb"CSV (Comma delimited) (*.csv)" (you may have to scroll down), provide a file name (don't have any periods in the name besides for ``.csv'', which you should not have to type), select a location to save the file (don't forget this location!!), and press \verb"Save". Two ``warning'' dialog boxes may then appear -- select \verb"OK" for the first and \verb"YES" for the second. You can now close the spreadsheet file (you may be asked to save changes -- you should say \verb"No").
\begin{center}
  \includegraphics[width=3.5in]{Figs/Data_File_2.jpg}
\end{center}

The following steps are used to load the data in the CSV file into RStudio.

\begin{Itemize}
  \item Open RStudio.
  \item Open a new script by selecting the \verb"File", \verb"New File", \verb"R Script" menu items.
  \item Type \R{library(NCStats)} in the new script (i.e., in the upper-left pane).
  \item Save this script by selecting the \verb"File", \verb"Save" menu items. In the ensuing dialog box, navigate to the \textbf{exact same directory} where you saved the data, type a name for the file in the \verb"File name:" box (\textbf{do not use a period in this name!!}), and press \verb"Save".

\begin{center}
  \includegraphics[width=3.5in]{Figs/Data_File_3.jpg}
\end{center}

  \item Set the working directory (tell R where the file is) with the \verb"Session", \verb"Set Working Directory ...", \verb"To Source File Location" menu items in RStudio. RStudio will print an appropriate \R{setwd()} command to the console (lower-left pane). Copy this command from the console to the second line in your script.\footnote{Doing this will eliminate the need to manually select the menu options every time you want to run this script.}  For example, I stored the file created above in the \verb"C:/data" directory, so that RStudio will create this \R{setwd("C:/data")}.
  \item The CSV file is read into R by including the name of the file (in quotes) in \R{read.csv()}. For example, \R{"Bears.csv"} is read into R and stored into an object called \R{bears} with \R{bears <- read.csv("Bears.csv")}.


  \item One should check the data in this object as descried in \sectref{sect:RViewdf} below.
\end{Itemize}

It is important that each row of the data.frame correspond to one individual. This is critically important when data are recorded for two different groups (e.g., for a two-sample t-test; see \modref{chap:tTest2}). For example, the following data are methyl mercury levels recorded in mussels from two locations labeled as ``impacted'' and ``reference.''
\begin{Verbatim}
  impacted   0.011  0.054  0.056  0.095  0.051  0.077
  reference  0.031  0.040  0.029  0.066  0.018  0.042  0.044
\end{Verbatim}
To follow the ``one individual per row'' rule, these data are entered in stacked format where the ``reference'' data are stacked underneath the ``impacted'' data and a column is used to indicate to which group the individuals belong. For example, the Excel file for data entry would look like the following.

\begin{center}
  \includegraphics[width=1in]{Figs/StackedData.jpg}
\end{center}

\subsubsection*{Alternative Forms of Getting Data} \label{sect:RAltData}
Some of the data files that you will use are provided on the \href{http://derekogle.com/NCMTH107/resources/data_107}{Data for MTH107} resource page of the class webpage. In these cases, the data should be downloaded from the webpage and saved in the same directory or folder as your analysis script. The downloaded file is then read into R in the same manner as described previously (i.e., set the working directory with \R{setwd()} and use \R{read.csv()}).

A few data files used in these notes are supplied with R or the \R{NCStats} package. These files are loaded with \R{data()}. For example, the \dfile{iris} data file is loaded into R with
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> data(iris)
\end{verbatim}
\end{kframe}
\end{knitrout}


\vspace{24pt}
\subsection{Working With Data Frames}  \label{sect:RWorkdf}
\subsubsection{Viewing a Data Frame}  \label{sect:RViewdf}
Many users are disoriented in R because they cannot ``see'' their data in the same way that they see it in a spreadsheet program. There are, however, several options for viewing your data. First, you can type the name of the data.frame object to see its entire contents.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> bears
   length.cm weight.kg      loc
1      139.0       110 Bayfield
2      138.0        60 Bayfield
3      139.0        90 Bayfield
4      120.5        60 Bayfield
5      149.0        85 Bayfield
6      141.0       100  Ashland
7      141.0        95  Ashland
8      150.0        85  Douglas
9      166.0       155  Douglas
10     151.5       140  Douglas
11     129.5       105  Douglas
12     150.0       110  Douglas
\end{verbatim}
\end{kframe}
\end{knitrout}

Typing the name is adequate for small data.frames, but not useful for large data.frames. The entire data.frame is opened in a separate window by double-clicking on the name of the data.frame in the \R{Environment} tab of RStudio (in upper-right pane). Alternatively, the first and last three rows of a data.frame are viewed by including the data.frame object in \R{headtail()}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> headtail(bears)
   length.cm weight.kg      loc
1      139.0       110 Bayfield
2      138.0        60 Bayfield
3      139.0        90 Bayfield
10     151.5       140  Douglas
11     129.5       105  Douglas
12     150.0       110  Douglas
\end{verbatim}
\end{kframe}
\end{knitrout}

In addition to viewing the contents, it is useful to examine the structure of the data.frame as returned from \R{str()}. In this example, it is seen that three variables were recorded on 12 individuals. The first variables -- \var{length.cm} and \var{weight.kg} -- are numerical measurements made on the bears. The last variable -- \var{loc} -- is a factor variable that records the capture location for each bear.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> str(bears)
'data.frame':	12 obs. of  3 variables:
 $ length.cm: num  139 138 139 120 149 ...
 $ weight.kg: int  110 60 90 60 85 100 95 85 155 140 ...
 $ loc      : Factor w/ 3 levels "Ashland","Bayfield",..: 2 2 2 2 2 1 1 3 3 3 ...
\end{verbatim}
\end{kframe}
\end{knitrout}
The levels of the \var{loc} variable may be seen by including this variable (with the data.frame name) as the argument to \R{levels()}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> levels(bears$loc)
[1] "Ashland"  "Bayfield" "Douglas" 
\end{verbatim}
\end{kframe}
\end{knitrout}

In the previous example, the \R{\$} notation was used to identify a particular variable (i.e., \R{loc}) within a data.frame (\R{bears}). Think of variables as being nested inside data.frames and, thus, to access the variable you must first identify the data.frame in which it exists and then the name of the variable. The \R{\$} simply separates the data.frame from the variable.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> bears$length.cm
 [1] 139.0 138.0 139.0 120.5 149.0 141.0 141.0 150.0 166.0 151.5 129.5 150.0
> bears$loc
 [1] Bayfield Bayfield Bayfield Bayfield Bayfield Ashland  Ashland  Douglas  Douglas 
[10] Douglas  Douglas  Douglas 
Levels: Ashland Bayfield Douglas
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsubsection{Filtering a data.frame}  \label{sect:RSubsetdf}
It is common to create a new data.frame that contains only some of the individuals from an existing data.frame. For example, a researcher may want only bears captured in Bayfield County or bears that weighed more than 100 kg. The process of creating the newer, smaller data.frame is called filtering (or subsetting) and is accomplished with \R{filterD()}. The \R{filterD()} function requires the original data.frame as the first argument and a condition statement as the second argument. The condition statement is used to either include or exclude individuals from the original data.frame. Condition statements consist of the name of a variable in the original data.frame, a comparison operator, and a comparison value \tabrefp{tab:RSubsetConditions}. The result from \R{filterD()} should be assigned to an object, which is then the name of the new data.frame.
\vspace{6pt}

\begin{table}[htbp]
  \caption{Condition operators used in \R{filterD()} and their results. Note that \emph{variable} generically represents a variable in the original data.frame and \emph{value} is a generic value or level. Both \emph{variable} and \emph{value} would be replaced with specific items (see examples in main text).}  \label{tab:RSubsetConditions}
  \centering
\begin{tabular}{cc}
\hline\hline
Condition Operator &  Individuals Returned from Original Data Frame \\
\hline
\widen{-1}{6}{\emph{variable}} $==$ \emph{value} & all individual that are \textbf{equal} to the given value \\
\widen{-1}{5}{\emph{variable}} $!=$ \emph{value} & all individuals that are \textbf{NOT equal} to the given value \\
\widen{-1}{5}{\emph{variable}} $>$ \emph{value} & all individuals that are \textbf{greater than} the given value \\
\widen{-1}{5}{\emph{variable}} $>=$ \emph{value} & all individuals that are \textbf{greater than or equal} to the given value \\
\widen{-1}{5}{\emph{variable}} $<$ \emph{value} & all individuals that are \textbf{less than} the given value \\
\widen{-1}{5}{\emph{variable}} $<=$ \emph{value} & all individuals that are \textbf{less than or equal} to the given value \\
\widen{-1}{5}{\emph{condition}}, \emph{condition} & all individuals that \textbf{meet both conditions} \\
\widen{-2}{6}{\emph{condition}} $|$ \emph{condition} & all individuals that \textbf{meet one or both conditions}\footnote{Note that this ``or'' operator is a ``vertical line'' which is typed with the shift-backslash key.} \\
\hline\hline
\end{tabular}
\end{table}

\vspace{6pt}
The following are examples of new data.frames created from \var{bears}. The name of the new data.frame (i.e., object left of the assignment operator) can be any valid object name. As demonstrated below, the new data.frame (or its structure) should be examined after each filtering to ensure that the data.frame actually contains the items that you desire.

\vspace{-8pt}
\begin{itemize}
  \item Only individuals from \emph{Bayfield} county.
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> bf <- filterD(bears,loc=="Bayfield")
> bf
  length.cm weight.kg      loc
1     139.0       110 Bayfield
2     138.0        60 Bayfield
3     139.0        90 Bayfield
4     120.5        60 Bayfield
5     149.0        85 Bayfield
\end{verbatim}
\end{kframe}
\end{knitrout}

  \item Individuals from both \emph{Bayfield} and \emph{Ashland} counties.
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> bfash <- filterD(bears,loc %in% c("Bayfield","Ashland"))
> bfash
  length.cm weight.kg      loc
1     139.0       110 Bayfield
2     138.0        60 Bayfield
3     139.0        90 Bayfield
4     120.5        60 Bayfield
5     149.0        85 Bayfield
6     141.0       100  Ashland
7     141.0        95  Ashland
\end{verbatim}
\end{kframe}
\end{knitrout}

  \item Individuals with a weight greater than 100 kg.
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> gt100 <- filterD(bears,weight.kg>100)
> gt100
  length.cm weight.kg      loc
1     139.0       110 Bayfield
2     166.0       155  Douglas
3     151.5       140  Douglas
4     129.5       105  Douglas
5     150.0       110  Douglas
\end{verbatim}
\end{kframe}
\end{knitrout}

  \item Individuals from \emph{Douglas} County that weighed at least 150 kg.
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> do150 <- filterD(bears,loc=="Douglas",weight.kg>=150)
> do150
  length.cm weight.kg     loc
1       166       155 Douglas
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{itemize}


\subsection{Vectors}  \label{sect:RVectors}
\vspace{-4pt}
Data.frames are the primary structure in which to store real data. However, much simpler situations that don't require a data.frame may arise. In R, items of the same data type \sectrefp{sect:RDataTypes} are stored in a one-dimensional \emph{vector}. Vectors are usually displayed in one row (with many columns), but they may also be thought of as a single column (with many rows). Items are entered into a vector with \R{c()}, where the individual arguments are specific numbers, characters, or logical values.\footnote{Note that \R{c} comes from the word ``concatenate.''}  Items for a vector of characters must be contained within paired quotes.
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( v <- c(1,2,5) )
[1] 1 2 5
> ( y <- c("Iowa","Minnesota","Wisconsin") )
[1] "Iowa"      "Minnesota" "Wisconsin"
\end{verbatim}
\end{kframe}
\end{knitrout}

Single variables from a data.frame are vectors.
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> bears$length.cm
 [1] 139.0 138.0 139.0 120.5 149.0 141.0 141.0 150.0 166.0 151.5 129.5 150.0
\end{verbatim}
\end{kframe}
\end{knitrout}

Vectors that are not extracted from a data.frame will only be used in this course for very simple lists of items, usually as arguments in a function.


\subsubsection{Selecting Individuals}  \label{sect:RSelectIndivs}
In some instances, you may need to select or exclude an individual from a data.frame or vector. Vectors are one-dimensional objects that are indexed simply by the position within the object. Positions within an object are identified with square brackets. For example, the second item from the \R{y} vector created above is extracted with
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> y[2]
[1] "Minnesota"
\end{verbatim}
\end{kframe}
\end{knitrout}

Alternatively, the second item can be excluded by preceding the position number with a negative sign.
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> y[-2]
[1] "Iowa"      "Wisconsin"
\end{verbatim}
\end{kframe}
\end{knitrout}

Data.frames are two-dimensional objects that are indexed by a row and a column, in that order. Again, positions within an object identified within square brackets. However, for data.frames, both a row and a column specification must be given. For example, the item in the third row and second column of \R{bears} is selected below.
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> bears[3,2]
[1] 90
\end{verbatim}
\end{kframe}
\end{knitrout}

An entire row or column may be selected by omitting the other dimension. For example, one could select the entire second column with \R{bears[,2]}, but this is also the \R{weight.kg} variable and is better selected, as shown above, with \R{bears\$weight.kg}. As a better example, the entire third row is selected below (note that the column designation was omitted).
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> bears[3,]
  length.cm weight.kg      loc
3       139        90 Bayfield
\end{verbatim}
\end{kframe}
\end{knitrout}

Multiple rows are selected by combining row indices together with \R{c()}. For example, the third, fifth, and eighth rows are selected below (again, the column index is omitted).
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> bears[c(3,5,8),]
  length.cm weight.kg      loc
3       139        90 Bayfield
5       149        85 Bayfield
8       150        85  Douglas
\end{verbatim}
\end{kframe}
\end{knitrout}

Finally, rows can be excluded by preceding the row indices with a negative sign.
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> bears[-c(3,5,8,10,12),]
   length.cm weight.kg      loc
1      139.0       110 Bayfield
2      138.0        60 Bayfield
4      120.5        60 Bayfield
6      141.0       100  Ashland
7      141.0        95  Ashland
9      166.0       155  Douglas
11     129.5       105  Douglas
\end{verbatim}
\end{kframe}
\end{knitrout}



\chapter[Summary One Quant Var]{Summaries for One Quantitative Variable)} \label{chap:UnivEDAQuant1}

\vspace{-48pt}
\minitoc
\vspace{12pt}

\lettrine{S}{ummarizing large quantities of data with} few graphical or numerical summaries makes it is easier to identify meaning from data (discussed in \modref{chap:WhyStatsImportant}). Numeric and graphical summaries specific to a single quantitative variable are described in this module. Interpretations from these numeric and graphical summaries are described in the next module.

Two data sets will be considered in this module when making calculations ``by hand'' (i.e., without using R). The first data set consists of the number of open pit mines in countries that have open pit mines \tabrefp{tab:MineData}.\footnote{These data were collected from \href{https://en.wikipedia.org/wiki/List_of_open-pit_mines}{this page}. See \sectref{sect:REnterData} for how to enter these data into R.} The second data set is Richter scale recordings for 15 major earthquakes \tabrefp{tab:EQData}. A third data set -- number of days of ice cover at ice gauge station 9004 in Lake Superior -- will be used to demonstrate calculations with R. These data are in \href{https://raw.githubusercontent.com/droglenc/NCData/master/LakeSuperiorIce.csv}{LakeSuperiorIce.csv} and are loaded into \R{LSI} below.\footnote{See \sectref{sect:RAltData} for how to access these data. These data are originally from the \href{http://www.nsidc.org/}{National Snow and Ice Data Center}.}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> LSI <- read.csv("data/LakeSuperiorIce.csv")
\end{verbatim}
\end{kframe}
\end{knitrout}


% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:27 2017
\begin{table}[ht]
\centering
\caption{Number of open pit mines in countries that have open pit mines.} 
\label{tab:MineData}
\begin{tabular}{rrrrrrrrrrrrr}
   \hline
2.0 & 11.0 & 4.0 & 1.0 & 15.0 & 12.0 & 1.0 & 1.0 & 3.0 & 2.0 & 2.0 & 1.0 & 1.0 \\ 
  1.0 & 1.0 & 2.0 & 4.0 & 1.0 & 4.0 & 2.0 & 4.0 & 2.0 & 1.0 & 4.0 & 11.0 & 1.0 \\ 
   \hline
\end{tabular}
\end{table}


% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:27 2017
\begin{table}[ht]
\centering
\caption{Richter scale recordings for 15 major earthquakes.} 
\label{tab:EQData}
\begin{tabular}{rrrrrrrrrrrrrrr}
   \hline
5.5 & 6.3 & 6.5 & 6.5 & 6.8 & 6.8 & 6.9 & 7.1 & 7.3 & 7.3 & 7.7 & 7.7 & 7.7 & 7.8 & 8.1 \\ 
   \hline
\end{tabular}
\end{table}



\section{Numerical Summaries} \label{sec:quEDACenter}
A ``typical'' value and the ``variability'' of a quantitative variable are often described from numerical summaries. Calculation of these summaries is described in this module, whereas their interpretation is described in \modref{chap:UnivEDAQuant1}. As you will see in \modref{chap:UnivEDAQuant1}, ``typical'' values are measures of \textbf{center} and ``variability'' is often described as \textbf{dispersion} (or spread). Three measures of center are the median, mean, and mode. Three measures of dispersion are the inter-quartile range, standard deviation, and range.

All measures computed in this module are summary statistics -- i.e., they are computed from individuals in a sample. Thus, the name of each measure should be preceded by ``sample'' -- e.g., sample median, sample mean, and sample standard deviation. These measures could be computed from every individual, if the population was known. These values would then be parameters and would be preceded by ``population'' -- e.g., population median, population mean, and population standard deviation.\footnote{See \modref{sect:IVPPSS} for clarification on the differences between populations and samples and parameters and statistics.}


\subsection{Median} \label{sec:Median}
The median is the value of the individual in the position that splits the \textbf{ordered} list of individuals into two equal-\textbf{sized} halves. In other words, if the data are ordered, half the values will be smaller than the median and half will be larger.

The process for finding the median consists of three steps,\footnote{Most computer programs use a more sophisticated algorithm for computing the median and, thus, will produce different results than what will result from applying these steps.}
\vspace{-8pt}
\begin{Enumerate}
  \item Order the data from smallest to largest.
  \item Find the ``middle \textbf{position}'' ($mp$) with $mp=\frac{n+1}{2}$.
  \item If $mp$ is an integer (i.e., no decimal), then the median is the value of the individual in that position. If $mp$ is not an integer, then the median is the average of the value immediately below and the value immediately above the $mp$.
\end{Enumerate}

As an example, the open pit data from \tabref{tab:MineData} are,

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:27 2017
\begin{tabular}{rrrrrrrrrrrrr}
  1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 2 & 2 & 2 \\ 
  2 & 2 & 2 & 3 & 4 & 4 & 4 & 4 & 4 & 11 & 11 & 12 & 15 \\ 
  \end{tabular}


Because $n=26$, the $mp=\frac{26+1}{2}=13.5$. The $mp$ is not an integer so the median is the average of the values in the 13th and 14th ordered positions (i.e., the two positions closest to $mp$). Thus, the median number of open pit mines in this sample of countries is $\frac{2+2}{2}=2$.

Consider finding the median of the Richter Scale magnitude recorded for fifteen major earthquakes as another example (ordered data are in \tabref{tab:EQData}). Because $n=15$, the $mp=\frac{15+1}{2}=8$. The $mp$ is an integer so the median is the value of the individual in the 8th ordered position, which is 7.1.

\warn{Don't forget to order the data when computing the median.}


\subsection{Inter-Quartile Range}
Quartiles are the values for the three individuals that divide ordered data into four (approximately) equal parts. Finding the three quartiles consists of finding the median, splitting the data into two equal parts at the median, and then finding the medians of the two halves.\footnote{You should review how a median is computed before proceeding with this section.}  A concern in this process is that the median is NOT part of either half if there is an odd number of individuals. These steps are summarized as,
\begin{Enumerate}
  \item Order the data from smallest to largest.
  \item Find the median -- this is the second quartile (Q2).
  \item Split the data into two halves at the median. If $n$ is odd (so that the median is one of the observed values), then the median is not part of either half.\footnote{Some authors put the median into both halves when $n$ is odd. The difference between the two methods is minimal for large $n$.}
  \item Find the median of the lower half of data -- this is the 1st quartile (Q1).
  \item Find the median of the upper half of data -- this is the third quartile (Q3).
\end{Enumerate}

These calculations are illustrated with the open pit mine data (the median was computed in \sectref{sec:Median}). Because $n=26$ is even, the halves of the data split naturally into two halves each with 13 individuals. Therefore, the $mp=\frac{13+1}{2}=7$ and the median of each half is the value of the individual in the seventh position. Thus, $Q1=1$ and $Q3=4$.

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:27 2017
\begin{tabular}{rrrrrrr}
  1 & 1 & 1 & 1 & 1 & 1 & 1 \\ 
  1 & 1 & 1 & 2 & 2 & 2 &  \\ 
  2 & 2 & 2 & 3 & 4 & 4 & 4 \\ 
  4 & 4 & 11 & 11 & 12 & 15 &  \\ 
  \end{tabular}


In summary, the first, second, and third quartiles for the open pit mine data are 1, 2, and 4, respectively. These three values separate the ordered individuals into approximately four equally-sized groups -- those with values less than (or equal to) 1, with values between (inclusive) 1 and 2, with values between (inclusive) 2 and 4, and with values greater (or equal to) than 4.

As another example, consider finding the quartiles for the earthquake data \tabrefp{tab:EQData}. Recall from above \sectrefp{sec:Median} that the median (=7.1) is in the eighth position of the ordered data. The value in the eighth position will not be included in either half. Thus, the two halves of the data are 5.5, 6.3, 6.5, 6.5, 6.8, 6.8, 6.9 and 7.3, 7.3, 7.7, 7.7, 7.7, 7.8, 8.1. The middle position for each half is then $mp=\frac{7+1}{2}=4$. Thus, the median for each half is the individual in the fourth position. Therefore, the median of the first half is $Q1=6.5$ and the median of the second half is $Q3=7.7$.

The interquartile range (IQR) is the difference between $Q3$ and $Q1$, namely $Q3-Q1$. However, the IQR (as strictly defined) suffers from a lack of information. For example, what does an IQR of 9 mean?  It can have a completely different interpretation if the IQR is from values of 1 to 10 or if it is from values of 1000 to 1009. Thus, the IQR is more useful if presented as both $Q3$ and $Q1$, rather than as the difference. Thus, for example, the IQR for the open pit mine data is from a $Q3$ of 4 to a $Q1$ of 1 and the IQR for the earthquake data is from a $Q3$ of 7 to a $Q1$ of 6.5.

\warn{The IQR can be thought of as the ``range of the middle half of the data.''}

\vspace{-12pt}
\warn{When reporting the IQR, explicitly state both $Q3$ and $Q1$ (i.e., do not subtract them).}


\subsection{Mean}
The mean is the arithmetic average of the data. The sample mean is denoted by $\bar{x}$ and the population mean by $\mu$. The mean is simply computed by adding up all of the values and dividing by the number of individuals. If the measurement of the generic variable $x$ on the $i$th individual is denoted as $x_{i}$, then the sample mean is computed with these two steps,
\begin{Enumerate}
  \item Sum (i.e., add together) all of the values -- $\Sum_{i=1}^{n}x_{i}$.
  \item Divide by the number of individuals in the sample -- $n$.
\end{Enumerate}
or more succinctly summarized with this equation,

\begin{equation} \label{eqn:SampleMean}
     \bar{x} = \frac{\Sum_{i=1}^{n}x_{i}}{n}
\end{equation}

For example, the sample mean of the open pit mine data is computed as follows:

\[ \bar{x} = \frac{2+11+4+1+15+ ... +2+1+4+11+1}{26} = \frac{94}{26} = 3.6  \]

Note in this example with a discrete variable that it is possible (and reasonable) to present the mean with a decimal. For example, it is not possible for a country to have 3.6 open pit mines, but it IS possible for the mean of a sample of countries to be 3.6 open pit mines.

\warn{As a general rule-of-thumb, present the mean with one more decimal than the number of decimals it was recorded in.}



\subsection{Standard Deviation}\label{sect:StdDev}
The sample standard deviation, denoted by $s$, is computed with these six steps:
\begin{Enumerate}
  \item Compute the sample mean (i.e., $\bar{x}$).
  \item For each value ($x_{i}$), find the difference between the value and the mean (i.e., $x_{i}-\bar{x}$).
  \item Square each difference (i.e., $(x_{i}-\bar{x})^{2}$).
  \item Add together all the squared differences.
  \item Divide this sum by $n-1$. [\textit{Stopping here gives the sample variance, $s^{2}$.}]
  \item Square root the result from the previous step to get $s$.
\end{Enumerate}

These steps are neatly summarized with
\vspace{-6pt}
\begin{equation}
  \label{eqn:SampleSD}
     s = \sqrt{\frac{\Sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}{n-1}}
\end{equation}

The calculation of the standard deviation of the earthquake data \tabrefp{tab:EQData} is facilitated with the calculations shown in \tabref{tab:SDCalc}. In \tabref{tab:SDCalc}, note that
\vspace{-6pt}
\begin{Itemize}
  \item $\bar{x}$ is the sum of the ``Value'' column divided by $n=15$ (i.e., $\bar{x}=7.07$).
  \item The ``Diff'' column is each observed value minus $\bar{x}$ (i.e., Step 2).
  \item The ``Diff$^2$'' column is the square of the differences (i.e., Step 3).
  \item The sum of the ``Diff$^2$'' column is Step 4.
  \item The sample variance (i.e., Step 5) is equal to this sum divided by $n-1=14$ or $\frac{6.773}{14}=0.484$.
  \item The sample standard deviation is the square root of the sample variance or $s=\sqrt{0.484}=0.696$.
\end{Itemize}

\begin{table}[htbp]
  \caption{Table showing an efficient calculation of the standard deviation of the earthquake data.}
  \label{tab:SDCalc}
    \centering
    \begin{tabular}{cccc}
\hline\hline
Indiv & Value & Diff & Diff$^2$ \\
i & $x_{i}$ & $x_{i}-\bar{x}$ & $(x_{i}-\bar{x})^{2}$ \\
\hline
1 & 5.5 & -1.57 & 2.454 \\
2 & 6.3 & -0.77 & 0.588 \\
3 & 6.5 & -0.57 & 0.321 \\
4 & 6.5 & -0.57 & 0.321 \\
5 & 6.8 & -0.27 & 0.071 \\
6 & 6.8 & -0.27 & 0.071 \\
7 & 6.9 & -0.17 & 0.028 \\
8 & 7.1 & 0.03 & 0.001 \\
9 & 7.3 & 0.23 & 0.054 \\
10 & 7.3 & 0.23 & 0.054 \\
11 & 7.7 & 0.63 & 0.401 \\
12 & 7.7 & 0.63 & 0.401 \\
13 & 7.7 & 0.63 & 0.401 \\
14 & 7,8 & 0.73 & 0.538 \\
15 & 8.1 & 1.03 & 1.068 \\
\hline
Sum & 106 & 0 & 6.773 \\
\hline\hline
    \end{tabular}
\end{table}

From this, on average, each earthquake is approximately 0.7 Richter Scale units different than the average earthquake in these data.

\warn{In the standard deviation calculations don't forget to take the square root of the variance.}

\vspace{-12pt}
\warn{The standard deviation is greater than or equal to zero.}

The standard deviation can be thought of as ``the average difference between the values and the mean.'' This is, however, not a strict definition because the formula for the standard deviation does not simply add the differences and divide by $n$ as this definition would imply. Notice in \tabref{tab:SDCalc} that the sum of the differences from the mean is 0. This will be the case for all standard deviation calculations using the correct mean, because the mean balances the distance to individuals below the mean with the distance of individuals above the mean (see \sectref{sect:MeanMedian} in the next module). Thus, the mean difference will always be zero. This ``problem'' is corrected by squaring the differences before summing them. To get back to the original units, the squaring is later ``reversed'' by the square root. So, more accurately, the standard deviation is the square root of the average squared differences between the values and the mean. Therefore, ``the average difference between the values and the mean'' works as a practical definition of the meaning of the standard deviation, but it is not strictly correct.

\warn{Use the fact that the sum of all differences from the mean equals zero as a check of your standard deviation calculation.}

Further note that the mean is the value that minimizes the value of the standard deviation calculation -- i.e., putting any other value besides the mean into the standard deviation equation will result in a larger value.

Finally, you may be wondering why the sum of the squared differences in the standard deviation calculation is divided by $n-1$, rather than $n$. Recall (from \sectref{sect:IVPPSS}) that statistics are meant to estimate parameters. The sample standard deviation is supposed to estimate the population standard deviation ($\sigma$). Theorists have shown that if we divide by $n$, $s$ will consistently underestimate $\sigma$. Thus, $s$ calculated in this way would be a biased estimator of $\sigma$. Theorists have found, though, that dividing by $n-1$ will cause $s$ to be an unbiased estimator of $\sigma$. Being unbiased is generally good -- it means that on average our statistic estimates our parameter (this concept is discussed in more detail in \modref{chap:SamplingDist}).


\subsection{Mode}
The mode is the value that occurs most often in a data set. For example, one open pit mine is the mode in the open pit mine data \tabrefp{tab:MCmode}.

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:27 2017
\begin{table}[ht]
\centering
\caption{Frequency of countries by each number of open pit mines.} 
\label{tab:MCmode}
\begin{tabular}{rrrrrrrr}
   \hline
Number of Mines & 1 & 2 & 3 & 4 & 11 & 12 & 15 \\ 
  Freq of Countries & 10 & 6 & 1 & 5 & 2 & 1 & 1 \\ 
   \hline
\end{tabular}
\end{table}


The mode for a continuous variable is the class or bin with the highest frequency of individuals. For example, if 0.5-unit class widths are used in the Richter scale data, then the modal class is 6.5-6.9 \tabrefp{tab:EQmode}.

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:27 2017
\begin{table}[ht]
\centering
\caption{Frequency of earthquakes by Richter Scale class.} 
\label{tab:EQmode}
\begin{tabular}{rllllll}
   \hline
Richter Scale Class & 5.5-5.9 & 6-6.4 & 6.5-6.9 & 7-7.4 & 7.5-7.9 & 8-8.4 \\ 
  Freq of Earthquakes & 1 & 1 & 5 & 3 & 4 & 1 \\ 
   \hline
\end{tabular}
\end{table}


Some data sets may have two values or classes with the maximum frequency. In these situations the variable is said to be \textbf{bimodal}.

\subsection{Range}
The range is the difference between the maximum and minimum values in the data and measures the ultimate dispersion or spread of the data. The range in the open pit mine data is 15-1 = 14.

The range should \textbf{never be used by itself} as a measure of dispersion. The range is extremely sensitive to outliers and is best used only to show all possible values present in the data. The range (as strictly defined) also suffers from a lack of information. For example, what does a range of 9 mean?  It can have a completely different interpretation if it came from values of 1 to 10 or if it came from values of 1000 to 1009. Thus, the range is more instructive if presented as both the maximum and minimum value rather than the difference.


\subsection{Computation of Summaries in R} \label{sect:DescStats}
All summary statistics described above, with the exception of the mode, is  calculated in R with \R{Summarize()}. To summarize a single variable a one-sided formula of the form \R{\TILDE quant} is used, where \R{quant} generically represents the quantitative variable, along with the \R{data=} argument. The number of digits after the decimal place is controlled with \R{digits=}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> Summarize(~days,data=LSI,digits=2)
     n nvalid   mean     sd    min     Q1 median     Q3    max 
 42.00  39.00 107.85  21.59  48.00  97.00 114.00 118.00 146.00 
\end{verbatim}
\end{kframe}
\end{knitrout}

From this it is seen that the sample median is 114 days, sample mean is 107.8 days, sample IQR is from 97 to 118 days, the sample standard deviation is 21.59 days, and the range is from 48 to 146.



\newpage
\section{Graphical Summaries}
\subsection{Histogram}
A histogram plots the frequency of individuals (y-axis) in classes of values of the quantitative variable (x-axis). Construction of a histogram begins by creating classes of values for the variable of interest. The easiest way to create a list of classes is to divide the range (i.e., maximum minus minimum value) by a ``nice'' number near eight to ten, and then round up to make classes that are easy to work with. The ``nice'' number between eight and ten is chosen to make the division easy and will be the number of classes. For example, the range of values in the open pit mine example is 15-1 = 14. A ``nice'' value near eight and ten to divide this range by is seven. Thus, the classes should be two units wide (=14/7) and, for ease, will begin at 0 \tabrefp{tab:MineFreqTable}.

\vspace{-6pt}
\begin{table}[htbp]
  \caption{Frequency table of number of countries in two-mine-wide classes.}
  \label{tab:MineFreqTable}
    \begin{Verbatim}[xleftmargin=25mm]
Class      0-1   2-3   4-5   6-7   8-9 10-11 12-13 14-15
Frequency   10     7     5     0     0     2     1     1
    \end{Verbatim}
\end{table}
\vspace{-12pt}

The frequency of individuals in each class is then counted (shown in the second row of \tabref{tab:MineFreqTable}). The plot is prepared with values of the classes forming the x-axis and frequencies forming the y-axis (\figref{fig:MineHist1}A). The first bar added to this skeleton plot has the bottom-left corner at 0 and the bottom-right corner at 2 on the x-axis, and a height equal to the frequency of individuals in the 0 and 1 class (\figref{fig:MineHist1}B). A second bar is then added with the bottom-left corner at 2 and the bottom-right corner at 4 on the x-axis, and a height equal to the frequency of individuals in the 2 and 3 class (\figref{fig:MineHist1}C). This process is continued with the remaining classes until the full histogram is constructed (\figref{fig:MineHist1}D).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.6\linewidth]{Figs/MineHist1-1} 

}

\caption[Steps (described in text) illustrating the construction of a histogram]{Steps (described in text) illustrating the construction of a histogram.}\label{fig:MineHist1}
\end{figure}


\end{knitrout}

Ideally eight to ten classes are used in a histogram. Too many or too few bars make it difficult to identify the shape and may lead to different interpretations. A dramatic example of the effect of changing the number of classes is seen in histograms of the length of eruptions for the Old Faithful geyser \figrefp{fig:histOF}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}


















































\begin{figure}[hbtp]

{\centering \animategraphics[width=.35\linewidth,controls,palindrome,autoplay]{1}{Figs/histOF-}{1}{52}

}

\caption[Histogram of length of eruptions for Old Faitfhul geyser with varying number of classes]{Histogram of length of eruptions for Old Faitfhul geyser with varying number of classes.}\label{fig:histOF}
\end{figure}


\end{knitrout}


\subsection{Boxplot}
The \textbf{five-number summary} consists of the minimum, Q1, median, Q3, and maximum values (effectively contains the range, IQR, and median). For example, the five-number summary for the open pit mine data is 1, 1, 2, 4, and 15 (all values computed in the previous section). The five-number summary may be displayed as a \textbf{boxplot}. A traditional boxplot (\figref{fig:MineBoxplot}-Left) consists of a horizontal line at the median, horizontal lines at Q1 and Q3 that are connected with vertical lines to form a box, and vertical lines from Q1 to the minimum and from Q3 to the maximum. In modern boxplots (\figref{fig:MineBoxplot}-Right) the upper line extends from Q3 to the last observed value that is within 1.5 IQRs of Q3 and the lower line extends from Q1 to the last observed value that is within 1.5 IQRs of Q1. Observed values outside of the whiskers are termed ``outliers'' by this algorithm and are typically plotted with circles or asterisks. If no individuals are deemed ``outliers'' by this algorithm, then the two traditional and modern boxplots will be the same.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.65\linewidth]{Figs/MineBoxplot-1} 

}

\caption[Traditional (Left) and modern (Right) boxplots of the open pit mine data]{Traditional (Left) and modern (Right) boxplots of the open pit mine data.}\label{fig:MineBoxplot}
\end{figure}


\end{knitrout}


\subsection{Construction of Graphs in R}
A simple (by default) histogram is constructed in R with \R{hist()} using a one-sided formula of the form \R{\TILDE quant}, where \R{quant} generically represents the quantitative variable, and the corresponding data frame in \R{data=}.\footnote{Note that this is the same formula used in \R{Summarize()}.} The x-axis label may be improved from the default value by including a label in \R{xlab=}. The width of the classes may be controlled with a positive integer in \R{w=}.\footnote{The endpoints for the classes may also be set by giving a vector of endpoints to \R{breaks=}.}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> hist(~days,data=LSI,xlab="Days of Ice Cover")      # Fig 5.4-Left
> hist(~days,data=LSI,xlab="Days of Ice Cover",w=20) # Fig 5.4-Right
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.34\linewidth]{Figs/Histogram1-1} 
\includegraphics[width=.34\linewidth]{Figs/Histogram1-2} 

}

\caption[Histograms of the duration of ice cover at ice gauge 9004 in Lake Superior using the default class widths (Left) and widths of 20 days (Right)]{Histograms of the duration of ice cover at ice gauge 9004 in Lake Superior using the default class widths (Left) and widths of 20 days (Right).}\label{fig:Histogram1}
\end{figure}


\end{knitrout}

A modern boxplot of a single variable is constructed in R with \R{boxplot()}, where the first argument is usually a specific variable in a data.frame. Additionally, the y-axis may be properly labeled with \R{ylab=}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> boxplot(LSI$days,ylab="Days of Ice Cover")
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.3\linewidth]{Figs/BoxplotLSI-1} 

}

\caption[Boxplot of the duration of ice cover at ice gauge 9004 in Lake Superior]{Boxplot of the duration of ice cover at ice gauge 9004 in Lake Superior.}\label{fig:BoxplotLSI}
\end{figure}


\end{knitrout}

\warn{The default histogram and boxplot should be modified by properly labeling the axes.}


\section{Multiple Groups} \label{sect:MultGroups}
It is common to need to compute numerical or construct graphical summaries of a quantitative variable separately for groups of individuals. In these cases it is beneficial to have a function that will efficiently construct a histogram and compute summary statistics for the quantitative variable separated by the levels of a factor variable. Separate histograms are constructed with \R{hist()}, if the first argument is a ``formula'' of the type \R{quant\TILDE group} where \R{quant} represents the quantitative response variable of interest and \R{group} represents the factor variable that indicates to which group the individual belongs. The data.frame that contains \R{quant} and \R{group} is given to \R{data=}. Summary statistics are separated by group by supplying the same formula and \R{data=} arguments to \R{Summarize()}.

As an example, the LSI data.frame contains a \R{period} variable that indicates whether the ice season was ``pre-1975'' or ``post-1975'' (which included 1975). Thus, one may be interested in examining the distribution of annual days of ice for each of these periods period. Histograms \figrefp{fig:mhist1} and summary statistics separated by period are constructed below.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> hist(days~period,data=LSI,ylab="Days of Ice Cover",w=20)
> Summarize(days~period,data=LSI,digits=2)
     period  n nvalid   mean    sd min Q1 median  Q3 max
1 post-1975 22     21 106.76 26.01  48 99  116.0 123 146
2  pre-1975 20     18 109.11 15.59  82 97  110.5 118 137
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.8\linewidth]{Figs/mhist1-1} 

}

\caption[Histograms of the duration of ice cover at ice gauge 9004 in Lake Superior by period]{Histograms of the duration of ice cover at ice gauge 9004 in Lake Superior by period.}\label{fig:mhist1}
\end{figure}


\end{knitrout}

Side-by-side boxplots \figrefp{fig:Boxplot1} are an alternative to separated histograms and are constructed by including the same formula and \R{data=} arguments to \R{boxplot()}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> boxplot(days~period,data=LSI,ylab="Days of Ice Cover",xlab="Period")
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/Boxplot1-1} 

}

\caption[Boxplot of the duration of ice cover at ice gauge 9004 in Lake Superior by period]{Boxplot of the duration of ice cover at ice gauge 9004 in Lake Superior by period.}\label{fig:Boxplot1}
\end{figure}


\end{knitrout}

Note that the formulae above required the grouping variable to be a factor. In some instances, a grouping variable may appear as an integer variable to R. For example, one may want to explore days of ice by decade, but the decade variable is not a factor variable.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> str(LSI)
'data.frame':	42 obs. of  5 variables:
 $ season: int  1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 ...
 $ decade: int  1950 1950 1950 1950 1950 1960 1960 1960 1960 1960 ...
 $ period: Factor w/ 2 levels "post-1975","pre-1975": 2 2 2 2 2 2 2 2 2 2 ...
 $ temp  : num  22.9 23 25.7 20 24.8 ...
 $ days  : int  87 137 106 97 105 118 118 136 91 NA ...
\end{verbatim}
\end{kframe}
\end{knitrout}

In these cases, the variale needs to be explicitly converted to a factor variable using \R{factor()}, as shown below. The use of \R{factor()} is not needed if R already recognizes the variable as a factor variable.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> LSI$decade <- factor(LSI$decade)
> str(LSI)
'data.frame':	42 obs. of  5 variables:
 $ season: int  1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 ...
 $ decade: Factor w/ 5 levels "1950","1960",..: 1 1 1 1 1 2 2 2 2 2 ...
 $ period: Factor w/ 2 levels "post-1975","pre-1975": 2 2 2 2 2 2 2 2 2 2 ...
 $ temp  : num  22.9 23 25.7 20 24.8 ...
 $ days  : int  87 137 106 97 105 118 118 136 91 NA ...
\end{verbatim}
\end{kframe}
\end{knitrout}



\chapter[Univ EDA Quantitative]{Univariate EDA - Quantitative} \label{chap:UnivEDAQuant2}

\minitoc
\vspace{40pt}

\lettrine{A}{ univariate EDA for a quantitative variable} is concerned with describing the distribution of values for that variable; i.e., describing what values occurred and how often those values occurred. Specifically, the distribution is described by four specific attributes:

\vspace{-12pt}
\begin{Enumerate}
  \item \textbf{shape} of the distribution,
  \item presence of \textbf{outliers},
  \item \textbf{center} of the distribution, and
  \item \textbf{dispersion} or spread of the distribution.
\end{Enumerate}
\vspace{-8pt}

Graphs are used to identify shape and the presence of outliers and to get a general feel for center and dispersion. Numerical summaries, however, are used to specifically describe center and dispersion of the variable. Computing and constructing the required numerical and graphical summaries was described in \modref{chap:UnivEDAQuant1}. Those summaries are interpreted here to provide an overall description of the distribution of the quantitative variable.

The same three data sets used in \modref{chap:UnivEDAQuant1} are used here.

\vspace{-12pt}
\begin{Itemize}
  \item Number of open pit mines in countries with open pit mines \tabrefp{tab:MineData}.
  \item Richter scale recordings for 15 major earthquakes \tabrefp{tab:EQData}.
  \item The number of days of ice cover at ice gauge station 9004 in Lake Superior.
\end{Itemize}

\section{Interpreting Shape}
A distribution has two tails -- a left-tail of smaller or more negative values and a right-tail of larger or more positive values \figrefp{fig:ShapeExamples1}. The relative appearance of these two tails is used to identify three different shapes of distributions -- symmetric, left-skewed, and right-skewed. If the left- and right-tail of a distribution are approximately equal in shape (length and height), then the distribution is said to be \textbf{symmetric} (or more specifically \textbf{approximately symmetric}). If the left-tail is stretched out or is longer and flatter than the right-tail, then the distribution is negatively- or \textbf{left-skewed}. If the right-tail is stretched out or is longer and flatter than the left-tail, then the distribution is positively- or \textbf{right-skewed}. The type of skew is defined by the longer tail; a longer right-tail means the distribution is right-skewed and a longer left-tail means it is left-skewed.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.9\linewidth]{Figs/ShapeExamples1-1} 

}

\caption[Examples of left-skewed (center), symmetric (left), and right-skewed (right) distributions]{Examples of left-skewed (center), symmetric (left), and right-skewed (right) distributions.}\label{fig:ShapeExamples1}
\end{figure}


\end{knitrout}

\warn{The longer tail defines the type of skew.}

In practice, these labels form a continuum. For example, it may be difficult to discern whether the shape approximately symmetric or one of the skewed distributions. To partially address this issue, ``slightly'' or ``strongly'' may be used with ``skewed'' to distinguish whether the distribution is obviously skewed (i.e., ``strongly skewed'') or nearly symmetric (i.e., ``slightly skewed'').

\warn{Symmetric, left-skewed, and right-skewed descriptors are guides; many ``real'' distributions will not fall neatly into these categories.}

The shape of a distribution is most easily identified from a histogram. Histograms that are examples of each shape are in \figref{fig:ShapeExamples2}. For the sets of skewed distributions, the distributions are less strongly skewed from left-to-right.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.7\linewidth]{Figs/ShapeExamples2-1} 

}

\caption[Examples of approximately symmetric (top, red), left-skewed (middle, blue), and right-skewed (bottom, green) histograms]{Examples of approximately symmetric (top, red), left-skewed (middle, blue), and right-skewed (bottom, green) histograms. Note that the axes labels were removed to focus on the shape of the histograms.}\label{fig:ShapeExamples2}
\end{figure}


\end{knitrout}

The shape of a distribution can also be determined from a boxplot. The relative length from the median to Q1 and the median to Q3 (i.e., the relative position of the median line in the box) indicates the shape of the distribution. If the distribution is left-skewed (i.e., lesser-valued individuals are ``spread out''; \figref{fig:BoxplotShape}-Right), then median-Q1 will be greater than Q3-median. In contrast, if the distribution is right-skewed (i.e., larger-valued individuals are spread out; \figref{fig:BoxplotShape}-Middle), then Q3-median will be greater than median-Q1. Thus, the median is nearer the top of the box for a left-skewed distribution, nearer the bottom of the box for a right-skewed distribution, and nearer the center of the box for a symmetric distribution \figrefp{fig:BoxplotShape}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.25\linewidth]{Figs/BoxplotShape-1} 
\includegraphics[width=.25\linewidth]{Figs/BoxplotShape-2} 
\includegraphics[width=.25\linewidth]{Figs/BoxplotShape-3} 
\includegraphics[width=.25\linewidth]{Figs/BoxplotShape-4} 
\includegraphics[width=.25\linewidth]{Figs/BoxplotShape-5} 
\includegraphics[width=.25\linewidth]{Figs/BoxplotShape-6} 

}

\caption[Histograms and boxplots for several different shapes of distributions]{Histograms and boxplots for several different shapes of distributions.}\label{fig:BoxplotShape}
\end{figure}


\end{knitrout}

\warn{Even though shape can be described from a boxplot, it is always easier to describe shape from a histogram.}

\section{Interpreting Outliers}
An outlier is an individual whose value is widely separated from the main cluster of values in the sample. On histograms, outliers appear as bars that are separated from the main cluster of bars by ``white space'' or areas with no bars \figrefp{fig:OutlierExHist}. In general, outliers must be \textbf{on the margins of the histogram, should be separated by one or two missing bars, and should only be one or two individuals.}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/OutlierExHist-1} 

}

\caption[Example histogram with an outlier to the right]{Example histogram with an outlier to the right.}\label{fig:OutlierExHist}
\end{figure}


\end{knitrout}

An outlier may be a result of human error in the sampling process. If this is the case, then the value should be corrected or removed. Other times an outlier may be an individual that was not part of the population of interest -- e.g., an adult animal that was sampled when only immature animals were being considered. In this case, the individual should be removed from the sample. Still other times, an outlier is part of the population and should generally not be removed from the sample. In fact you may wish to highlight an outlier as an interesting observation! Regardless, it is important that you construct a histogram to determine if outliers are present or not.

Don't let outliers completely influence how you define the shape of a distribution. For example, if the main cluster of values is approximately symmetric and there is one outlier to the right of the main cluster (as illustrated in \figref{fig:OutlierExHist}), \textbf{DON'T} call the distribution right-skewed. You should describe this distribution as approximately symmetric with an outlier to the right.

\warn{Not all outliers warrant removal from your sample.}

\vspace{-12pt}
\warn{Don't let outliers completely influence how you define the shape of a distribution.}



\section{Comparing the Median and Mean} \label{sect:MeanMedian}
As mentioned previously, numerical measures will be used to describe the center and dispersion of a distribution. However, which values should be used? Should one use the mean or the median as a measure of center? Should one use the IQR or the standard deviation as a measure of dispersion? Which measures are used depends on how the measures respond to skew and the presence of outliers. Thus, before stating a rule for which measures should be used, a fundamental difference among the measures discussed in \modref{chap:UnivEDAQuant1} is explored here.

The following discussion is focused on comparing the mean and the median. However, note that the IQR is fundamentally linked to the median (i.e., to find the IQR, the median must first be found) and the standard deviation is fundamentally linked to the mean (i.e., to find the standard deviation, the mean must first be found). Thus, \textbf{the median and IQR will always be used together to measure center and dispersion, as will the mean and standard deviation.}

The mean and median measure center in different ways. The median balances the number of individuals smaller and larger than it. The mean, on the other hand, balances the sum of the distances from it to all points smaller than it and the sum of the distances from it to all points greater than it. Thus, the median is primarily concerned with the \textbf{position} of the value rather than the value itself, whereas the mean is very much concerned about the \textbf{values} for each individual (i.e., the values are used to find the ``distance'' from the mean).

\warn{The actual values of the data (beyond ordering the data) are not considered when calculating the median; whereas the actual values are very much considered when calculating the mean.}

A plot of the Richter scale data against the corresponding ordered individual number is shown in \figref{fig:MeanMedianComp1}-Left.\footnote{This is a rather non-standard graph but it is useful for comparing how the mean and median measure the center of the data.}  The median (blue line) is found by locating the middle position on the individual number axis and then finding the corresponding Richter scale value (move right until the point is intercepted and then move down to the x-axis). The vertical blue line represents the median; i.e., it has the same \textbf{number} of individuals (i.e., points) above and below it. In contrast, the mean finds the Richter scale value that has the same total distance to values below it as total distance to values above it. In other words, the mean is the vertical red line placed such that the total \textbf{length} of the horizontal dashed red lines is the same to the left as it is to the right. Thus, the median balances the number of individuals above and below the median, whereas the mean balances the total difference in values above and below the mean.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.45\linewidth]{Figs/MeanMedianComp1-1} 
\includegraphics[width=.45\linewidth]{Figs/MeanMedianComp1-2} 

}

\caption[Plot of the individual number versus Richter scale values for the original earthquake data (\textbf{Left}) and the earthquake data with an extreme outlier (\textbf{Right})]{Plot of the individual number versus Richter scale values for the original earthquake data (\textbf{Left}) and the earthquake data with an extreme outlier (\textbf{Right}). The median value is shown as a blue vertical line and the mean value is shown as a red vertical line. Differences between each individual value and the mean value are shown with horizontal red lines.}\label{fig:MeanMedianComp1}
\end{figure}


\end{knitrout}

\warn{The mean balances the distance to individuals above and below the mean. The median balances the number of individuals above and below the median.}

\vspace{-12pt}
\warn{The sum of all differences between individual values and the mean (as properly calculated) equals zero.}

The mean and median differ in their sensitivity to outliers (\figref{fig:MeanMedianComp1}-Right). For example, suppose that an incredible earthquake with a Richter Scale value of 19.0 was added to the earthquake data set. With this additional individual, the median increases from 7.1 to 7.2, but the mean increases from 7.1 to 7.8. The outlier impacts the value of the mean more than the value of the median because of the way that each statistic measures center. The mean will be pulled towards an outlier because it must ``put'' many values on the ``side'' of the mean away from the outlier so that the sum of the differences to the larger values and the sum of the differences to the smaller values will be equal. In this example, the outlier creates a large difference to the right of the mean such that the mean has to ``move'' to the right to make this difference smaller, move more individuals to the left side of the mean, and increase the differences of individuals to the left of the mean to balance this one large individual. The median on the other hand will simply ``put'' one more individual on the side opposite of the outlier because it balances the number of individuals on each side of it. Thus, the median has to move very little to the right to accomplish this balance.

\warn{The mean is more sensitive (i.e., changes more) to outliers than the median; it will be ``pulled'' towards the outlier more than the median.}

The shape of the distribution, even if outliers are not present, also has an impact on the mean and median \figrefp{fig:MeanMedianShape}. If a distribution is approximately symmetric, then the median and mean (along with the mode) will be nearly identical. If the distribution is left-skewed, then the mean will be less than the median. Finally, if the distribution is right-skewed, then the mean will be greater than the median.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.3\linewidth]{Figs/MeanMedianShape-1} 
\includegraphics[width=.3\linewidth]{Figs/MeanMedianShape-2} 
\includegraphics[width=.3\linewidth]{Figs/MeanMedianShape-3} 

}

\caption[Three differently shaped histograms with vertical lines superimposed at the median (M]{Three differently shaped histograms with vertical lines superimposed at the median (M; blue lines) and the mean ($\bar{x}$; red lines).}\label{fig:MeanMedianShape}
\end{figure}


\end{knitrout}

\warn{The mean is pulled towards the long tail of a skewed distribution. Thus, the mean is greater than the median for right-skewed distributions and the mean is less than the median for left-skewed distributions.}

As shown above, the mean and median measure center in different ways. The question now becomes ``which measure of center is better?''  The median is a ``better'' measure of center when outliers are present. In addition, the median gives a better measure of a typical individual when the data are skewed. Thus, in this course, the median is used when outliers are present or the distribution of the data is skewed. If the distribution is symmetric, then the purpose of the analysis will dictate which measure of center is ``better.''  However, in this course, use the mean when the data are symmetric or, at least, not strongly skewed.

As note above, the IQR and standard deviation behave similarly to the median and mean, respectively, in the face of outliers and skews. Specifically, the IQR is less sensitive to outliers than the standard deviation.


\section{Synthetic Interpretations}
The graphical and numerical summaries from \modref{chap:UnivEDAQuant1} and the rationale described above can be used to construct a synthetic description of the shape, outliers, center, and dispersion of the distribution of a quantitative variable. In the examples below specifically note the 1) reference to figures and tables, 2) labeling of the figures and tables, 3) that only the mean and standard deviation or the median and IQR are discussed, 4) the range was not used alone as a measure of dispersion, 5) the explanation for why either the median and IQR or the mean and standard deviation were used, and 6) an appendix of R code used was provided.

\subsubsection{Number of Open Pit Mines}
\begin{quote}
\textit{Construct a proper EDA for the following situation and data -- ``The number of open pit mines in countries that have open pit mines \tabrefp{tab:MineData}.''}
\end{quote}
\vspace{-12pt}

The number of open pit mines in countries with open pit mines is strongly right-skewed with no outliers present \figrefp{fig:MineHist2}. [\textit{I did not call the group of four countries with 10 or more open pit mines outliers because there were more than one or two countries there.}] The center of the distribution is best measured by the median, which is 2 \tabrefp{tab:MineStats}. The range of open pit mines in the sample is from 1 to 15 while the dispersion as measured by the inter-quartile range (IQR) is from a Q1 of 1.0 to a Q3 of  4.0 \tabrefp{tab:MineStats}. I chose to use the median and IQR because the distribution was strongly skewed.

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:28 2017
\begin{table}[ht]
\centering
\caption{Descriptive statistics of number of open pit mines in countries with open pit mines.} 
\label{tab:MineStats}
\begin{tabular}{rrrrrrrr}
  \hline
n & mean & sd & min & Q1 & median & Q3 & max \\ 
  \hline
26.0 & 3.6 & 4.0 & 1.0 & 1.0 & 2.0 & 4.0 & 15.0 \\ 
   \hline
\end{tabular}
\end{table}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/MineHist2-1} 

}

\caption[Histogram of number of open pit mines in countries with open pit mines]{Histogram of number of open pit mines in countries with open pit mines.}\label{fig:MineHist2}
\end{figure}


\end{knitrout}

\begin{minipage}{\textwidth}
R Code Appendix:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
setwd("c:/data/")
mc <- read.csv("MineData.csv")
str(mc)
Summarize(~mines,data=mc,digits=1)
hist(~mines,data=mc,w=2,xlab="Number of open pit mines")
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}


\subsubsection{Lake Superior Ice Cover}
\begin{quote}
\textit{Thoroughly describe the distribution of number of days of ice cover at ice gauge station 9004 in Lake Superior (data are in \href{https://raw.githubusercontent.com/droglenc/NCData/master/LakeSuperiorIce.csv}{LakeSuperiorIce.csv}).}
\end{quote}



The shape of number of days of ice cover at gauge 9004 in Lake Superior is approximately symmetric with no obvious outliers \figrefp{fig:LSIHist}. The center is at a mean of 107.8 days and the dispersion is a standard deviation of 21.6 days \tabrefp{tab:LSIStats}. The mean and standard deviation were used because the distribution was not strongly skewed and no outlier was present.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/LSIHist-1} 

}

\caption[Histogram of number of days of ice cover at ice gauge 9004 in Lake Superior]{Histogram of number of days of ice cover at ice gauge 9004 in Lake Superior.}\label{fig:LSIHist}
\end{figure}


\end{knitrout}

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:28 2017
\begin{table}[ht]
\centering
\caption{Descriptive statistics of number of days of ice cover at ice gauge 9004 in Lake Superior..} 
\label{tab:LSIStats}
\begin{tabular}{rrrrrrrrr}
  \hline
n & nvalid & mean & sd & min & Q1 & median & Q3 & max \\ 
  \hline
42.0 & 39.0 & 107.8 & 21.6 & 48.0 & 97.0 & 114.0 & 118.0 & 146.0 \\ 
   \hline
\end{tabular}
\end{table}


\begin{minipage}{\textwidth}
R Appendix:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
setwd("c:/data/")
LSI <- read.csv("LakeSuperiorIce.csv")
str(LSI)
hist(~days,data=LSI,xlab="Day of Ice Cover",ylab="Frequency of Years",w=20)
Summarize(~days,data=LSI,digits=1)
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}

\subsubsection{Crayfish Temperature Selection}
\begin{quote}
\textit{Peck (1985) examined the temperature selection of dominant and subdominant crayfish (\textit{Orconectes virilis}) together in an artificial stream. The temperature ($^{o}$C) selection by the dominant crayfish in the presence of subdominant crayfish in these experiments was recorded below. Thoroughly describe all aspects of the distribution of selected temperatures.}
\end{quote}

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:28 2017
\begin{tabular}{rrrrrrrrrrrrrrrr}
  30 & 26 & 26 & 26 & 25 & 25 & 25 & 25 & 25 & 24 & 24 & 24 & 24 & 24 & 24 & 23 \\ 
  23 & 23 & 23 & 22 & 22 & 22 & 22 & 21 & 21 & 21 & 20 & 20 & 19 & 19 & 18 & 16 \\ 
  \end{tabular}


The shape of temperatures selected by the dominant crayfish is slightly left-skewed \figrefp{fig:CrayfishTempHist} with a possible weak outlier at the maximum value of 30$^{o}$C \tabrefp{tab:CrayfishTempStats}. The center is best measured by the median, which is 23$^{o}$C \tabrefp{tab:CrayfishTempStats} and the dispersion is best measured by the IQR, which is from 21 to 25$^{o}$C \tabrefp{tab:CrayfishTempStats}. I used the median and IQR because of the (combined) skewed shape and outlier present.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/CrayfishTempHist-1} 

}

\caption[Histogram of crayfish temperature preferences]{Histogram of crayfish temperature preferences.}\label{fig:CrayfishTempHist}
\end{figure}


\end{knitrout}

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:28 2017
\begin{table}[ht]
\centering
\caption{Descriptive statistics of crayfish temperature preferences.} 
\label{tab:CrayfishTempStats}
\begin{tabular}{rrrrrrrr}
  \hline
n & mean & sd & min & Q1 & median & Q3 & max \\ 
  \hline
32.00 & 22.88 & 2.79 & 16.00 & 21.00 & 23.00 & 25.00 & 30.00 \\ 
   \hline
\end{tabular}
\end{table}


\begin{minipage}{\textwidth}
R Appendix:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
setwd("c:/data/")
cray <- read.csv("Crayfish.csv")
str(cray)
hist(~temp,data=cray,xlab="Preferred Temperature",ylab="Frequency of Crayfish",w=2)
Summarize(~temp,data=cray,digits=2)
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}



\chapter{Univariate EDA - Categorical} \label{chap:UnivEDACat}

\vspace{-24pt}
\minitoc
\vspace{12pt}

\lettrine{I}{nterpreting summaries of a} single categorical variable is more intuitive and less defined than that for quantitative data. Specifically, one DOES NOT describe shape, center, dispersion, and outliers for categorical data. In this module, methods to construct tables and graphs for categorical data are described and the interpretation of the results demonstrated.

\warn{Do not describe shape, center, dispersion, and outliers for a categorical variable.}

These concepts are illustrated with three data sets. First, data recorded about MTH107 students in the Winter 2010 semester will be used. Specifically, whether or not a student was required to take the courses and the student's year-in-school will be summarized. Whether or not a student was required to take the course for a subset of individuals is shown in \tabref{tab:MTH107Subset}.

\begin{table}[htbp]
  \caption{Whether (Y) or not (N) MTH107 was required for eight individuals in MTH107 in Winter 2010.}
  \label{tab:MTH107Subset}
  \centering
  \begin{Verbatim}[xleftmargin=10mm]
Individual  1  2  3  4  5  6  7  8
Required    Y  N  N  Y  Y  Y  N  Y
  \end{Verbatim}
\end{table}
\vspace{-12pt}

Second, the General Sociological Survey (GSS) is a very large survey that has been administered 25 times since 1972. The purpose of the GSS is to gather data on contemporary American society in order to monitor and explain trends in attitudes, behaviors, and attributes. One question that was asked in a recent GSS was ``How often do you make a special effort to sort glass or cans or plastic or papers and so on for recycling?''  These data are found in the \var{recycle} variable in \href{https://raw.githubusercontent.com/droglenc/NCData/master/GSSEnviroQues.csv}{GSSEnviroQues.csv}.


\section{Summaries}
\subsection{Frequency and Percentage Tables}
A simple method to summarize categorical data is to count the number of individuals in each level of the categorical variable. These counts are called frequencies and the resulting table \tabrefp{tab:MTH107SubsetFreq} is called a frequency table. From this table, it is seen that there were five students that were required and three that were not required to take MTH107.

\begin{table}[htbp]
  \caption{Frequency table for whether MTH107 was required (Y) or not (N) for eight individuals in MTH107 in Winter 2010.}
  \label{tab:MTH107SubsetFreq}
  \centering
  \begin{Verbatim}[xleftmargin=15mm]
Required  Freq
    Y       5
    N       3
  \end{Verbatim}
\end{table}

The remainder of this module will use results from the entire class rather than the subset used above. For example, frequency tables of individuals by sex and year-in-school for the entire class are in \tabref{tab:Mth107Freq}.

\begin{table}[htbp]
  \caption{Frequency tables for whether (Y) or not (N) MTH107 was required (Left) and year-in-school (Right) for all individuals in MTH107 in Winter 2010.}
  \label{tab:Mth107Freq}
  \centering
  \begin{Verbatim}[xleftmargin=15mm]
Required  Freq          Year  Freq
    Y      38            Fr    19
    N      30            So    12
                         Jr    29
                         Sr     9
   \end{Verbatim}
\end{table}

Frequency tables are often modified to show the percentage of individuals in each level. \textbf{Percentage tables} are constructed from frequency tables by dividing the number of individuals in each level by the total number of individuals examined ($n$) and then multiplying by 100. For example, the percentage tables for both whether or not MTH107 was required and year-in-school \tabrefp{tab:Mth107Prop} for students in MTH107 is constructed from \tabref{tab:Mth107Freq} by dividing the value in each cell by 68, the total number of students in the class, and then multiplying by 100. From this it is seen that 55.9\% of students were required to take the course and 13.2\% were seniors \tabrefp{tab:Mth107Prop}.

\begin{table}[htbp]
  \caption{Percentage tables for whether (Y) or not (N) MTH107 was required (Left) and year-in-school (Right) for all individuals in MTH107 in Winter 2000.}
  \label{tab:Mth107Prop}
  \centering
  \begin{Verbatim}[xleftmargin=5mm]
Required   Perc          Year   Perc
   Y       55.9           Fr    27.9
   N       44.1           So    17.6
                          Jr    42.6
                          Sr    13.2
  \end{Verbatim}
\end{table}

\subsection{Bar Plots}
Bar plots, or bar charts, are used to display the frequency or percentage of individuals in each level of a categorical variable. Bar plots look similar to histograms in that they have the frequency of individuals on the y-axis. However, category labels rather than quantitative values are plotted on the x-axis. In addition, to highlight the categorical nature of the data, bars on a bar plot do not touch. A bar plot for whether or not individuals were required to take MTH107 is in \figref{fig:MTH107BarChart}-Left. This bar plot does not add much to the frequency table because there were only two categories. However, bar plots make it easier to compare the number of individuals in each of several categories as in \figref{fig:MTH107BarChart}-Right.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/MTH107BarChart-1} 
\includegraphics[width=.4\linewidth]{Figs/MTH107BarChart-2} 

}

\caption[Bar charts of the frequency of individuals in MTH107 during Winter 2010 by whether or not they were required to take MTH107 (\textbf{Left}) and year-in-school (\textbf{Right})]{Bar charts of the frequency of individuals in MTH107 during Winter 2010 by whether or not they were required to take MTH107 (\textbf{Left}) and year-in-school (\textbf{Right}).}\label{fig:MTH107BarChart}
\end{figure}


\end{knitrout}

\warn{Bar charts are used to display the frequency of individuals in the categories of a categorical variable. Histograms are used to display the frequency of individuals in classes created from quantitative variables.}


\subsection{Using in R}
The General Sociological Survey (GSS) data are loaded, the structure of the data.frame is examined, and the levels of the \var{recycle} variable are shown below. These results show the five levels in the \var{recycle} factor variable, ordered alphabetically as is the default in R. However, the levels should be ``Always'', ``Often'', ``Sometimes'', ``Never'', and ``Not Avail'' to follow the natural order of this ordinal variable.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> GSS <- read.csv("data/GSSEnviroQues.csv")
\end{verbatim}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> str(GSS)
'data.frame':	3539 obs. of  2 variables:
 $ recycle: Factor w/ 5 levels "Always","Never",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ tempgen: Factor w/ 5 levels "Extremely","Not",..: 1 1 1 1 1 1 1 1 1 1 ...
> levels(GSS$recycle)
[1] "Always"    "Never"     "Not Avail" "Often"     "Sometimes"
\end{verbatim}
\end{kframe}
\end{knitrout}

The order of a factor variable is controlled by including the ordered level names within a vector given to \R{levels=} in \R{factor()}. The names of the levels in this vector must be exactly as they appear in the original variable and they must be contained within quotes. The levels of \var{recycle} were reordered below. The advantage of correcting this order is that when the summary table is made, the order will follow the natural order of the variable rather than the alphabetical order.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> lvls <- c("Always","Often","Sometimes","Never","Not Avail")
> GSS$recycle <- factor(GSS$recycle,levels=lvls)
> levels(GSS$recycle)
[1] "Always"    "Often"     "Sometimes" "Never"     "Not Avail"
\end{verbatim}
\end{kframe}
\end{knitrout}

\warn{When changing the order of the levels with the \R{levels=} argument, the level names must be contained within quotes and they must be spelled exactly as they were spelled in the original variable.}

A frequency table of a single categorical variable is computed with \R{xtabs()}, where the first argument is a one-sided formula of the form \R{\TILDE var} and the corresponding data.frame is in \R{data=}. The result from \R{xtabs()} should be assigned to an object for further use. For example, the frequency table is produced, stored in \R{tabRecycle}, and displayed below. Thus, 1289 respondents answered ``Always'' to the recycling question.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( tabRecycle <- xtabs(~recycle,data=GSS) )
recycle
   Always     Often Sometimes     Never Not Avail 
     1289       850       823       448       129 
\end{verbatim}
\end{kframe}
\end{knitrout}

A percentage table is computed by including the saved frequency table as the first argument to \R{percTable()}.\footnote{Thus, \R{xtabs()} must be completed and saved to an object before \R{percTable()}.} The number of digits of output is controlled with \R{digits=}. Thus, 36.4\% of respondents answered ``Always'' to the recycling question.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> percTable(tabRecycle,digits=1)
recycle
   Always     Often Sometimes     Never Not Avail 
     36.4      24.0      23.3      12.7       3.6 
\end{verbatim}
\end{kframe}
\end{knitrout}

A bar plot is produced by giving the saved \R{xtabs()} object as the first argument to \R{barplot()}. The x- and y-axes may be explicitly labeled with \R{xlab=} and \R{ylab=}, respectively. For example, the bar plot for the recycling data \figrefp{fig:Barchart1} is produced below.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> barplot(tabRecycle,ylab="Frequency",xlab="Recycle Response")
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.55\linewidth]{Figs/Barchart1-1} 

}

\caption[Bar chart of the frequency of responses to the recycling question on the GSS]{Bar chart of the frequency of responses to the recycling question on the GSS.}\label{fig:Barchart1}
\end{figure}


\end{knitrout}


\section{Example Interpretations}
For categorical data, an appropriate EDA consists of identifying the major characteristics among the categories. Shape, center, dispersion, and outliers are NOT described for categorical data because the data is not numerical and, if nominal, no order exists. In general, the major characteristics of the table or graph are described from an intuitive basis. For example, there were more males than females in the Winter 2010 MTH107 class and mostly juniors and Freshmen. Other examples are below.

\subsection{Mixture Seed Count}
\begin{quote}
\textit{A bag of seeds was purchased for seeding a recently constructed wetland. The purchaser wanted to determine if the percentage of seeds in four broad categories -- ``grasses'', ``sedges'', ``wildflowers'', and ``legumes'' -- was similar to what the seed manufacturer advertised. The purchaser examined a 0.25-lb sample of seeds from the bag and recorded the results in \href{https://raw.githubusercontent.com/droglenc/NCData/master/WetlandSeeds.csv}{WetlandSeeds.csv}. Use these data to describe the distribution of seed counts into the four broad categories.}
\end{quote}



The majority of seeds were either sedge or grass with sedge being more than twice as abundant as grass (\tabref{tab:SeedTable}; \figref{fig:SeedBarplot}). Very few legumes or wildflowers were found in the sample.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.5\linewidth]{Figs/SeedBarplot-1} 

}

\caption[Barplot of the percentage of wetland seeds by type]{Barplot of the percentage of wetland seeds by type.}\label{fig:SeedBarplot}
\end{figure}


\end{knitrout}

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:28 2017
\begin{table}[ht]
\centering
\caption{Percentage distribution of wetland seeds by type.} 
\label{tab:SeedTable}
\begin{tabular}{rrrr}
  \hline
grass & legume & sedge & wildflower \\ 
  \hline
27.9 & 1.6 & 64.5 & 6.0 \\ 
   \hline
\end{tabular}
\end{table}


\begin{minipage}{\textwidth}
R Appendix:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
ws <- read.csv("data/WetlandSeeds.csv")
str(ws)
wtbl <- xtabs(~type,data=ws)
percTable(wtbl,digits=1)
barplot(wptbl[-5],ylab="Percentage of Total Seeds",xlab="Seed Type")
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}



\chapter{Normal Distribution}  \label{chap:NormDist}

\vspace{-24pt}
\minitoc
\vspace{24pt}

\lettrine{A}{ model for the distribution} of a single quantitative variable can be visualized by ``fitting'' a smooth curve to a histogram (\figref{fig:NormDensityEx}-Left), removing the histogram (\figref{fig:NormDensityEx}-Center), and using the remaining curve (\figref{fig:NormDensityEx}-Right) as a model for the distribution of the entire population of individuals. The smooth red curve drawn over the histogram serves as a model for the distribution of the \textbf{entire population}. If the smooth curve follows a known distribution, then certain calculations are greatly simplified.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.95\linewidth]{Figs/NormDensityEx-1} 

}

\caption[Depiction of fitting a smooth curve to a histogram to serve as a model for the distribution]{Depiction of fitting a smooth curve to a histogram to serve as a model for the distribution.}\label{fig:NormDensityEx}
\end{figure}


\end{knitrout}

The normal distribution is one of the most important distributions in statistics because it serves as a model for the distribution of individuals in many natural situations and the distribution of statistics from repeated samplings (i.e., sampling distributions).\footnote{See \modref{chap:SamplingDist}.}  The use of a normal distribution model to make certain calculations is demonstrated in this module.


\section{Characteristics of a Normal Distribution}
The normal distribution is the familiar bell-shaped curve (\figref{fig:NormDensityEx}-Right). Normal distributions have two parameters -- the population mean, $\mu$, and the population standard deviation, $\sigma$ -- that control the exact shape and position of the distribution. Specifically, the mean $\mu$ controls the center and the standard deviation $\sigma$ controls the dispersion of the distribution \figrefp{fig:NormMultDists}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.8\linewidth]{Figs/NormMultDists-1} 

}

\caption[Nine normal distributions]{Nine normal distributions. Distributions with the same line type have the same value of $\mu$ (solid is $\mu$=0, dashed is $\mu$=2, dotted is $\mu$=5). Distributions with the same color have the same value of $\sigma$ (black is $\sigma$=0.5, red is $\sigma$=1, and green is $\sigma$=2).}\label{fig:NormMultDists}
\end{figure}


\end{knitrout}

There are an infinite number of normal distributions because there are an infinite number of combinations of $\mu$ and $\sigma$. However, each normal distribution will
\begin{Enumerate}
  \item be bell-shaped and symmetric,
  \item centered at $\mu$,
  \item have inflection points at $\mu \pm \sigma$, and
  \item have a total area under the curve equal to 1.
\end{Enumerate}

If a generic variable $X$ follows a normal distribution with a mean of $\mu$ and a standard deviation of $\sigma$, then it is said that $X\sim N(\mu,\sigma)$. For example, if the heights of students ($H$) follows a normal distribution with a $\mu$ of 66 and a $\sigma$ of 3, then it is said that $H\sim N(66,3)$. As another example, $Z\sim N(0,1)$ means that the variable $Z$ follows a normal distribution with a mean of $\mu$=0 and a standard deviation of $\sigma$=1.


\section{Simple Areas Under the Curve}
A common problem is to determine the proportion of individuals with a value of the variable between two numbers. For example, you might be faced with determining the proportion of all sites that have lead concentrations between 1.2 and 1.5 $\mu g \cdot m^{-3}$, the proportion of students that scored higher than 700 on the SAT, or the proportion of Least Weasels that are shorter than 150 mm. Before considering these more realistic situations, we explore calculations for the generic variable $X$ shown in \figref{fig:NormDistShade}.

Let's consider finding the proportion of individuals in a \textit{sample} with values between 0 and 2. A histogram can be used to answer this question because it is about the individuals in a sample (\figref{fig:NormDistShade}-Left). In this case, the proportion of individuals with values between 0 and 2 is computed by dividing the number of individuals in the red shaded bars by the total number of individuals in the histogram. The analogous computation on the superimposed smooth curve is to find the area under the curve between 0 and 2 (\figref{fig:NormDistShade}-Right). The area under the curve is a ``proportion of the total'' because, as stated above, the area under the entire curve is equal to 1. The actual calculations on the normal curve are shown in the following sections. However, at this point, note that the calculation of an area on a normal curve is analogous to summing the number of individuals in the appropriate classes of the histogram and dividing by $n$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.8\linewidth]{Figs/NormDistShade-1} 

}

\caption[Depiction of finding the proportion of individuals between 0 and 2 on a histogram (\textbf{Left}) and on a standard normal distribution (\textbf{Right})]{Depiction of finding the proportion of individuals between 0 and 2 on a histogram (\textbf{Left}) and on a standard normal distribution (\textbf{Right}).}\label{fig:NormDistShade}
\end{figure}


\end{knitrout}

\warn{The proportion of individuals between two values of a variable that is normally distributed is the area under the normal distribution between those two values.}

The 68-95-99.7 (or Empirical) Rule states that 68\% of individuals that follow a normal distribution have values between $\mu-1\sigma$ and $\mu+1\sigma$, 95\% have values between $\mu-2\sigma$ and $\mu+2\sigma$, and 99.7\% have values between $\mu-3\sigma$ and $\mu+3\sigma$ \figrefp{fig:NormEmpiricalRule}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.5\linewidth]{Figs/NormEmpiricalRule-1} 

}

\caption[Depiction of the 68-95-99.7 (or Empirical) Rule on a normal distribution]{Depiction of the 68-95-99.7 (or Empirical) Rule on a normal distribution.}\label{fig:NormEmpiricalRule}
\end{figure}


\end{knitrout}

\vspace{12pt} % needed because spaced is gobbled up by LaTeX
The 68-95-99.7 Rule is true no matter what $\mu$ and $\sigma$ are as long as the distribution is normal. For example, if $A\sim N(3,1)$, then 68\% of the individuals will fall between 2 (i.e., 3-1*1) and 4 (i.e., 3+1*1) and 99.7\% will fall between 0 (i.e., 3-3*1) and 6 (i.e., 3+3*1). Alternatively, if $B\sim N(9,3)$, then 68\% of the individuals will fall between 6 (i.e., 9-1*3) and 12 (i.e., 9+1*3) and 95\% will be between 3 (i.e., 9-2*3) and 15 (i.e., 9+2*3). Similar calculations can be made for any normal distribution.

The 68-95-99.7 Rule is used to find areas under the normal curve as long as the value of interest is an \textbf{integer} number of standard deviations away from the mean. For example, the proportion of individuals that have a value of A greater than 5 \figrefp{fig:NormEmpiricalRuleCalc} is found by first realizing that 95\% of the individuals on this distribution fall between 1 and 5 (i.e., $\pm2\sigma$ from $\mu$). By subtraction this means that 5\% of the individuals must be less than 1 \textbf{AND} greater than 5. Finally, because normal distributions are symmetric, the same percentage of individuals must be less than 1 as are greater than 5. Thus, half of 5\%, or 2.5\%, of the individuals have a value of A greater than 5.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/NormEmpiricalRuleCalc-1} 

}

\caption[The N(3,1) distribution depicting how the 68-95-99.7 Rule is used to compute the percentage of individuals with values greater than 5]{The N(3,1) distribution depicting how the 68-95-99.7 Rule is used to compute the percentage of individuals with values greater than 5.}\label{fig:NormEmpiricalRuleCalc}
\end{figure}


\end{knitrout}

\warn{The 68-95-99.7 Rule can only be used for questions involving \textbf{integer} standard deviations away from the mean.}


\section[Forward Calculations]{More Complex Areas (Forward Calculations)}
Areas under the curve relative to non-integer numbers of standard deviations away from the mean can only be found with the help of special tables or computer software. In this course, we will use R.

The area under a normal curve relative to a particular value is computed in R with \R{distrib()}. This function requires the \textit{particular value} as the first argument and the mean and standard deviation of the normal distribution in the \R{mean=} and \R{sd=} arguments, respectively. The \R{distrib()} function defaults to finding the area under the curve to the \textbf{left of} the particular value, but it can find the area under the curve to the right of the particular value by including \R{lower.tail=FALSE}.

For example, suppose that the heights of a population of students is known to be $H\sim N(66,3)$. The proportion of students in this population that have a height less than 71 inches is computed below. Thus, approximately 95.2\% of students in this population have a height less than 71 inches \figrefp{fig:NormZCalc1}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(71,mean=66,sd=3) )
[1] 0.9522096
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/NormZCalc1-1} 

}

\caption[Calculation of the proportion of individuals on a $N(66,3)$ with a value less than 71]{Calculation of the proportion of individuals on a $N(66,3)$ with a value less than 71.}\label{fig:NormZCalc1}
\end{figure}


\end{knitrout}

The proportion of students in this population that have a height \textit{greater} than 68 inches is computed below (note use of \R{lower.tail=FALSE}). Thus, approximately 25.2\% of students in this population have a height greater than 68 inches \figrefp{fig:NormZCalc2}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(68,mean=66,sd=3,lower.tail=FALSE) )
[1] 0.2524925
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/NormZCalc2-1} 

}

\caption[Calculation of the proportion of individuals on a $N(66,3)$ with a value greater than 68]{Calculation of the proportion of individuals on a $N(66,3)$ with a value greater than 68.}\label{fig:NormZCalc2}
\end{figure}


\end{knitrout}

Finding the area between two particular values is a bit more work. To answer ``between''-type questions, the area less than the smaller of the two values is subtracted from the area less than the larger of the two values. This is illustrated by noting that two values split the area under the normal curve into three parts -- A, B, and C in \figref{fig:NormDistBetween}. The area between the two values is B. The area to the left of the larger value corresponds to the area A+B. The area to the left of the smaller value corresponds to the area A. Thus, subtracting the latter from the former leaves the ``in-between'' area B (i.e., (A+B)-A = B).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/NormDistBetween-1} 

}

\caption[Schematic representation of how to find the area between two $Z$ values]{Schematic representation of how to find the area between two $Z$ values.}\label{fig:NormDistBetween}
\end{figure}


\end{knitrout}
\vspace{12pt} % because the spacing is gobbled by the R code.

For example, the area between 62 and 70 inches of height is found below. Thus, 81.8\% of students in this population have a height between 62 and 70 inches.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( AB <- distrib(70,mean=66,sd=3) )  # left-of 70
[1] 0.9087888
> ( A <- distrib(62,mean=66,sd=3) )   # left-of 62
[1] 0.09121122
> AB-A                                # between 62 and 70
[1] 0.8175776
\end{verbatim}
\end{kframe}
\end{knitrout}

\warn{The area between two values is found by subtracting the area less than the smaller value from the area less than the larger value.}



\section[Reverse Calculations]{Values from Areas (Reverse Calculations)}
Another important calculation with normal distributions is finding the value or values of $X$ with a given proportion of individuals less than, greater than, or between. For example, it may be necessary to find the test score such that 90\% (or 0.90 as a proportion) of the students scored lower. In contrast to the calculations in the previous section (where the value of $X$ was given and a proportion of individuals was asked for), the calculations in this section give a proportion and ask for a value of $X$. These types of questions are called \textbf{``reverse'' normal distribution questions} to contrast them with questions from the previous section.

Reverse questions are also answered with \R{distrib()}, though the first argument is now the given proportion (or area) of interest. The calculation is treated as a ``reverse'' question when \R{type="q"} is given to \R{distrib()}.\footnote{``q'' stands for quantile.}  For example, the height that has 20\% of all students shorter is  63.5 inches, as computed below \figrefp{fig:NormZCalc4}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(0.20,mean=66,sd=3,type="q") )
[1] 63.47514
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/NormZCalc4-1} 

}

\caption[Calculation of the height with 20\% of all students shorter]{Calculation of the height with 20\% of all students shorter.}\label{fig:NormZCalc4}
\end{figure}


\end{knitrout}

``Greater than'' reverse questions are computed by including \R{lower.tail=FALSE}. For example, 10\% of the population of students is taller than 69.8 inches, as computed below \figrefp{fig:NormZCalc5}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(0.10,mean=66,sd=3,type="q",lower.tail=FALSE) )
[1] 69.84465
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/NormZCalc5-1} 

}

\caption[Calculation of the height with 10\% of all students taller]{Calculation of the height with 10\% of all students taller.}\label{fig:NormZCalc5}
\end{figure}


\end{knitrout}

``Between'' questions can only be easily handled if the question is looking for endpoint values that are symmetric about $\mu$. In other words, the question must ask for the two values that contain the ``most common'' proportion of individuals. For example, suppose that you were asked to find the most common 80\% of heights. This type of question is handled by converting this ``symmetric between'' question into two ``less than'' questions. For example, in \figref{fig:NormRevBetween} the area D is the symmetric area of interest. If D is 0.80, then C+E must be 0.20.\footnote{Because all three areas must sum to 1.}  Because D is symmetric about $mu$, C and E must both equal 0.10. Thus, the lower bound on D is the value that has 10\% of all values smaller. Similarly, because the combined area of C and D is 0.90, the upper bound on D is the value that has 90\% of all values smaller. This question has now been converted from a ``symmetric between'' to two ``less than'' questions that can be answered exactly as shown above. For example, the two heights that have a symmetric 80\% of individuals between them are 62.2 and 69.8 as computed below.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(0.10,mean=66,sd=3,type="q") )
[1] 62.15535
> ( distrib(0.90,mean=66,sd=3,type="q") )
[1] 69.84465
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/NormRevBetween-1} 

}

\caption[Depiction of areas in a reverse between type normal distribution question]{Depiction of areas in a reverse between type normal distribution question.}\label{fig:NormRevBetween}
\end{figure}


\end{knitrout}


\section{Distinguish Calculation Types}
It is critical to be able to distinguish between the two main types of calculations made from normal distributions. The first type of calculation is a ``forward'' calculation where the area or proportion of individuals relative to a value of the variable must be found. The second type of calculation is a ``reverse'' calculation where the value of the variable relative to a particular area is calculated.

Distinguishing between these two types of calculations is a matter of deciding if (i) the value of the variable is given and the proportion (or area) is to be found or (ii) if the proportion (or area) is given and the value of the variable is to be found. Therefore, distinguishing between the calculation types is as simple as identifying what is given (or known) and what must be found. If the value of the variable is given but not the proportion or area, then a forward calculation is used. If the area or proportion is given, then a reverse calculation to find the value of the variable is used.


\section{Standardization and Z-Scores}\label{sect:Standardizing}
An individual that is 59 inches tall is 7 inches shorter than average if heights are $N(66,3)$. Is this a large or a small difference?  Alternatively, this same individual is $\frac{-7}{3}$ = $-2.33$ standard deviations below the mean. Thus, a height of 59 inches is relatively rare in this population because few individuals are more than two standard deviations away from the mean.\footnote{From the 68-95-99.7\% Rule.} As seen here, the relative magnitude that an individual differs from the mean is better expressed as the number of standard deviations that the individual is away from the mean.

Values are ``standardized'' by changing the original scale (inches in this example) to one that counts the number of standard deviations (i.e., $\sigma$) that the value is away from the mean (i.e., $\mu$). For example, with the height variable above, 69 inches is one standard deviation above the mean, which corresponds to +1 on the standardized scale. Similarly, 60 inches is two standard deviations below the mean, which corresponds to -2 on the standardized scale. Finally, 67.5 inches on the original scale is one half standard deviation above the mean or +0.5 on the standardized scale.

The process of computing the number of standard deviations that an individual is away from the mean is called \textbf{standardizing}. Standardizing is accomplished with
\begin{equation}
  \label{eqn:Zgeneral}
    Z = \frac{\text{``value''}-\text{``center''}}{\text{``dispersion''}}
\end{equation}
or, more specifically,
\begin{equation}
  \label{eqn:Zspecific}
    Z = \frac{x-\mu}{\sigma}
\end{equation}
For example, the standardized value of an individual with a height of 59 inches is $z=\frac{59-66}{3}=-2.33$. Thus, this individual's height is 2.33 standard deviations below the average height in the population.

Standardized values ($Z$) follow a $N(0,1)$. Thus, $N(0,1)$ is called the ``standard normal distribution.''  The relationship between $X$ and $Z$ is one-to-one meaning that each value of $X$ converts to one and only one value of $Z$. This means that the area to the left of $X$ on a $N(\mu,\sigma)$ is the same as the area to the left of $Z$ on a $N(0,1)$. This one-to-one relationship is illustrated in \figref{fig:NormStandardizingEx} using the individual with a height of 59 inches and $Z=-2.33$.

\vspace{-6pt}
\warn{The standardized scale (i.e., z-scores) represents the number of standard deviations that a value is from the mean.}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/NormStandardizingEx-1} 
\includegraphics[width=.4\linewidth]{Figs/NormStandardizingEx-2} 

}

\caption{Plots depicting the area to the left of 59 on a $N(66,3)$ (\textbf{Left}) and the area to the right of the corresponding Z-score of $Z=-2.33$ on a $N(0,1)$ (\textbf{Right}). Not that the x-axis scales are different.}\label{fig:NormStandardizingEx}
\end{figure}


\end{knitrout}



\chapter{Bivariate EDA - Quantitative} \label{chap:BivEDAQuant}

\vspace{-30pt}
\minitoc
\vspace{18pt}

\lettrine{B}{ivariate data occurs when two} variables are measured on the same individuals. For example, you may measure (i) the height and weight of students in class, (ii) depth and area of a lake, (iii) gender and age of welfare recipients, or (iv) number of mice and biomass of legumes in fields. This module is focused on describing the bivariate relationship between two quantitative variables. Bivariate relationships between two categorical variables is described in \modref{chap:BivEDACat}.

Data on the \var{weight} (lbs) and highway miles per gallon (\var{HMPG}) for 93 cars from the 1993 model year are used as an example throughout this module. Ultimately, the relationship between highway MPG and the weight of a car is described. These data are read from \href{https://raw.githubusercontent.com/droglenc/NCData/master/93cars.csv}{93cars.csv} into R and several observations of \var{HMPG} and \var{weight} are shown below.\footnote{The vector in the second argument to \R{headtail()} is used to show only the two variables of interest.}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> cars93 <- read.csv("data/93cars.csv")
\end{verbatim}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> headtail(cars93,which=c("HMPG","Weight"))
   HMPG Weight
1    31   2705
2    25   3560
3    26   3375
91   25   2810
92   28   2985
93   28   3245
\end{verbatim}
\end{kframe}
\end{knitrout}



\section[Response and Explanatory] {Response and Explanatory Variables} \label{sect:RespExplan1}
\vspace{-3pt}
The \textbf{response variable} is the variable that one is interested in explaining something (i.e., variability) or in making future predictions about. The \textbf{explanatory variable} is the variable that may help explain or allow one to predict the response variable. In general, the response variable is thought to depend on the explanatory variable. Thus, the response variable is often called the \textbf{dependent variable}, whereas the explanatory variable is often called the \textbf{independent variable}.

One may identify the response variable by determining which of the two variables depends on the other. For example, in the car data, highway MPG is the response variable because gas mileage is most likely affected by the weight of the car (e.g., hypothesize that heavier cars get worse gas mileage), rather than vice versa.

In some situations it is not obvious which variable is the response. For example, does the number of mice in the field depend on the number of legumes (lots of feed=lots of mice) or the other way around (lots of mice=not much food left)? Similarly, does area depend on depth or does depth depend on area of the lake? In these situations, the context of the research question is needed to identify the response variable. For example, if the researcher hypothesized that number of mice will be greater if there is more legumes, then number of mice is the response variable. In many cases, the more difficult variable to measure will likely be the response variable. For example, researchers likely wish to predict area of a lake (hard to measure) from depth of the lake (easy to measure).

\vspace{-9pt}
\warn{Which variable is the response may depend on the context of the research question.}


\vspace{-12pt}
\section{Summaries}
\vspace{-6pt}
\subsection{Scatterplots} \label{sect:ScatterplotsR}
\vspace{-3pt}
A scatterplot is a graph where each point simultaneously represents the values of both the quantitative response and quantitative explanatory variable. The value of the explanatory variable gives the x-coordinate and the value of the response variable gives the y-coordinate of the point plotted for an individual. For example, the first individual in the cars data is plotted at x (\var{Weight}) = 2705 and y (\var{HMPG}) = 31, whereas the second individual is at x = 3560 and y = 25 \figrefp{fig:carscat2}.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/carscat2-1} 

}

\caption[Scatterplot between the highway MPG and weight of cars manufactured in 1993]{Scatterplot between the highway MPG and weight of cars manufactured in 1993. For reference to the main text, the first individual is red and the second individual is green.}\label{fig:carscat2}
\end{figure}


\end{knitrout}

Scatterplots are constructed in R with \R{plot()} with a formula of the form \R{Y\TILDE X}, where \R{Y} and \R{X} are variables to be plotted on the y- and x-axes, as the first argument, and the corresponding data.frame in \R{data=}. The x- and y-axis labels may be modified with \R{xlab=} and \R{ylab=}. The character plotted at each point can be changed with \R{pch=},\footnote{This argument is short for ``plotting character''.} which defaults to 1 or an open-circle \figrefp{fig:Rpch}. The scatterplot, excluding the two highlighted points, of highway MPG versus car weight \figrefp{fig:carscat2} was created with the code below.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(HMPG~Weight,data=cars93,xlab="Weight (lbs)",ylab="Highway MPG",pch=16)
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/Rpch-1} 

}

\caption[Plotting characters available in R and their numerical codes]{Plotting characters available in R and their numerical codes. Note that for values of 21-25 that \R{bg='gray70'} is used to provide the background color.}\label{fig:Rpch}
\end{figure}


\end{knitrout}


\subsection{Correlation Coefficient}\label{sect:corr}
The sample correlation coefficient, abbreviated as $r$, is calculated with
\begin{equation} \label{eqn:Correlation}
  r = \frac{\Sum_{i=1}^{n}\left[\left(\frac{x_{i}-\bar{x}}{s_{x}}\right)\left(\frac{y_{i}-\bar{y}}{s_{y}}\right)\right]}{n-1}
\end{equation}
where $s_{x}$ and $s_{y}$ are the sample standard deviations for the explanatory and response variables, respectively.\footnote{See \sectref{sect:StdDev} for a review of standard deviations.} The formulae in the two sets of parentheses in the numerator are standardized values;\footnote{See \sectref{sect:Standardizing} for a review of standardized values.} thus, the value in each parenthesis is called the standardized x or standardized y, respectively. Using this terminology, Equation \eqref{eqn:Correlation} reduces to these steps:
\begin{Enumerate}
  \item For each individual, standardize x and standardize y.
  \item For each individual, find the product of the standardized x and standardized y.
  \item Sum all of the products from step 2.
  \item Divide the sum from step 3 by n-1.
\end{Enumerate}

The table below illustrates these calculations for the first five individuals in the cars data.\footnote{The five cars are treated as if they are the entire sample.} Note that the ``i'' column is an index for each individual, the $x_{i}$ and $y_{i}$ columns are the observed values of the two variables for individual $i$, $\bar{x}$ was computed by dividing the sum of the $x_{i}$ column by $n$, $s_{x}$ was computed by dividing the sum of the $(x_{i}-\bar{x})^{2}$ column by $n-1$ and taking the square root, and the ``std x'' column are the standardized x values found by dividing the values in the $x_{i}-\bar{x}$ column by $s_{x}$. Similar calculations were made for the y variable. The final correlation coefficient is the sum of the last column divided by $n-1$. Thus, the correlation between car weight and highway mpg for these five cars is -0.54.

\begin{center}
  \begin{tabular}{cccccccccc}
\hline\hline
 & HMPG & Weight & & & & & & & \\
i & $y_{i}$ & $x_{i}$ & $y_{i}-\bar{y}$ & $x_{i}-\bar{x}$ & $(y_{i}-\bar{y})^{2}$ & $(x_{i}-\bar{x})^{2}$ & std. y & std. x & (std. y)(std. x) \\
\hline
1 & 31 & 2705 &  3.4 & -632 & 11.56 & 399424 &  1.26 & -1.71 & -2.15 \\
2 & 25 & 3560 & -2.6 &  223 &  6.76 &  49729 & -0.96 &  0.6  & -0.58 \\
3 & 26 & 3375 & -1.6 &   38 &  2.56 &   1444 & -0.59 &  0.1  & -0.06 \\
4 & 26 & 3405 & -1.6 &   68 &  2.56 &   4624 & -0.59 &  0.18 & -0.11 \\
5 & 30 & 3640 &  2.4 &  303 &  5.76 &  91809 &  0.89 &  0.82 &  0.73 \\
\hline
sum & 138 & 16685 & 0 & 0 & 29.2 & 547030 & 0 & 0 &  -2.17 \\
\hline\hline
  \end{tabular}
\end{center}

The meaning and interpretation of $r$ is discussed in more detail in \sectref{sect:BivEDAItems}.

The correlation coefficient ($r$) between two quantitative variables is computed with \R{corr()} using a formula of the form \R{Y\TILDE X} or \R{\TILDE Y+X}, where \R{Y} and \R{X} are the names of quantitative variables, as the first argument and the corresponding data.frame in \R{data=}. For example, the correlation coefficient between highway MPG and weight for all cars in the car data is -0.81.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> corr(HMPG~Weight,data=cars93)
[1] -0.8106581
> corr(~HMPG+Weight,data=cars93)  # alternative form
[1] -0.8106581
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Pairs of Multiple Variables}
\vspace{-3pt}

Correlation coefficients can be computed or scatterplots can be constructed simultaneously for all pairs of many quantitative variables. A matrix of correlation coefficients is constructed with \R{corr()} as above using a formula of the form \R{\TILDE X1+X2+X3} (and so on), where the \R{X1}, \R{X2}, etc. are all quantitative variables to be used. In some instances, the data.frame may contain missing values (i.e., data that were not recorded). The individuals with missing data are efficiently removed from the correlation matrix with \R{use="pairwise.complete.obs"} in \R{corr()}.\footnote{Missing data are automatically removed from the scatterplots.} The number of digits reported in the correlation matrix is controlled with \R{digits=}. For example, the correlation between highway MPG and size of the fuel tank is -0.786, whereas the correlation between length and weight of the car is 0.806.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> corr(~HMPG+FuelTank+Length+Weight,data=cars93,use="pairwise.complete.obs",digits=3)
           HMPG FuelTank Length Weight
HMPG      1.000   -0.786 -0.543 -0.811
FuelTank -0.786    1.000  0.690  0.894
Length   -0.543    0.690  1.000  0.806
Weight   -0.811    0.894  0.806  1.000
\end{verbatim}
\end{kframe}
\end{knitrout}

A matrix of scatterplots is constructed with \R{pairs()} using the same formula notation as in \R{corr()}. The plotting character can be changed, as with \R{plot()}, with \R{pch=}. Each subplot in the resulting scatterplot matrix \figrefp{fig:Scatplot4} is a scatterplot with the variable listed in the same column on the x-axis and the variable listed in the same row on the y-axis. For example, the scatterplot in the upper-right corner of \figref{fig:Scatplot4} has highway MPG on the y-axis and car weight on the x-axis.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> pairs(~HMPG+FuelTank+Length+Weight,data=cars93,pch=21,bg="gray70")
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.7\linewidth]{Figs/Scatplot4-1} 

}

\caption[Scatterplot matrix of the highway MPG, fuel tank size, length, and weight of cars]{Scatterplot matrix of the highway MPG, fuel tank size, length, and weight of cars.}\label{fig:Scatplot4}
\end{figure}


\end{knitrout}


\section{Items to Describe} \label{sect:BivEDAItems}
Four characteristics should be described for a bivariate EDA with two quantitative variables:
\vspace{-8pt}
\begin{Enumerate}
  \item \textbf{form} of the relationship,
  \item presence (or absence) of \textbf{outliers}, and
  \item \textbf{association} or \textbf{direction} of the relationship,
  \item \textbf{strength} of the relationship.
\end{Enumerate}
\vspace{-8pt}
All four of these items can be described from a scatterplot. However, for certain relationships (discussed below), strength is best described from the correlation coefficient.

\subsection{Form and Outliers}
The form of a relationship is determined by whether the ``cloud'' of points on a scatterplot forms a line or some sort of curve \figrefp{fig:corrassn}. For the purposes of this introductory course, if the ``cloud'' appears linear then the form will said to be linear, whereas if the ``cloud'' is curved then the form will be nonlinear. Scatterplots should be considered \textbf{linear} unless there is an OBVIOUS curvature in the points.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.3\linewidth]{Figs/forms-1} 
\includegraphics[width=.3\linewidth]{Figs/forms-2} 
\includegraphics[width=.3\linewidth]{Figs/forms-3} 

}

\caption[Depictions of two linear (Left and Center) and one nonlinear (Right) relationship]{Depictions of two linear (Left and Center) and one nonlinear (Right) relationship.}\label{fig:forms}
\end{figure}


\end{knitrout}

\vspace{12pt} % because it got gobbled up
An outlier is a point that is far removed from the main cluster of points. Keep in mind (as always) that just because a point is an outlier doesn't mean it is wrong.

\subsection{Association or Direction}
A positive association is when the scatterplot resembles an increasing function (i.e., increases from lower-left to upper-right; \figref{fig:corrassn}-Left). For a positive association, most of the individuals are above average or below average for both of the variables. A negative association is when the scatterplot looks like a decreasing function (i.e., decreases from upper-left to lower-right; \figref{fig:corrassn}-Right). For a negative association, most of the individuals are above average for one variable and below average for the other variable. No association is when the scatterplot looks like a ``shotgun blast'' of points (\figref{fig:corrassn}-Center). For no association, there is no tendency for individuals to be above or below average for one variable and above or below average for the other.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.3\linewidth]{Figs/corrassn-1} 
\includegraphics[width=.3\linewidth]{Figs/corrassn-2} 
\includegraphics[width=.3\linewidth]{Figs/corrassn-3} 

}

\caption[Depiction of three types of association present in scatterplots]{Depiction of three types of association present in scatterplots. Dashed vertical lines are at the means of each variable.}\label{fig:corrassn}
\end{figure}


\end{knitrout}

\subsection{Strength (and Association, Again)} \label{sect:CorrStrength}
Strength is a summary of how closely the points cluster about the general form of the relationship. For example, if a linear form exists, then strength is how closely the points cluster around the line. Strength is difficult to define from a scatterplot because it is a relative term. However, the correlation coefficient ($r$; \sectref{sect:corr}) is a measure of strength (and association) between two variables, \textit{if the form is linear}.

The sign of $r$ indicates the association between the two variables. A positive $r$ means a positive association and a negative $r$ means a negative association. The absolute value of $r$ (i.e., the value ignoring the sign) is an indicator of strength of relationship. Absolute values nearer 1 are stronger relationships.

To better understand how $r$ is a measure of association and strength, reconsider the steps in calculating $r$ from \sectref{sect:corr}. The scatterplots in \figref{fig:corrdefn1} represent a positive (Left) and negative (Right) association. These scatterplots have dashed lines at the mean of both the x- and y-axis variables. Because the mean is subtracted from observed values when standardizing, points that fall above the mean will have positive standardized values and points that fall below the mean will have negative standardized values. The sign for the standardized values are depicted along the axes.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.65\linewidth]{Figs/corrdefn1-1} 

}

\caption[Scatterplot with mean lines superimposed and the signs of standardized values for both x and y shown for a positive (\textbf{Left}) and negative (\textbf{Right}) association]{Scatterplot with mean lines superimposed and the signs of standardized values for both x and y shown for a positive (\textbf{Left}) and negative (\textbf{Right}) association. Blue points have a positive product of standardized values, whereas red points have a negative product of standardized values.}\label{fig:corrdefn1}
\end{figure}


\end{knitrout}

\vspace{-6pt}
Now consider the product of standardized x's and y's in each quadrant of the scatterplots in \figref{fig:corrdefn1}. The product of standardized values is positive (blue points) in the quadrant where both standardized values are above average (i.e., both positive signs) and both are below average. The product of standardized values is negative (red points) in the other two quadrants.

Thus, for a positive association (\figref{fig:corrdefn1}-Left) the numerator of the correlation coefficient is positive because it is the sum of many positive (blue points) and few negative (red points) products of standardized values. The denominator (recall that it is $n-1$) is always positive. Therefore, $r$ for a positive association is positive. Conversely, for a negative association (\figref{fig:corrdefn1}-Right) the numerator of the correlation coefficient is negative because it is the sum of few positive (blue points) and many negative (red points) products of standardized values. Therefore, $r$ for a negative association is negative.

Correlations range from -1 to 1. Absolute values of $r$ equal to 1 indicate a perfect association (i.e., all points exactly on a line). A correlation of 0 indicates no association. Thus, absolute values of $r$ near 1 indicate strong relationships and those near 0 are weak. How strength and association of the relationship changes along the range of $r$ values is illustrated in \figref{fig:corrstrength2}. Categorizations in \tabref{tab:StrengthCriteria} can be used as a guideline for describing the strength of relationship between two variables.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[h]

{\centering \includegraphics[width=.95\linewidth]{Figs/corrstrength2-1} 

}

\caption[Scatterplots along the continuum of $r$ values]{Scatterplots along the continuum of $r$ values.}\label{fig:corrstrength2}
\end{figure}


\end{knitrout}

\begin{table}[htbp]
  \caption{Classifications of strength of relationship for absolute values of $r$ by type of study.}
  \label{tab:StrengthCriteria}
  \centering
  \begin{tabular}{c|ccc}
\hline\hline
\widen{0}{5}{Strength of} & Uncontrolled/ & Controlled/ \\
\widen{-2}{0}{Relationship} & Observational & Experimental \\
\hline
\widen{0}{4}{Strong} & $>0.8$ & $>0.95$ \\
\widen{0}{4}{Moderate} & $>0.6$ & $>0.9$ \\
\widen{-1}{5}{Weak} & $>0.4$ & $>0.8$ \\
\hline\hline
  \end{tabular}
\vspace{36pt} % to get some space before the next section
\end{table}


\newpage
\section{Example Interpretations}
When performing a bivariate EDA for two quantitative variables, the form, presence (or absence) of outliers, association, and strength should be specifically addressed. In addition, you should state how you assessed strength. Specifically, you should use $r$ to assess strength (see \sectref{sect:CorrStrength}) \textbf{IF} the relationship is linear without any outliers. However, if the relationship is nonlinear, has outliers, or both, then strength should be subjectively assessed from the scatterplot.

Two other points to consider when performing a bivariate EDA with quantitative variables. First, if outliers are present, do not let them completely influence your conclusions about form, association, and strength. In other words, assess these items ignoring the outlier(s). If you have raw data and the form excluding the outlier is linear, then compute $r$ with the outlier eliminated from the data. Second, the form of weak relationships is difficult to describe because, by definition, there is very little clustering to a form. As a rule-of-thumb, if the scatterplot is not obviously curved, then it is described as linear by default.

\warn{Outliers should not influence the descriptions of association, strength, and form.}

\vspace{-12pt}
\warn{The form is linear unless there is an OBVIOUS curvature.}

\vspace{12pt}
\subsection*{Highway MPG and Weight}
\textit{The following overall bivariate summary for the relationship between highway MPG and weight is made using the calculations from the previous sections.}

The relationship between highway MPG and weight of cars \figrefp{fig:carscat2} appears to be primarily linear (although I see a very slight concavity), negative, and moderately strong with a correlation of -0.81. The three points at (2400,46), (2500,27), and (1800,33) might be considered SLIGHT outliers (these are not far enough removed for me to consider them outliers, but some people may). The correlation coefficient was used to assess strength because I deemed the relationship to be linear without any outliers.

\newpage
\subsection*{State Energy Usage}
\begin{quote}
\textit{A 2001 report from the \href{http://www.eia.doe.gov/}{Energy Information Administration} of the Department of Energy details the total consumption of a variety of energy sources by state in 2001. Construct a proper EDA for the relationship between total petroleum and coal consumption (in trillions of BTU).}
\end{quote}

The relationship between total petroleum and coal consumption is generally linear, with two outliers at total petroleum levels greater than 3000 trillions of BTU, positive, and weak (\figref{fig:scatNRG1}-Left). I did not use the correlation coefficient because of the outliers. If the two outliers (Texas and California) are removed then the relationship is linear, with no additional outliers, positive, and weak ($r=$0.53) (\figref{fig:scatNRG1}-Right).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/scatNRG1-1} 
\includegraphics[width=.4\linewidth]{Figs/scatNRG1-2} 

}

\caption[Scatterplot of the total consumption of petroleum versus the consumption of coal (in trillions of BTU) by all 50 states and the District of Columbia]{Scatterplot of the total consumption of petroleum versus the consumption of coal (in trillions of BTU) by all 50 states and the District of Columbia. The points shown in the left with total petroleum values greater than 3000 trillion BTU are deleted in the right plot.}\label{fig:scatNRG1}
\end{figure}


\end{knitrout}

\subsubsection*{R Appendix}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
NRG <- read.csv("data/NRG_Consump_2001.csv")
NRG1 <- NRG[-c(5,44),]
plot(TotalPet~Coal,data=NRG,pch=21,bg="gray70",xlab="Coal Consumption (trillion BTU)",
     ylab="Total Petroleum (trillion BTU)")
plot(TotalPet~Coal,data=NRG1,pch=21,bg="gray70",xlab="Coal Consumption (trillion BTU)",
     ylab="Total Petroleum (trillion BTU)")
corr(~Coal+TotalPet,data=NRG1)
\end{verbatim}
\end{kframe}
\end{knitrout}

\newpage
\subsection*{Hatch Weight and Incubation Time of Geckos}
\begin{quote}
\textit{A \href{http://www.moonvalleyreptiles.com/breeding/incubation-length-and-hatch-weight}{hobbyist} hypothesized that there would be a positive association between length of incubation (days) and hatchling weight (grams) for Crested Geckos (Rhacodactylus ciliatus). To test this hypothesis she collected the incubation time and weight for 21 hatchlings (shown below). Construct a proper EDA for the relationship between incubation time and hatchling weight.}
\end{quote}

\begin{verbatim}
Time  53  54  56  60  60  60  60  60  63  63  77  77  78  81  82  82  83  83  84  90  90
Wt   1.5 1.7 1.4 1.0 1.4 1.5 1.7 1.8 1.4 1.5 1.1 1.6 1.5 1.9 1.4 1.5 1.3 1.7 1.6 1.4 1.8
\end{verbatim}



The relationship between hatchling weight and incubation time for the Crested Geckos is linear, without obvious outliers (\textit{though some may consider the small hatchling at 60 days to be an outlier}), without a definitive association, and weak ($r$=0.11) \figrefp{fig:scatGecko}. I did compute $r$ because no outliers were present and the relationship was linear (or, at least, it was not nonlinear).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/scatGecko-1} 

}

\caption[Scatterplot of hatchling weight versus incubation time for Crested Geckos]{Scatterplot of hatchling weight versus incubation time for Crested Geckos.}\label{fig:scatGecko}
\end{figure}


\end{knitrout}

\vspace{-18pt}
\subsubsection*{R Appendix}
\vspace{-6pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
df <- read.csv("data/Gecko.csv")
plot(hatchwt~inctime,data=df,pch=21,bg="gray70",xlab="Incubation Time (days)",
     ylab="Hatchling Weight (grams)")
corr(~inctime+hatchwt,data=df)
\end{verbatim}
\end{kframe}
\end{knitrout}


\section{Cautions About Correlation}
\vspace{-6pt}
Examining relationships between pairs of quantitative variables is common practice. Using $r$ can be an important part of this analysis, as described above. However, $r$ can be abused through misapplication and misinterpretation. Thus, it is important to remember the following characteristics of correlation coefficients:
\vspace{-12pt}
\begin{Itemize}
  \item Variables must be quantitative (i.e., if you cannot make a scatterplot, then you cannot calculate $r$).
  \item The correlation coefficient only measures strength of \textbf{LINEAR} relationships (i.e., if the form of the relationship is not linear, then $r$ is meaningless and should not be calculated).
  \item The units that the variables are measured in do not matter (i.e., $r$ is the same between heights and weights measured in inches and lbs, inches and kg, m and kg, cm and kg, and cm and inches). This is because the variables are standardized when calculating $r$.
  \item The distinction between response and explanatory variables is not needed to compute $r$. That is, the correlation of GPA and ACT scores is the same as the correlation of ACT scores and GPA.
  \item Correlation coefficients are between -1 and 1.
  \item Correlation coefficients are strongly affected by outliers (simply, because both the mean and standard deviation, used in the calculation of $r$, are strongly affected by outliers).
\end{Itemize}

Additionally, correlation is not causation! In other words, just because a strong correlation is observed it does not mean that the explanatory variable caused the response variable (an exception may be in carefully designed experiments). For example, it was found above that highway gas mileage decreased linearly as the weight of the car increased. One must be careful here to not state that increasing the weight of the car CAUSED a decrease in MPG because these data are part of an observational study and several other important variables were not considered in the analysis. For example, the scatterplot in \figref{fig:carscat3}, coded for different numbers of cylinders in the car's engine, indicates that the number of cylinders may be inversely related to highway MPG and positively related to weight of the car. So, does the weight of the car, the number of cylinders, or both, explain the decrease in highway MPG?

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/carscat3-1} 

}

\caption[Scatterplot between the highway MPG and weight of cars manufactured in 1993 separated by number of cylinders]{Scatterplot between the highway MPG and weight of cars manufactured in 1993 separated by number of cylinders.}\label{fig:carscat3}
\end{figure}


\end{knitrout}

More interesting examples (e.g., high correlation between number of people who drowned by falling into a pool and the annual number of films that Nicolas Cage appeared in) that further demonstrate that ``correlation is not causation'' can be found on the \href{http://www.tylervigen.com/spurious-correlations}{Spurious Correlations website.}

Finally, the word ``correlation'' is often misused in everyday language. ``Correlation'' should only be used when discussing the actual correlation coefficient (i.e., $r$). When discussing the association between two variables, one should use ``association'' or ``relationship'' rather than ``correlation.'' For example, one might ask ``What is the relationship between age and rate of cancer?'', but should not ask (unless specifically interested in $r$) ``What is the correlation between age and rate of cancer?''.



\chapter{Bivariate EDA - Categorical} \label{chap:BivEDACat}

\minitoc
\vspace{60pt}

\lettrine{T}{wo-way frequency tables} summarize two categorical variables recorded on the same individual by displaying levels of the first variable as rows and levels of the second variable as columns. Each cell in this table contains the frequency of individuals that were in the corresponding levels of each variable. These frequency tables are often converted to percentage tables for ease of summarization and comparison among populations. This module explores the construction and interpretation of frequency and percentage tables.

The General Sociological Survey (GSS) is a very large survey that has been administered 25 times since 1972. The purpose of the GSS is to gather data on contemporary American society in order to monitor and explain trends in attitudes, behaviors, and attributes. Data from the following two questions on the GSS are used throughout this module.

\begin{Itemize}
  \item What is your highest degree earned? [choices -- ``less than high school diploma'', ``high school diploma'', ``junior college'', ``bachelors'', or ``graduate''; labeled as \var{degree}]
  \item How willing would you be to accept cuts in your standard of living in order to protect the environment? [choices -- ``very willing'', ``fairly willing'', ``neither willing nor unwilling'', ``not very willing'', or ``not at all willing''; labeled as \var{grnsol}]
\end{Itemize}

These data, stored in \href{https://raw.githubusercontent.com/droglenc/NCData/master/GSSWill2Pay.csv}{GSSWill2Pay.csv}, are loaded into R and examined below.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> gss <- read.csv("data/GSSWill2Pay.csv")
\end{verbatim}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> str(gss)
'data.frame':	3955 obs. of  2 variables:
 $ degree: Factor w/ 5 levels "BS","grad","HS",..: 5 5 5 5 5 5 5 5 5 5 ...
 $ grnsol: Factor w/ 5 levels "neither","un",..: 4 4 4 4 4 4 4 4 4 4 ...
> headtail(gss)
     degree grnsol
1      ltHS  vwill
2      ltHS  vwill
3      ltHS  vwill
3953   grad    vun
3954   grad    vun
3955   grad    vun
\end{verbatim}
\end{kframe}
\end{knitrout}
The \var{degree} and \var{grnsol} variables are both \emph{ordinal} categorical variables. By default the levels of factor variables are ordered alphabetically in R (as seen below with \R{levels()}).
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> levels(gss$degree)
[1] "BS"   "grad" "HS"   "JC"   "ltHS"
> levels(gss$grnsol)
[1] "neither" "un"      "vun"     "vwill"   "will"   
\end{verbatim}
\end{kframe}
\end{knitrout}
The order of levels can be specified using \R{factor()}. The variable to be reordered is the first argument to \R{factor()}, as well as the object to the left of the assignment operator. The desired order of the levels is listed in a vector that is given to \R{levels=}. It is important that the levels in this vector are ``spelled'' exactly as they appeared originally. Correct orders for \var{degree} and \var{grnsol} in the \R{gss} data.frame are created below.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> gss$degree <- factor(gss$degree,levels=c("ltHS","HS","JC","BS","grad"))
> gss$grnsol <- factor(gss$grnsol,levels=c("vwill","will","neither","un","vun"))
> levels(gss$degree)
[1] "ltHS" "HS"   "JC"   "BS"   "grad"
> levels(gss$grnsol)
[1] "vwill"   "will"    "neither" "un"      "vun"    
\end{verbatim}
\end{kframe}
\end{knitrout}
If the natural order of levels is alphabetical or the variable is nominal, then \R{factor()} is not needed.

\warn{Levels for a factor variable are ordered alphabetically by default in R. If the factor variable is ordinal, then \R{factor()} with \R{levels=} may be needed to specify the correct order of levels.}


\newpage
\section{Frequency Tables}
\vspace{-6pt}

A common method of summarizing bivariate categorical data is to count individuals that have each combination of levels of the two categorical variables. For example, how many respondents had less than a HS degree and were very willing, how many had a high school degree and were willing, and so on. The count of the number of individuals of each combination is called a frequency. A two-way frequency table offers an efficient way to display these frequencies \tabrefp{tab:EnvFreq}. For example, 40 of the respondents had less than a high school degree and were very willing to take a cut in their standard of living to protect the environment. Similarly, 542 respondents had a high school degree and were willing to cut their standard of living.

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:31 2017
\begin{table}[ht]
\centering
\caption{Frequency table of respondent's highest completed degree and willingness to cut their standard of living to protect the environment.} 
\label{tab:EnvFreq}
\begin{tabular}{rrrrrrr}
  \hline
 & vwill & will & neither & un & vun & Sum \\ 
  \hline
ltHS & 40 & 145 & 132 & 151 & 178 & 646 \\ 
  HS & 87 & 542 & 512 & 557 & 392 & 2090 \\ 
  JC & 15 & 61 & 64 & 54 & 44 & 238 \\ 
  BS & 42 & 199 & 179 & 187 & 75 & 682 \\ 
  grad & 24 & 104 & 83 & 64 & 24 & 299 \\ 
  Sum & 208 & 1051 & 970 & 1013 & 713 & 3955 \\ 
   \hline
\end{tabular}
\end{table}


The margins of a two-way frequency table may be augmented with row and column totals (as in \tabref{tab:EnvFreq}). Each marginal total represents the distribution of one of the, while ignoring the other, categorical variable. The total column represents the distribution of the row variable; in this case, the highest degree completed. The total row represents the distribution of the column variable; in this case, willingness to cut their standard of living to protect the environment. Thus, for example there were 238 respondents whose highest completed degree was junior college and there were 713 respondents who were very unwilling to cut their standard of living to protect the environment.

If one variables can be considered as the response, then this variable should form the columns of the frequency table. For example, ``willingness to cut'' could be considered the response variable and it was, appropriately, placed as the column variable in \tabref{tab:EnvFreq}.

\vspace{-6pt}
\subsection*{Frequency Tables in R}
\vspace{-6pt}
Two-way frequency tables are constructed in R with \R{xtabs()}, where the first argument is a formula of the form \R{\TILDE rowvar+colvar} and the corresponding data.frame is in \R{data=}. The result of \R{xtabs()} should be assigned to an object for further use.
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( tbl1 <- xtabs(~degree+grnsol,data=gss) )
      grnsol
degree vwill will neither  un vun
  ltHS    40  145     132 151 178
  HS      87  542     512 557 392
  JC      15   61      64  54  44
  BS      42  199     179 187  75
  grad    24  104      83  64  24
\end{verbatim}
\end{kframe}
\end{knitrout}
\vspace{-4pt}
Totals may be added to the margins of a saved table with \R{addMargins()}. For example, \R{addMargins()} was used to construct \tabref{tab:EnvFreq} from \R{tbl1}.
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> addMargins(tbl1)
\end{verbatim}
\end{kframe}
\end{knitrout}


\section{Percentage Tables}
Two-way frequency tables may be converted to percentage tables for ease of comparison between levels of the variables and also between populations. For example, it is difficult to determine from \tabref{tab:EnvFreq} if respondents with a high school degree are more likely to be very willing to cut their standard of living than respondents with a graduate degree, because there are approximately seven times as many respondents with a high school degree. However, if the frequencies are converted to percentages, then this comparison is easily made. Three types of percentage tables may be constructed from a frequency table.

\subsection{Row-Percentage Table}
A \textbf{row-percentage table} is computed by dividing each cell of the frequency table by the total in the same row of the frequency table and multiplying by 100 \tabrefp{tab:EnvRowP}. For example, the value in the ``vwill'' column and ``ltHS'' row of the row-percentage table is computed by dividing the value in the ``vwill'' column and ``ltHS'' row of the frequency table (i.e., 40; \tabref{tab:EnvFreq}) by the ``Sum'' of the ``ltHS'' row of the frequency table (i.e., 646) and multiplying by 100.

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:31 2017
\begin{table}[ht]
\centering
\caption{Row-percentage table of respondent's highest completed degree and willingness to cut their standard of living to protect the environment.} 
\label{tab:EnvRowP}
\begin{tabular}{rrrrrrr}
  \hline
 & vwill & will & neither & un & vun & Sum \\ 
  \hline
ltHS & 6.2 & 22.4 & 20.4 & 23.4 & 27.6 & 100.0 \\ 
  HS & 4.2 & 25.9 & 24.5 & 26.7 & 18.8 & 100.1 \\ 
  JC & 6.3 & 25.6 & 26.9 & 22.7 & 18.5 & 100.0 \\ 
  BS & 6.2 & 29.2 & 26.2 & 27.4 & 11.0 & 100.0 \\ 
  grad & 8.0 & 34.8 & 27.8 & 21.4 & 8.0 & 100.0 \\ 
   \hline
\end{tabular}
\end{table}


The value in each cell of a row-percentage table is the percentage OF ALL individuals in that row that have the characteristic of that column. For example, 6.2\% of the respondents with less than a high school degree are very willing to cut their standard of living to protect the environment. This statement must be read carefully. OF THE RESPONDENTS WITH LESS THAN A HIGH SCHOOL DEGREE, not of all respondents, 6.2\% were very willing to cut their standard of living.

If the response variable formed the columns, then the row-percentage table  allows one to compare percentages in levels of the response (i.e., columns) across groups (i.e., rows). For example, one can see that there is a general decrease in the percentage of respondents that were ``very unwilling'' to cut their standard of living to protect the environment as the level of education increased \tabrefp{tab:EnvRowP}.

\subsubsection*{Row-Percentage Table in R}
Percentage tables are constructed in R by submitting the saved \R{xtabs()} object to \R{percTable()}. The number of decimals to display is controlled with \R{digits=}. A row-percentage table is constructed by including \R{margin=1}. For example, the code below produced \tabref{tab:EnvRowP}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> percTable(tbl1,margin=1,digits=1)
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{Column-Percentage Table}
A \textbf{column-percentage table} is computed by dividing each cell of the frequency table by the total in the same column of the frequency table and multiplying by 100 \tabrefp{tab:EnvColP}. For example, the value in the ``vwill'' column and ``ltHS'' row on the column-percentage table is computed by dividing the value in the ``vwill'' column and ``ltHS'' row of the frequency table (i.e., 40; \tabref{tab:EnvFreq}) by the ``Sum'' of the ``vwill'' column of the frequency table (i.e., 208) and multiplying by 100.

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:31 2017
\begin{table}[ht]
\centering
\caption{Column-percentage table of respondent's highest completed degree and willingness to cut their standard of living to protect the environment.} 
\label{tab:EnvColP}
\begin{tabular}{rrrrrr}
  \hline
 & vwill & will & neither & un & vun \\ 
  \hline
ltHS & 19.2 & 13.8 & 13.6 & 14.9 & 25.0 \\ 
  HS & 41.8 & 51.6 & 52.8 & 55.0 & 55.0 \\ 
  JC & 7.2 & 5.8 & 6.6 & 5.3 & 6.2 \\ 
  BS & 20.2 & 18.9 & 18.5 & 18.5 & 10.5 \\ 
  grad & 11.5 & 9.9 & 8.6 & 6.3 & 3.4 \\ 
  Sum & 99.9 & 100.0 & 100.1 & 100.0 & 100.1 \\ 
   \hline
\end{tabular}
\end{table}


The value in each cell of a column-percentage table is the percentage OF ALL individuals in that column that have the characteristic of that row. For example, 19.2\% of respondents who were very willing to cut their standard of living had less than a high school degree. Again, this is a very literal statement. OF THE RESPONDENTS WHO WERE VERY WILLING TO CUT THEIR STANDARD OF LIVING, not of all respondents, 19.2\% had less than a high school degree.

\subsubsection*{Column-Percentage Table in R}
A column-percentage table is constructed by submitting the saved \R{xtabs()} object to \R{percTable()} with \R{margin=2}. For example, the code below produced \tabref{tab:EnvColP}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> percTable(tbl1,margin=2,digits=1)
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{Total-Percentage Table}
Each value in a \textbf{total-percentage table} is computed by dividing each cell of the frequency table by the total number of ALL individuals in the frequency table and multiplying by 100. For example, the value in the ``vwill'' column and ``ltHS'' row of the table-percentage table \tabrefp{tab:EnvTblP} is computed by dividing the value in the ``vwill'' column and ``ltHS'' row of the frequency table (i.e., 40; \tabref{tab:EnvFreq}) by the ``Sum'' of the entire frequency table (i.e., 3955) and multiplying by 100.

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 17 13:04:31 2017
\begin{table}[ht]
\centering
\caption{Table-percentage table of respondent's highest completed degree and willingness to cut their standard of living to protect the environment.} 
\label{tab:EnvTblP}
\begin{tabular}{rrrrrrr}
  \hline
 & vwill & will & neither & un & vun & Sum \\ 
  \hline
ltHS & 1.0 & 3.7 & 3.3 & 3.8 & 4.5 & 16.3 \\ 
  HS & 2.2 & 13.7 & 12.9 & 14.1 & 9.9 & 52.8 \\ 
  JC & 0.4 & 1.5 & 1.6 & 1.4 & 1.1 & 6.0 \\ 
  BS & 1.1 & 5.0 & 4.5 & 4.7 & 1.9 & 17.2 \\ 
  grad & 0.6 & 2.6 & 2.1 & 1.6 & 0.6 & 7.5 \\ 
  Sum & 5.3 & 26.5 & 24.4 & 25.6 & 18.0 & 99.8 \\ 
   \hline
\end{tabular}
\end{table}


The value in each cell of a table-percentage table is the percentage OF ALL individuals that have the characteristic of that column AND that row. For example, 1.0\% of ALL respondents had less than a high school degree AND were very willing to cut their standard of living to protect the environment. Compare this interpretation to the interpretations from the row and column-percentage tables above. This interpretation DOES refer to all respondents.

\subsubsection*{Total-Percentage Table in R}
A table-percentage table is constructed by submitting the saved \R{xtabs()} object to \R{percTable()} and omitting \R{margin=}. For example, the code below produced \tabref{tab:EnvTblP}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> percTable(tbl1,digits=1)
\end{verbatim}
\end{kframe}
\end{knitrout}


\section{Which Table to Use?}
Determining which table to use comes from applying one simple rule and practicing with several tables. The rule comes from determining if the question restricts the frame of reference to a particular level or category of one of the variables. If the question does restrict to a particular level, then either the row or column-percentage table that similarly restricts the frame of reference must be used. If a restriction to a particular level is not made, then the total-percentage table is used.

For example, consider the question -- ``What percentage of respondents with a bachelor's degree were very unwilling to cut their standard of living to protect the environment?''  This question refers to only respondents with bachelor's degrees (i.e., ``... of respondents with a bachelor's degree ...''). Thus, the answer is restricted to the ``BS'' row of the frequency table. The ROW-percentage table restricts the original table to the row levels and would be used to answer this question. Therefore, 11.0\% of respondents with bachelor's degrees were very unwilling to cut their standard of living to protect the environment \tabrefp{tab:EnvRowP}.

Now consider the question -- ``What percentage of all respondents had a high school degree and were very willing to cut their standard of living?''  This question does not restrict the frame of reference because it refers to ``... of all respondents ...''. Therefore, from the total-percentage table \tabrefp{tab:EnvTblP}, 2.2\% of respondents had a high school degree and were very willing to cut their standard of living.

Also consider this question -- ``What percentage of respondents who were neither willing nor unwilling to cut their standard of living had graduate degrees?''  This question refers only to respondents who were neither willing nor unwilling to cut their standard of living and, thus, restricts the question to the ``neither'' column of the frequency table. Thus, the answer will come from the COLUMN-percentage table. Therefore, 8.6\% of respondents who were neither willing nor unwilling to cut their standard of living had graduate degrees \tabrefp{tab:EnvColP}.

Finally, consider this question -- ``What percentage of all respondents were very willing to cut their standard of living to help the environment?''  This question has no restrictions, so the total-percentage table would be used. In addition, this question is only concerned with one of the two variables; thus, the answer will come from a marginal distribution. Therefore, 208 out of all 3955 respondents, or 5.3\%, were very willing to cut their standard of living to help the environment.

\warn{To determine which percentage table to use determine what type of restriction, if any, has been placed on the frame of reference for the question.}

\vspace{-12pt}
\warn{If a question does not refer to one of the two variables, then the answer will generally come from the marginal distribution of the other variable.}



\chapter{Linear Regression}  \label{chap:Regress}

\minitoc
\vspace{48pt}

\lettrine{L}{inear regression analysis is used to model the relationship} between two quantitative variables for two related purposes -- (i) explaining variability in the response variable and (ii) predicting future values of the response variable. Examples include predicting future sales of a product from its price, family expenditures on recreation from family income, an animal's food consumption in relation to ambient temperature, and a person's score on a German assessment test based on how many years the person studied German.

Exact predictions cannot be made because of natural variability. For example, two people with the same intake of mercury (from consumption of fish) will not have the same level of mercury in their blood stream (e.g., observe the two individuals in \figref{fig:HGscat} that had intakes of 580 ug HG/day). Thus, the best that can be accomplished is to predict the average or expected value for a person with a particular intake value. This is accomplished by finding the line that best ``fits'' the points on a scatterplot of the data and using that line to make predictions. Finding and using the ``best-fit'' line is the topic of this module.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/HGscat-1} 

}

\caption[Scatterplot of intake of mercury in fish and the mercury in the blood stream]{Scatterplot of intake of mercury in fish and the mercury in the blood stream. The two individuals mentioned in the main text are shown as red points.}\label{fig:HGscat}
\end{figure}


\end{knitrout}


\section{Response and Explanatory Variables}
Recall from \sectref{sect:RespExplan1} that the response (or dependent) variable is the variable to be predicted or explained and the explanatory (or independent) variable is the variable that will help do the predicting or explaining. In the examples mentioned above, future sales, family expenditures on recreation, the animal's food consumption, and score on the assessment test are response variables and product price, family income, temperature, and years studying German are explanatory variables, respectively. The response variable is on the y-axis and the explanatory variable is on the x-axis of scatterplots.


\section{Slope and Intercept}
The equation of a line is commonly expressed as,
  \[ y = mx + b  \]
where both $x$ and $y$ are variables, $m$ represents the slope of the line, and $b$ represents the y-intercept.\footnote{Hereafter, simply called the ``intercept.''}  It is important that you can look at the equation of a line and identify the response variable, explanatory variable, slope, and intercept. The response variable will always appear on one side of the equation (usually the left) by itself.  The value or symbol that is multiplied by the explanatory variable (e.g., $x$) is the slope, and the value or symbol by itself is the intercept. For example, in
\[ blood = 3.501 + 0.579*intake \]
\var{blood} is the response variable, \var{intake} is the explanatory variable, $0.579$ is the slope (it is multiplied by the explanatory variable), and $3.501$ is the intercept (it is not multiplied by anything in the equation). The same conclusions would be made if the equation had been written as
  \[ blood = 0.579*intake+3.501 \]

\warn{In the equation of a line, the slope is always multiplied by the explanatory variable and the intercept is always by itself.}

In addition to being able to identify the slope and intercept of a line you also need to be able to interpret these values. Most students define the slope as ``rise over run'' and the intercept as ``where the line crosses the y-axis.''  These ``definitions'' are loose geometric representations. For our purposes, the slope and intercept must be more strictly defined.

To define the slope, first think of ``plugging'' two values of intake into the equation discussed above. For example, if $intake=100$, then $blood=3.501+0.579*100$=61.40 and if $intake$ is one unit larger at $101$), then $blood=3.501+0.579*101$=61.98.\footnote{For simplicity of exposition, the actual units are not used in this discussion. However, ``units'' would usually be replaced with the actual units used for the measurements.} The difference between these two values is 61.98-61.40=$0.579$. Thus, the slope is the change in value of the response variable for a single unit change in the value of the explanatory variable \figrefp{fig:SlopeInt}. That is, mercury in the blood changes 0.579 units for a single unit change in mercury intake. So, if an individual increases mercury intake by one unit, then mercury in the blood will increase by 0.579 units, ON AVERAGE. Alternatively, if one individual has one more unit of mercury intake than another individual, then the first individual will have, ON AVERAGE, 0.579 more units of mercury in the blood.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/SlopeInt-1} 

}

\caption[Schematic representation of the meaning of the intercept and slope in a linear equation]{Schematic representation of the meaning of the intercept and slope in a linear equation.}\label{fig:SlopeInt}
\end{figure}


\end{knitrout}

To define the intercept, first ``plug'' $intake=0$ into the equation discussed above; i.e., $blood=3.501+0.579*0$ = $3.501$. Thus, the intercept is the value of the response variable when the explanatory variable is equal to zero \figrefp{fig:SlopeInt}. In this example, the AVERAGE mercury in the blood for an individual with no mercury intake is 3.501. Many times, as is true with this example, the interpretation of the intercept will be nonsensical. This is because $x=0$ will likely be outside the range of the data collected and, perhaps, outside the range of possible data that could be collected.

The equation of the line is a model for the relationship depicted in a scatterplot. Thus, the interpretations for the slope and intercept represent the \textit{average} change or the \textit{average} response variable. Thus, whenever a slope or intercept is being interpreted it must be noted that the result is an \textit{average} or \textit{on average}.


\section{Predictions}
Once a best-fit line has been identified (criteria for doing so is discussed in \sectref{sect:BestFitLine}), the equation of the line can be used to predict the average value of the response variable for individuals with a particular value of the explanatory variable. For example, the best-fit line for the mercury data shown in \figref{fig:HGscat} is
  \[ blood = 3.501 + 0.579*intake \]
Thus, the predicated average level of mercury in the blood for an individual that consumed 240 ug HG/day is found with
  \[ blood = 3.501 + 0.579*240 = 142.461 \]
Similarly, the predicted average level of mercury in the blood for an individual that consumed 575 ug HG/day is found with
  \[ blood = 3.501 + 0.579*575 = 336.426 \]
A prediction may be visualized by finding the value of the explanatory variable on the x-axis, drawing a vertical line until the best-fit line is reached, and then drawing a horizontal line over to the y-axis where the value of the response variable is read \figrefp{fig:HGpredict}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/HGpredict-1} 

}

\caption[Scatterplot between the intake of mercury in fish and the mercury in the blood stream of individuals with superimposed best-fit regression line illustrating predictions for two values of mercury intake]{Scatterplot between the intake of mercury in fish and the mercury in the blood stream of individuals with superimposed best-fit regression line illustrating predictions for two values of mercury intake.}\label{fig:HGpredict}
\end{figure}


\end{knitrout}

When predicting values of the response variable, it is important to not extrapolate beyond the range of the data. In other words, predictions with values outside the range of observed values of the explanatory variable should be made cautiously (if at all). An excellent example would be to consider height ``data'' collected during the early parts of a human's life (say the first ten years). During these early years there is likely a good fit between height (the response variable) and age. However, using this relationship to predict an individual's height at age 40 would likely result in a ridiculous answer (e.g., over ten feet). The problem here is that the linear relationship only holds for the observed data (i.e., the first ten years of life); it is not known if the same linear relationship exists outside that range of years. In fact, with human heights, it is generally known that growth first slows, eventually quits, and may, at very old ages, actually decline. Thus, the linear relationship found early in life does not hold for later years. Critical mistakes can be made when using a linear relationship to extrapolate outside the range of the data.

\section{Residuals}
The predicted value is a ``best-guess'' for an individual based on the best-fit line. The actual value for any individual is likely to be different from this predicted value. The \textbf{residual} is a measure of how ``far off'' the prediction is from what is actually observed. Specifically, the residual for an individual is found by subtracting the predicted value (given the individual's observed value of the explanatory variable) from the individual's observed value of the response variable, or
  \[ \text{residual}=\text{observed response}-\text{predicted response} \]

For example, consider an individual that has an observed intake of 650 and an observed level of mercury in the blood of 480. As shown in the previous section, the predicted level of mercury in the blood for this individual is
  \[ blood = 3.501 + 0.579*650 = 379.851 \]

The residual for this individual is then $480-379.851$ = $100.149$. This positive residual indicates that the observed value is approximately 100 units greater than the average for individuals with an intake of 650.\footnote{In other words, the observed value is ``above'' the line.}  As a second example, consider an individual with an observed intake of 250 and an observed level of mercury in the blood of 105. The predicted value for this individual is
  \[ blood = 3.501 + 0.579*250 = 148.251 \]

and the residual is $105-148.251$ = $-43.251$. This negative residual indicates that the observed value is approximately 43 units less than the average for individuals with an intake of 250.

Visually, a residual is the vertical distance between an individual's point and the best-fit line \figrefp{fig:HGresidual}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/HGresidual-1} 

}

\caption[Scatterplot between the intake of mercury in fish and the mercury in the blood stream of individuals with superimposed best-fit regression line illustrating the residuals for the two individuals discussed in the main text]{Scatterplot between the intake of mercury in fish and the mercury in the blood stream of individuals with superimposed best-fit regression line illustrating the residuals for the two individuals discussed in the main text.}\label{fig:HGresidual}
\end{figure}


\end{knitrout}

\section{Best-fit Criteria}\label{sect:BestFitLine}
An infinite number of lines can be placed on a graph, but many of those lines do not adequately describe the data. In contrast, many of the lines will appear, to our eye, to adequately describe the data. So, how does one find THE best-fit line from all possible lines. The \textbf{least-squares} method described below provides a quantifiable and objective measure of which line best ``fits'' the data.

Residuals are a measure of how far an individual is from a candidate best-fit line. Residuals computed from all individuals in a data set measure how far all individuals are from the candidate best-fit line. Thus, the residuals for all individuals can be used to identify the best-fit line.

The residual sum-of-squares (RSS) is the sum of all squared residuals. The least-squares criterion says that the ``best-fit'' line is the one line out of all possible lines that has the minimum RSS \figrefp{fig:RSSanim}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}









































































\begin{figure}[hbtp]

{\centering \animategraphics[width=.8\linewidth,controls,palindrome,autoplay]{1}{Figs/RSSanim-}{1}{75}

}

\caption[An animation illustrating how the residual sum-of-squares (RSS) for a series of candidate lines (red lines) is minimized at the best-fit line (green line)]{An animation illustrating how the residual sum-of-squares (RSS) for a series of candidate lines (red lines) is minimized at the best-fit line (green line).}\label{fig:RSSanim}
\end{figure}


\end{knitrout}

The discussion thusfar implies that all possible lines must be ``fit'' to the data and the one with the minimum RSS is chosen as the ``best-fit'' line. As there are an infinite number of possible lines, this would be impossible to do. Theoretical statisticians have shown that the application of the least-squares criterion always produces a best-fit line with a slope given by
  \[ slope = r\frac{s_{y}}{s_{x}}  \]

and an intercept given by
  \[ intercept = \bar{y}-slope*\bar{x}   \]

where $\bar{x}$ and $s_{x}$ are the sample mean and standard deviation of the explanatory variable, $\bar{y}$ and $s_{y}$ are the sample mean and standard deviation of the response variable, and $r$ is the sample correlation coefficient between the two variables. Thus, using these formulas finds the slope and intercept for the line, out of all possible lines, that minimizes the RSS.


\section{Assumptions}\label{sect:RegAssumptions}
The least-squares method for finding the best-fit line only works appropriately if each of the following five assumptions about the data has been met.

\begin{Enumerate}
  \item A line describes the data (i.e., a linear form).
  \item Homoscedasticity.
  \item Normally distributed residuals at a given x.
  \item Independent residuals at a given x.
  \item The explanatory variable is measured without error.
\end{Enumerate}

While all five assumptions of linear regression are important, only the first two are vital when the best-fit line is being used primarily as a descriptive model for data.\footnote{In contrast to using the model to make inferences about a population model.}  Description is the primary goal of linear regression used in this course and, thus, only the first two assumptions are considered further.

The linearity assumption appears obvious -- if a line does not represent the data, then don't try to fit a line to it!  Violations of this assumption are evident by a non-linear or curving form in the scatterplot.

The homoscedasticity assumption states that the variability about the line is the same for all values of the explanatory variable. In other words, the dispersion of the data around the line must be the same along the entire line. Violations of this assumption generally present as a ``funnel-shaped'' dispersion of points from left-to-right on a scatterplot.

Violations of these assumptions are often evident on a ``fitted-line plot'', which is a scatterplot with the best-fit line superimposed \figrefp{fig:ResidPlotEx}.\footnote{Residual plots, not discussed in this text, are another plot that often times is used to better assess assumption violations.} If the points look more-or-less like random scatter around the best-fit line, then neither the linearity nor the homoscedasticity assumption has been violated. A violation of one of these assumptions should be obvious on the scatterplot. In other words, there should be a clear curvature or funneling on the plot.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.8\linewidth]{Figs/ResidPlotEx-1} 

}

\caption[Fitted-line plots illustrating when the regression assumptions are met (upper-left) and three common assumption violations]{Fitted-line plots illustrating when the regression assumptions are met (upper-left) and three common assumption violations.}\label{fig:ResidPlotEx}
\end{figure}


\end{knitrout}
\vspace{12pt}  %fixes spaces gobbled up by R code

In this text, if an assumption has been violated, then one should not continue to interpret the linear regression. However, in many instances, an assumption violation can be ``corrected'' by transforming one or both variables to a different scale. Transformations are not discussed in this book.

\warn{If the regression assumptions are not met, then the regression results should not be interpreted.}


\newpage
\section{Coefficient of Determination}
The coefficient of determination ($r^{2}$) is the proportion of the total variability in the response variable that is explained away by knowing the value of the explanatory variable and the best-fit model. In simple linear regression, $r^{2}$ is literally the square of $r$, the correlation coefficient.\footnote{Simple linear regression is the fitting of a model with a single explanatory variable and is the only model considered in this module and this course. See \sectref{sect:corr} for a review of the correlation coefficient.} Values of $r^{2}$ are between 0 and 1.\footnote{It is common for $r^{2}$ to be presented as a percentage.}

The meaning of $r^{2}$ can be examined by making predictions of the response variable with and without knowing the value of the explanatory variable. First, consider predicting the value of the response variable without any information about the explanatory variable. In this case, the best prediction is the sample mean of the response variable (represented by the dashed blue horizontal line in \figref{fig:CoeffDeterm}). However, because of natural variability, not all individuals will have this value. Thus, the prediction might be ``bracketed'' by predicting that the individual will be between the observed minimum and maximum values (solid blue horizontal lines). Loosely speaking, this range is the ``total variability in the response variable'' (blue box).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.8\linewidth]{Figs/CoeffDeterm-1} 

}

\caption[Fitted line plot with visual representations of variabilities explained and unexplained]{Fitted line plot with visual representations of variabilities explained and unexplained. A full explanation is in the text.}\label{fig:CoeffDeterm}
\end{figure}


\end{knitrout}

Suppose now that the response variable is predicted for an individual with a known value of the explanatory variable (e.g., at the dashed vertical red line in \figref{fig:CoeffDeterm}). The predicted value for this individual is the value of the response variable at the corresponding point on the best-fit line (dashed horizontal red line). Again, because of natural variability, not all individuals with this value of the explanatory variable will have this exact value of the response variable. However, the prediction is now ``bracketed'' by the minimum and maximum value of the response variable \textbf{ONLY} for those individuals with the same value of the explanatory variable (solid red horizontal lines). Loosely speaking, this range is the ``variability in the response variable remaining after knowing the value of the explanatory variable'' (red box). This is the variability in the response variable that remains even after knowing the value of the explanatory variable or the variability in the response variable that cannot be explained away (by the explanatory variable).

The portion of the total variability in the response variable that was explained away consists of all the values of the response variable that would no longer be entertained as possible predictions once the value of the explanatory variable is known (green box in \figref{fig:CoeffDeterm}).

Now, by the definition of $r^{2}$, $r^{2}$ can be visualized as the area of the green box divided by the area of the blue box. This calculation does not depend on which value of the explanatory variable is chosen as long as the data are evenly distributed around the line (i.e., homoscedasticity -- see \sectref{sect:RegAssumptions}).

If the variability explained away (green box) approaches the total variability in the response variable (blue box), then $r^{2}$ approaches 1. This will happen only if the variability around the line approaches zero. In contrast, the variability explained (green box) will approach zero if the slope is zero (i.e., no relationship between the response and explanatory variables). Thus, values of $r^{2}$ also indicate the strength of the relationship; values near 1 are stronger than values near 0. Values near 1 also mean that predictions will be fairly accurate -- i.e., there is little variability remaining after knowing the explanatory variable.

\warn{A value of $r^{2}$ near 1 represents a strong relationship between the response and explanatory variables that will lead to accurate predictions.}


\section{Examples I}
There are twelve questions that are commonly asked about linear regression results. These twelve questions are listed below with some hints about things to remember when answering some of the questions. An example of these questions in context is then provided.

\begin{enumerate}
  \item What is the response variable?  \textit{Identify which variable is to be predicted or explained, which variable is dependent on another variable, which would be hardest to measure, or which is on the y-axis.}
  \item What is the explanatory variable?  \textit{The remaining variable after identifying the response variable.}
  \item Comment on linearity and homoscedasticity. \textit{Examine fitted-line plot for curvature (i.e., non-linearity) or a funnel-shape (i.e., heteroscedasticity).}
  \item What is the equation of the best-fit line?  \textit{In the generic equation of the line ($y=mx+b$) replace $y$ with the name of the response variable, $x$ with the name of the explanatory variable, $m$ with the value of the slope, and $b$ with the value of the intercept.}
  \item Interpret the value of the slope. \textit{Comment on how the response variable changes by slope amount for each one unit change of the explanatory variable, on average.}
  \item Interpret the value of the intercept. \textit{Comment on how the response variable equals the intercept, on average, if the explanatory variable is zero.}
  \item Make a prediction given a value of the explanatory variable. \textit{Plug the given value of the explanatory variable into the equation of the best-fit line. Make sure that this is not an extrapolation.}
  \item Compute a residual given values of both the explanatory and response variables. \textit{Make a prediction (see previous question) and then subtract this value from the observed value of the response. Make sure that the prediction is not an extrapolation.}
  \item Identify an extrapolation in the context of a prediction problem. \textit{Examine the x-axis scale on the fitted-line plot and do not make predictions outside of the plotted range.}
  \item What is the proportion of variability in the response variable explained by knowing the value of the explanatory variable?  \textit{This is $r^{2}$.}
  \item What is the correlation coefficient?  \textit{This is the square root of $r^{2}$. Make sure to put a negative sign on the result if the slope is negative.}
  \item How much does the response variable change if the explanatory variable changes by X units?  \textit{This is an alternative to asking for an interpretation of the slope. If the explanatory variable changes by X units, then the response variable will change by X*slope units, on average.}
\end{enumerate}

All answers should refer to the variables of the problem -- thus, ``y'', ``x'', ``response'', or ``explanatory'' should not be in any part of any answer. The questions about the slope, intercept, and predictions need to explicitly identify that the answer is an ``average'' or ``on average.''

\newpage
\subsection*{Chimp Hunting Parties}
\begin{quote}
\textit{Stanford (1996) gathered data to determine if the size of the hunting party (number of individuals hunting together) affected the hunting success of the party (number of hunts that resulted in a kill) for wild chimpanzees (Pan troglodytes) at Gombe. The results of their analysis for 17 hunting parties is shown in the figure below.\footnote{These data are in \href{https://raw.githubusercontent.com/droglenc/NCData/master/Chimp.csv}{Chimp.csv}.}  Use these results to answer the questions below.}
\end{quote}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}

{\centering \includegraphics[width=.4\linewidth]{Figs/ChimpFLP-1} 

}



\end{knitrout}

\begin{QAlist}
  \item What is the response variable?
  \begin{QAlist}
    \item The response variable is the percent of successful hunts because the authors are attempting to see if success depends on hunting party size. Additionally, the percent of successful hunts is shown on the y-axis.
  \end{QAlist}
  \item What is the explanatory variable?
  \begin{QAlist}
    \item The explanatory variable is the size of the hunting party.
  \end{QAlist}
  \item In terms of the variables of the problem, what is the equation of the best-fit line?
  \begin{QAlist}
    \item The equation of the best-fit line is \% Success of Hunt = 24.215 + 3.705*Number of Hunting Party Members.
  \end{QAlist}
  \item Interpret the value of the slope in terms of the variables of the problem.
  \begin{QAlist}
    \item The slope indicates that the percent of successful hunts increases by 3.705, on average, for every increase of one member to the hunting party.
  \end{QAlist}
  \item Interpret the value of the intercept in terms of the variables of the problem.
  \begin{QAlist}
    \item The intercept indicates that the percent of successful hunts is 24.215, on average, for hunting parties with no members.
  \end{QAlist}
  \item What is the predicted hunt success if the hunting party consists of 20 chimpanzees?
  \begin{QAlist}
    \item The predicted hunt success for parties with 20 individuals is an extrapolation, because 20 is outside the range of number of members observed on the x-axis of the fitted-line plot.
  \end{QAlist}
  \item What is the predicted hunt success if the hunting party consists of 12 chimpanzees?
  \begin{QAlist}
    \item The predicted hunt success for parties with 12 individuals is 24.215 + 3.705*12 = 68.7\%.
  \end{QAlist}
  \item What is the residual if the hunt success for 10 individuals is 50\%?
  \begin{QAlist}
    \item The residual in this case is $50$-(24.215 + 3.705*10) = $50$-61.3 = -11.3. Therefore, it appears that the success of this hunting party is 11.3\% lower than average for this size of hunting party.
  \end{QAlist}
  \item What proportion of the variability in hunting success is explained by knowing the size of the hunting party?
  \begin{QAlist}
    \item The proportion of the variability in hunting success that is explained by knowing the size of the hunting party is $r^{2}$=0.88.
  \end{QAlist}
  \item What is the correlation between hunting success and size of hunting party?
  \begin{QAlist}
    \item The correlation between hunting success and size of hunting party is $r=$0.94.
  \end{QAlist}
  \item How much does hunt success decrease, on average, if there are two fewer individuals in the party?
  \begin{QAlist}
    \item If the hunting party has two fewer members, then the hunting success would decrease by 7.4\% (i.e., $-2$*3.705), on average.
  \end{QAlist}
  \item Does any aspect of this regression concern you (i.e., consider the regression assumptions)?
  \begin{QAlist}
    \item The data appear to be very slightly curved but there is no evidence of a funnel-shape. Thus, the data may be slightly non-linear but they appear homoscedastic.
  \end{QAlist}
\end{QAlist}

\warn{All interpretations should be ``in terms of the variables of the problem'' rather than the generic terms of x, y, response variable, and explanatory variable.}


\section{Regression in R}
The mercury intake and amount in the blood data is loaded below to be used as an example for finding a regression line with R.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> setwd('c:/data/')
> merc <- read.csv("Mercury.csv")
\end{verbatim}
\end{kframe}
\end{knitrout}
\vspace{-14pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> str(merc)
'data.frame':	13 obs. of  2 variables:
 $ intake: num  180 200 230 410 600 550 275 580 580 105 ...
 $ blood : num  90 120 125 290 310 290 170 275 350 70 ...
\end{verbatim}
\end{kframe}
\end{knitrout}

The linear regression model is fit to two quantitative variables with \R{lm()}. The first argument is a formula of the form \R{response\TILDE explanatory}, where \R{response} contains the response variable and \R{explanatory} contains the explanatory variable, and the corresponding data.frame is in \R{data=}. The results of \R{lm()} should be assigned to an object so that specific results can be extracted.

\warn{The same formula used to make a scatterplot with \R{plot()} is used in \R{lm()} to find the best-fit line.}


The regression was fit to the mercury data below. From this it is seen that the intercept is 3.501 and the slope is 0.579.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( lm1 <- lm(blood~intake,data=merc) )
Coefficients:
(Intercept)       intake  
     3.5007       0.5791  
\end{verbatim}
\end{kframe}
\end{knitrout}

A fitted-line plot \figrefp{fig:HGFLP} is constructed by submitting the \R{lm()} object to \R{fitPlot()}. Aspects of this plot can be adjusted using the same arguments as described for \R{plot()} in \sectref{sect:ScatterplotsR}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> fitPlot(lm1,pch=21,bg="gray70",xlab="Mercury Intake",ylab="Mercury in the Blood")
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/HGFLP-1} 

}

\caption[Fitted-line plots for the regression of mercury in the blood on mercury intake]{Fitted-line plots for the regression of mercury in the blood on mercury intake.}\label{fig:HGFLP}
\end{figure}


\end{knitrout}

Predicted values from the linear regression are obtained with \R{predict()}. The \R{predict()} function requires the saved \R{lm()} object as its first argument. The second argument is a data.frame constructed with \R{data.frame()} that contains the \textbf{EXACT} name of the explanatory variable as it appeared in \R{lm()} set equal to the value of the explanatory at which the prediction should be made. For example, the predicted amount of mercury in the blood for an intake of 240 $\mu$g per day is 142.5, as obtained below.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> predict(lm1,data.frame(intake=240))
       1 
142.4895 
\end{verbatim}
\end{kframe}
\end{knitrout}

The coefficient of determination is computed by submitting the saved \R{lm()} object to \R{rSquared()}. For example, 88.4\% of the variability in mercury in the blood is explained by knowing the amount of mercury at intake. [Note the use of \R{digits=} to control the number of decimals.]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> rSquared(lm1,digits=3)
[1] 0.884
\end{verbatim}
\end{kframe}
\end{knitrout}


\section{Examples II}
\subsection*{Car Weight and MPG}
In \modref{chap:BivEDAQuant}, an EDA for the relationship between \var{HMPG} (the highway miles per gallon) and \var{Weight} (lbs) of 93 cars from the 1993 model year was performed. This relationship will be explored further here as an example of a complete regression analysis. In this analysis, the regression output will be examined within the context of answering the twelve typical questions. These data are read into R below and the linear regression model is fit, coefficients extracted, fitted-line plot constructed, and coefficient of determination extracted.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> cars93 <- read.csv("data/93cars.csv")
\end{verbatim}
\end{kframe}
\end{knitrout}
\vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( lm2 <- lm(HMPG~Weight,data=cars93) )
Coefficients:
(Intercept)       Weight  
  51.601365    -0.007327  
> fitPlot(lm2,ylab="Highway MPG")
> rSquared(lm2,digits=3)
[1] 0.657
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/CarFit-1} 

}

\caption[Fitted line plot of the regression of highway MPG on weight of 93 cars from 1993]{Fitted line plot of the regression of highway MPG on weight of 93 cars from 1993.}\label{fig:CarFit}
\end{figure}


\end{knitrout}

The simple linear regression model appears to fit the data moderately well as the fitted-line plot \figrefp{fig:CarFit} shows only a very slight curvature and only very slight heteroscedasticity.\footnote{In advanced statistics books, objective measures for determining whether there is significant curvature or heteroscedasticity in the data are used. In this book, we will only be concerned with whether there is strong evidence of curvature or heteroscedasticity. There does not seem to be either here.}  The sample slope is -0.0073, the sample intercept is 51.6, and the coefficient of determination is 0.657.

\begin{QAlist}
  \item What is the response variable?
  \begin{QAlist}
    \item The response variable in this analysis is the highway MPG, because that is the variable that we are trying to learn about or explain the variability of.
  \end{QAlist}
  \item What is the explanatory variable?
  \begin{QAlist}
    \item The explanatory variable in this analysis is the weight of the car (by process of elimination).
  \end{QAlist}
  \item In terms of the variables of the problem, what is the equation of the best-fit line?
  \begin{QAlist}
    \item The equation of the best-fit line for this problem is HMPG = 51.6 - 0.0073Weight.
  \end{QAlist}
  \item Interpret the value of the slope in terms of the variables of the problem.
  \begin{QAlist}
    \item The slope indicates that for every increase of one pound of car weight the highway MPG decreases by -0.0073, on average.
  \end{QAlist}
  \item Interpret the value of the intercept in terms of the variables of the problem.
  \begin{QAlist}
    \item The intercept indicates that a car with 0 weight will have a highway MPG value of 51.6, on average.\footnote{This is the correct interpretation of the intercept. However, it is nonsensical because it is an extrapolation; i.e., no car will weigh 0 pounds.}
  \end{QAlist}
  \item What is the predicted highway MPG for a car that weighs 3100 lbs?
  \begin{QAlist}
    \item The predicted highway MPG for a car that weighs 3100 lbs is 51.60137 - 0.00733(3100) = 28.9 MPG. Alternatively, this value is computed with
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> predict(lm2,data.frame(Weight=3100))
       1 
28.88748 
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{QAlist}
  \item What is the predicted highway MPG for a car that weighs 5100 lbs?
  \begin{QAlist}
    \item The predicted highway MPG for a car that weighs 5100 lbs should not be computed with the results of this regression, because 5100 lbs is outside the domain of the data \figrefp{fig:CarFit}.
  \end{QAlist}
  \item What is the residual for a car that weights 3500 lbs and has a highway MPG of 24?
  \begin{QAlist}
    \item The predicted highway MPG for a car that weighs 3500 lbs is 51.60137 - 0.00733(3500) = 26.0. Thus, the residual for this car is 24 - 26.0 = -2.0. Alternatively, this is computed in R with
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> 24-predict(lm2,data.frame(Weight=3500))
        1 
-1.956658 
\end{verbatim}
\end{kframe}
\end{knitrout}
Therefore, it appears that this car gets 2.0 MPG less than an average car with the same weight.
  \end{QAlist}
  \item What proportion of the variability in highway MPG is explained by knowing the weight of the car?
  \begin{QAlist}
    \item The proportion of the variability in highway MPG that is explained by knowing the weight of the car is $r^{2}$=0.66.
  \end{QAlist}
  \item What is the correlation between highway MPG and car weight?
  \begin{QAlist}
    \item The correlation between highway MPG and car weight is $r=$-0.81.\footnote{Put a negative sign in front of your result from taking the square root of $r^2$, because the relationship between highway MPG and weight is negative.}
  \end{QAlist}
  \item How much is the highway MPG expected to change if a car is 1000 lbs heavier?
  \begin{QAlist}
    \item If the car was 1000 lbs heavier, you would expect the car's highway MPG to decrease by 7.33 (i.e., 1000 slopes).
  \end{QAlist}
\end{QAlist}



\chapter{Probability Introduction} \label{chap:ProbIntro}

\lettrine{P}{robability} is the ``language'' used to describe the proportion of times that a random event will occur. The language of probability is at the center of statistical inference (see Modules \ref{chap:HypothesisTests} and \ref{chap:ConfidenceRegions}). Only a minimal understanding of probability is required to understand most basic inferential methods, including all of those in this course. Thus, only a short, example-based, introduction to probability is provided here.\footnote{A deeper understanding of probability is required to understand more complex inferential methods beyond those in this course.}

The most basic forms of probability assume that items are selected randomly. In other words, simple probability calculations require that each item, whether that item is an individual or an entire sample, has the same chance of being selected. Thus, in simple intuitive examples it will be stated that the individuals were ``thoroughly mixed'' and more realistic examples will require randomization.\footnote{See \modref{chap:DataProd} for methods to randomly select or allocate individuals.}

If every individual has the same chance of being selected, then the probability of an event is equal to the proportion of items in the event out of the entire population. In other words, the probability is the number of items in the event divided by the total number of items in the population.

For example, the probability of selecting a red ball from a thoroughly mixed box containing 15 red and 10 blue balls is equal to $\frac{15}{25}=0.6$ (i.e., 15 individuals (``balls'') in the event (``red'') divided by the total number of individuals (``all balls in the box''); \figref{fig:ProbBox}-Left). Similarly, the probability of randomly selecting a woman from a room with 20 women and 30 men is 0.4 ($=\frac{20}{50}$; \figref{fig:ProbBox}-Right). In both examples, the calculation can be considered a probability because (i) individuals were randomly selected and (ii) a proportion of a total was computed.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.3\linewidth]{Figs/ProbBox-1} 
\includegraphics[width=.3\linewidth]{Figs/ProbBox-2} 

}

\caption[Depictions of a `box' with 15 red balls and 10 blue balls (Left) and a `room' with 30 men and 20 women (Right)]{Depictions of a `box' with 15 red balls and 10 blue balls (Left) and a `room' with 30 men and 20 women (Right).}\label{fig:ProbBox}
\end{figure}


\end{knitrout}

\newpage
The two previous examples are simple because the selection is from a small, discrete set of items. Probabilities may be computed for a continuous variable if the distribution of that variable is known for the entire population. For example, the probability that a random individual is greater than 71 inches tall can be calculated if the distribution of heights for all individuals in the population is known (or reasonably approximated). For example, as shown in \modref{chap:NormDist}, if it can be assumed that heights is $N(66,3)$, then the proportion of individuals in the population taller than 71 inches tall is 0.0478 \figrefp{fig:ProbNorm}.\footnote{As computed with \R{distrib(71,mean=66,sd=3,lower.tail=FALSE)}.} This result is a probability because (i) the individual was randomly selected and (ii) the proportion of all individuals of interest in the entire population was found.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/ProbNorm-1} 

}

\caption[Calculation of the probability that a randomly selected individual from a $N(66,3)$ population will have a height greater than 71 inches]{Calculation of the probability that a randomly selected individual from a $N(66,3)$ population will have a height greater than 71 inches.}\label{fig:ProbNorm}
\end{figure}


\end{knitrout}

A theory that explains the distribution of statistics computed from all possible samples from a population will be developed in \modref{chap:SamplingDist}. This distribution will be used to compute the probability of observing a particular range of statistics from random samples. This technique is the basis for making statistical inferences in Modules \ref{chap:HypothesisTests} and \ref{chap:ConfidenceRegions}.



\chapter{Sampling Distributions} \label{chap:SamplingDist}

\vspace{24pt}
\minitoc
\vspace{36pt}

\lettrine{S}{tatistical inference is the process} of making a conclusion about the parameter of a population based on the statistic computed from a sample. This process is difficult becauses statistics depend on the specific individuals in the sample and, thus, vary from sample to sample. For example, recall from \sectref{sect:IVPPSS} that the mean length of fish differed among four samples ``taken'' from Square Lake. Thus, to make conclusions about the population from the sample, the distribution (i.e., shape, center, and dispersion) of the statistic computed from all possible samples must be understood.\footnote{See \modref{chap:WhyStatsImportant} for a review of sampling variability.} In this module, the distribution of statistics from all possible samples is explored and generalizations are defined that can be used to make inferences. In subsequent modules, this information, along with results from a single sample, will be used to make specific inferences about the population.

\warn{Statistical inference requires considering sampling variability.}


\section{What is a Sampling Distribution?}
\subsection{Definitions and Characteristics}  \label{sec:SDistDefn}
A \textbf{Sampling distribution} is the distribution of values of a particular statistic computed from all possible samples of the same size from the same population. The discussion of sampling distributions and all subsequent theories related to statistical inference are based on repeated samples from the same population. As these theories are developed, we will consider taking multiple samples; however, after the theories have been developed, then only one sample will be taken with the theory then being applied to those results. Thus, it is important to note that only one sample is ever actually taken from a population.



The concept of a sampling distribution is illustrated with a population of six students that scored 6, 6, 4, 5, 7 and 8 points, respectively, on an 8-point quiz. The mean of this population is $\mu=$ 6.000 points and the standard deviation is $\sigma=$ 1.414 points. Suppose that every sample of size $n=2$ is extracted from this population and that the sample mean is computed for each sample \tabrefp{tab:SDistQuiz2}.\footnote{These samples are found by putting the values into a vector with \R{vals <- c(6,6,4,5,7,8)} and then using \R{combn(vals,2)}. The means are found with \R{mns <- as.numeric(combn(vals,2,mean))}.} The sampling distribution of the sample mean from samples of $n=2$ from this population \figrefp{fig:SDistQuiz2} is the histogram of means from these 15 samples.\footnote{The histogram is constructed with \R{hist(\TILDE mns,w=0.5)}.}

\begin{table}[htbp]
  \caption{All possible samples of $n=2$ and corresponding sample mean from the quiz score population.}
  \label{tab:SDistQuiz2}
  \centering
    \begin{tabular}{cc||cc||cc||cc||cc}
\hline\hline
Scores & Mean & Scores & Mean & Scores &  Mean & Scores & Mean & Scores & Mean \\
\hline
6,6 & 6.0 & 6,7 & 6.5 & 6,5 & 5.5 & 4,5 & 4.5 & 5,7 & 6.0 \\
6,4 & 5.0 & 6,8 & 7 & 6,7 & 6.5 & 4,7 & 5.5 & 5,8 & 6.5 \\
6,5 & 5.5 & 6,4 & 5 & 6,8 & 7.0 & 4,8 & 6.0 & 7,8 & 7.5 \\
\hline\hline
    \end{tabular}
\end{table}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/SDistQuiz2-1} 

}

\caption[Sampling distribution of mean quiz scores from samples of $n=2$ from the quiz score population]{Sampling distribution of mean quiz scores from samples of $n=2$ from the quiz score population.}\label{fig:SDistQuiz2}
\end{figure}


\end{knitrout}

\vspace{-12pt}
The mean (=6.000) and standard deviation (=0.845) of the 15 sample means are measures of center and dispersion for the sampling distribution. The standard deviation of statistics (i.e., dispersion of the sampling distribution) is generally referred to as the \textbf{standard error of the statistic} (abbreviated as $SE_{stat}$). This new terminology is used to keep the dispersion of the sampling distribution separate from the dispersion of individuals in the population, which is measured by the standard deviation. Thus, the standard deviation of all possible sample means is referred to as the standard error of the sample means (SE). The SE in this example is 0.845. The standard deviation is the dispersion of individuals in the population and, in this example, is 1.414.

This example illustrates three major concepts concerning sampling distributions. First, the sampling distribution will more closely resemble a normal distribution than the original population distribution (unless, of course, the population distribution was normal).

Second, the center (i.e., mean) of the sampling distribution will equal the parameter that the statistic was intended to estimate (e.g., a sample mean is intended to be an estimate of the population mean). In this example, the mean of all possible sample means (= 6.0 points) is equal to the mean of the original population ($\mu=$ 6.0 points). A statistic is said to be \textbf{unbiased} if the center (mean) of its sampling distribution equals the parameter it was intended to estimate. This example illustrates that the sample mean is an unbiased estimate of the population mean.

Third, the standard error of the statistic is less than the standard deviation of the original population. In other words, the dispersion of statistics is less than the dispersion of individuals in the population. For example, the dispersion of individuals in the population is $\sigma=$ 1.414 points, whereas the dispersion of statistics from all possible samples is $SE_{\bar{x}}=$ 0.845 points.

\warn{All statistics in this course are unbiased.}


\vspace{-12pt}
\subsection{Critical Distinction}
\vspace{-6pt}
Three distributions are considered in statistics. The sampling distribution is the distribution of a statistic computed from all possible samples of the same size from the same population, the population distribution is the distribution of all individuals in a population (see \modref{chap:NormDist}), and the sample distribution is the distribution of all individuals in a sample (see histograms in \modref{chap:UnivEDAQuant1}). The sampling distribution is about \textbf{statistics}, whereas the population and sample distributions are about \textbf{individuals}. For inferential statistics, it is important to distinguish between population and sampling distributions. Keep in mind that one (population) is the distribution of individuals and the other (sampling) is the distribution of statistics.

Just as importantly, remember that a standard error measures the dispersion among statistics (i.e., sampling variability), whereas a standard deviation measures dispersion among individuals (i.e., natural variability). Specifically, the population standard deviation measures dispersion among all individuals in the population and the sample standard deviation measures dispersion of all individuals in a sample. In contrast, the standard error measures dispersion among statistics computed from all possible samples. The population standard deviation is the dispersion on a population distribution, whereas the standard error is the dispersion on a sampling distribution.


\vspace{-6pt}
\subsection{Dependencies}
\vspace{-6pt}


The sampling distribution of sample means from samples of $n=2$ from the population of quizzes was shown above. The sampling distribution will look different if any other sample size is used. For example, the samples and means from each sample of $n=3$ are shown in \tabref{tab:SDistQuiz3}. The mean of these means is 6.000, the standard error is 0.592, and the sampling distribution is symmetric, perhaps approximately normal \figrefp{fig:SDistQuiz3}. The three major characteristics of sampling distributions noted in \sectref{sec:SDistDefn} are still true: the sampling distribution is still more normal than the original population, the sample mean is still unbiased (i.e, the mean of the means is equal to $\mu$), and the standard error is smaller than the standard deviation of the original population. However, also take note that the standard error of the sample mean is smaller from samples of $n=3$ than from $n=2$.\footnote{One should also look at the results from $n=4$ in one of the online Review Exercises.}

\begin{table}[htbp]
  \caption{All possible samples of $n=3$ and corresponding sample means from the quiz score population.}
  \label{tab:SDistQuiz3}
  \centering
    \begin{tabular}{cc||cc||cc||cc||cc}
\hline\hline
Scores & Mean & Scores & Mean & Scores &  Mean & Scores & Mean & Scores & Mean \\
\hline
6,6,4 & 5.3 & 6,6,5 & 5.7 & 6,6,7 & 6.3 & 6,6,8 & 6.7 & 4,5,7 & 5.3 \\
6,4,5 & 5.0 & 6,4,7 & 5.7 & 6,4,8 & 6.0 & 6,5,7 & 6.0 & 4,5,8 & 5.7 \\
6,5,8 & 6.3 & 6,7,8 & 7.0 & 6,4,5 & 5.0 & 6,4,7 & 5.7 & 4,7,8 & 6.3 \\
6,4,8 & 6.0 & 6,5,7 & 6.0 & 6,5,8 & 6.3 & 6,7,8 & 7.0 & 5,7,8 & 6.7 \\
\hline\hline
    \end{tabular}
\end{table}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.37\linewidth]{Figs/SDistQuiz3-1} 

}

\caption[Sampling distribution of mean quiz scores from samples of $n=3$ from the quiz score population]{Sampling distribution of mean quiz scores from samples of $n=3$ from the quiz score population.}\label{fig:SDistQuiz3}
\end{figure}


\end{knitrout}

The sampling distribution will also be different if the statistic changes; e.g, if the sample median rather than sample mean is computed in each sample. Before showing the results of each sample, note that the population median (i.e., the median of the individuals in the population --- 6, 6, 4, 5, 7, and 8) is 6.0 points. The sample median from each sample is shown in \tabref{tab:SDistQuizMdns3} and the actual sampling distribution is shown in \figref{fig:SDistQuizMdns3}. Note that the sampling distribution of the sample medians is still ``more'' normal than the original population distribution, the mean of the sample medians (=6.000 points) still equals the parameter (population median) that the sample median is intended to estimate (thus the sample median is also unbiased), and this sampling distribution differs from the sampling distribution of sample means from samples of $n=3$.

\vspace{-4pt}
\begin{table}[htbp]
  \caption{All possible samples of $n=3$ and corresponding sample medians from the quiz score population.}
  \label{tab:SDistQuizMdns3}
  \centering
    \begin{tabular}{cc||cc||cc||cc||cc}
\hline\hline
Scores & Median & Scores & Median & Scores &  Median & Scores & Median & Scores & Median \\
\hline
6,6,4 & 6 & 6,6,5 & 6 & 6,6,7 & 6 & 6,6,8 & 6 & 4,5,7 & 5 \\
6,4,5 & 5 & 6,4,7 & 6 & 6,4,8 & 6 & 6,5,7 & 6 & 4,5,8 & 5 \\
6,5,8 & 6 & 6,7,8 & 7 & 6,4,5 & 5 & 6,4,7 & 6 & 4,7,8 & 7 \\
6,4,8 & 6 & 6,5,7 & 6 & 6,5,8 & 6 & 6,7,8 & 7 & 5,7,8 & 7 \\
\hline\hline
    \end{tabular}
\end{table}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.37\linewidth]{Figs/SDistQuizMdns3-1} 

}

\caption[Sampling distribution of median quiz scores from $n=3$ samples from the quiz score population]{Sampling distribution of median quiz scores from $n=3$ samples from the quiz score population.}\label{fig:SDistQuizMdns3}
\end{figure}


\end{knitrout}

These examples demonstrate that the naming of a sampling distribution must be specific. For example, the first sampling distribution in this module should be described as the ``sampling distribution of sample means from samples of n=2.''  This last example should be described as the ``sampling distribution of sample medians from samples of n=3.''  Doing this with each distribution reinforces the point that sampling distributions depend on the sample size and the statistic calculated.

\vspace{-3pt}
\warn{Each sampling distribution should be specifically labeled with the statistic calculated and the sample size of the samples.}


\subsection{Simulating} \label{sect:SDSimulate}
Exact sampling distributions can only be computed for very small samples taken from a small population. Exact sampling distributions are difficult to show for even moderate sample sizes from moderately-sized populations. For example, there are 15504 unique samples of $n=5$ from a population of 20 individuals. How are sampling distributions examined in these and even larger situations?

There are two ways to examine sampling distributions in situations with large sample and population sizes. First, theorems exist that describe the specifics of sampling distributions under certain conditions. One such theorem is described in \sectref{sect:CLT}. Second, the computer can take many (hundreds or thousands) samples and compute the statistic for each. These statistics can then be summarized to give an indication of what the actual sampling distribution would look like. This process is called ``simulating a sampling distribution.'' We will simulate some sampling distributions here so that the theorem will be easier to understand.

Sampling distributions are simulated by drawing many samples from a population, computing the statistic of interest for each sample, and constructing a histogram of those statistics \figrefp{fig:SamplingDistributionScheme}. The computer is helpful with this simulation; however, keep in mind that the computer is basically following the same process as used in \sectref{sec:SDistDefn}, with the exception that not every sample is taken.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=4.75in]{Figs/Sampling_Distribution_Scheme.png}
  \caption{Schematic representation of the process for simulating a sampling distribution.}
  \label{fig:SamplingDistributionScheme}
\end{figure}

Let's return to the Square Lake fish population from \sectref{sect:IVPPSS} to illustrate simulating a sampling distribution. Recall that this is a hypothetical population with 1015 fish, a population distribution shown in \figref{fig:SquareLakePopn}, and parameters shown in \tabref{tab:SquareLakePopn}. Further recall that four samples of $n=50$ were removed from this population and summarized in \tabref{tab:SquareLakeSample1} and \tabref{tab:SquareLakeSample234}. Suppose, that an additional 996 samples of $n=50$ were extracted in exactly the same way as the first four, the sample mean was computed in each sample, and the 1000 sample means were collected to form the histogram in \figref{fig:SampDistSLMean50}. This histogram is a simulated sampling distribution of sample means because it represents the distribution of sample means from 1000, rather than all possible, samples.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.33\linewidth]{Figs/SampDistSLMean50-1} 
\includegraphics[width=.33\linewidth]{Figs/SampDistSLMean50-2} 

}

\caption[Histogram (\textbf{Left}) and summary statistics (\textbf{Right}) from 1000 sample mean total lengths computed from samples of $n=50$ from the Square Lake fish population]{Histogram (\textbf{Left}) and summary statistics (\textbf{Right}) from 1000 sample mean total lengths computed from samples of $n=50$ from the Square Lake fish population.}\label{fig:SampDistSLMean50}
\end{figure}


\end{knitrout}

As with the actual sampling distributions discussed previously, three characteristics (shape, center, and dispersion) are examined with simulated sampling distributions. First, \figref{fig:SampDistSLMean50} looks at least approximately normally distributed. Second, the mean of the 1000 means (=98.25) is approximately equal to the mean of the original 1015 fish in Square Lake (=98.06). These two values are not exactly the same because the simulated sampling distribution was constructed from only a ``few'' rather than all possible samples. Third, the standard error of the sample means (=4.43) is much less than the standard deviation of individuals in the original population (=31.49). So, within reasonable approximation, the concepts identified with actual sampling distributions also appear to hold for simulated sampling distributions.

As before, computing a different statistic on each sample results in a different sampling distribution. This is illustrated by comparing the sampling distributions of a variety of statistics from the same 1000 samples of size n=50 taken above \figrefp{fig:SampDistSLOther50}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.95\linewidth]{Figs/SampDistSLOther50-1} 

}

\caption[Histograms from 1000 sample median (\textbf{Left}), standard deviation (\textbf{Center}), and range (\textbf{Right}) of total lengths computed from samples of $n=50$ from the Square Lake fish population]{Histograms from 1000 sample median (\textbf{Left}), standard deviation (\textbf{Center}), and range (\textbf{Right}) of total lengths computed from samples of $n=50$ from the Square Lake fish population. Note that the value in the parameter row is the value computed from the entire population.}\label{fig:SampDistSLOther50}
\end{figure}


\end{knitrout}
\vspace{12pt}  %because knitr gobbled it up.

Simulating a sampling distribution by taking many samples of the same size from a population is powerful for two reasons. First, it reinforces the ideas of sampling variability -- i.e., each sample results in a slightly different statistic. Second, the entire concept of inferential statistics is based on theoretical sampling distributions. Simulating sampling distributions will allow us to check this theory and better visualize the theoretical concepts. From this module forward, though, remember that sampling distributions are simulated primarily as a check of theoretical concepts. In real-life, only one sample is taken from the population and the theory is used to identify the specifics of the sampling distribution.

\warn{Simulating sampling distributions is a tool for checking the theory concerning sampling distributions; however, in ``real-life'' only one sample from the population is needed.}


\section{Central Limit Theorem} \label{sect:CLT}
The sampling distribution of the sample mean was examined in the previous sections by taking all possible samples from a small population \sectrefp{sec:SDistDefn} or taking a large number of samples from a large population \sectrefp{sect:SDSimulate}. In both instances, it was observed that the sampling distribution \textit{of the sample mean} was approximately normally distributed, centered on the true mean of the population, and had a standard error that was smaller than the standard deviation of the population and decreased as $n$ increased. In this section, the Central Limit Theorem (CLT) is introduced and explored as a method to identify the specific characteristics of the sampling distribution of the sample mean without going through the process of extracting multiple samples from the population.

The CLT specifically addresses the shape, center, and dispersion of the sampling distribution of the sample means by stating that $\bar{x}\sim N\left(\mu,\frac{\sigma}{\sqrt{n}}\right)$ as long as

\vspace*{-12pt}
\begin{Itemize}
  \item $n\geq30$,
  \item $n\geq15$ and the population distribution is not strongly skewed, \textbf{or}
  \item the population distribution is normally distributed.
\end{Itemize}
\vspace*{-12pt}

Thus, the sampling distribution of $\bar{x}$ should be normally distributed \textbf{no matter what the shape of the population distribution is} as long as $n\geq30$. The CLT also suggests that $\bar{x}$ is unbiased and that the formula for the $SE_{\bar{x}}$ is $\frac{\sigma}{\sqrt{n}}$ regardless of the size of $n$. In other words, $n$ impacts the shape of the sampling distribution of the sample means, but not the center or formula for computing the standard error.

The validity of the CLT can be examined by simulating several (with different $n$) sampling distributions of $\bar{x}$ from the Square Lake population and from a strongly right-skewed exponential distribution \figrefp{fig:SampDistCLT}. Several observations about the CLT can be made from \figref{fig:SampDistCLT}. First, the sampling distribution is approximately normal for $n\geq30$ for both scenarios and is approximately normal for smaller $n$ for the Square Lake example because that population is only slightly skewed. Second, the means of all sampling distributions in both examples are approximately equal to $\mu$, regardless of $n$. Third, the dispersion of the sampling distributions (i.e., the SE of the means) becomes smaller with increasing $n$. Furthermore, the SE from the simulated results closely match the SE expected from the CLT.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.9\linewidth]{Figs/SampDistCLT-1} 

}

\caption[Sampling distribution of sample means from Square Lake and exponential populations.]{Sampling distribution of the sample mean simulated from 5000 samples of four different sample sizes extracted from the Square Lake fish population (Left) and an exponential population (Right). The shapes of the populations are shown in the top histogram. On each simulated sampling distribution, the vertical blue line is the mean of the 5000 means and the horizontal red line represents $\pm1$SE from the mean.}\label{fig:SampDistCLT}
\end{figure}


\end{knitrout}


\section{Probability Calculations} \label{sect:sdprob}
If the sample size is large enough, then the CLT states that the sampling distribution of sample means is approximately normal. If the sampling distribution is normal, then the methods from \modref{chap:NormDist} may be used to compute probabilities. Therefore questions such as ``what is the probability of observing a sample mean of less than 95 mm from a sample of $n=50$ from Square Lake?'' can be answered. In other words, questions related to the probabilitiy of \textbf{statistics} can be answered.

The question above is answered by first recalling that, for the length of all fish in Square Lake, $\mu=$98.06 and $\sigma=$31.49. Because $n=50$ is greater than 30, the CLT says that the distribution of the sample means from these samples is $\bar{x}\sim N(98.06,\frac{34.19}{\sqrt{50}})$ or $\bar{x}\sim N(98.06,4.835)$. Thus, the proportion of samples of $n=50$ from Square Lake with an $\bar{x}<95$ mm is 0.2634, which comes from computing the area less than 95 on a $N(98.06,4.835)$ distribution (\figref{fig:NormTL95}-Left).\footnote{Notice that the standard error of $\bar{x}$ is put into the \R{sd=} argument of \R{distrib()}. Recall that a standard error really is a standard deviation, it is just named differently (see \sectref{sec:SDistDefn}). R has no way of knowing whether the question is about an individual or a statistic; it requires the dispersion in either case and calls both of them \R{sd=}.}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(95,mean=98.06,sd=34.19/sqrt(50)) )
[1] 0.2634127
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.3\textwidth]{Figs/NormTL95-1} 
\includegraphics[width=.3\textwidth]{Figs/NormTL95-2} 

}

\caption[Proportion of sample means less than 95 mm on a $N(98.06,4.84)$ (Left) and $N(98.06,5.406)$ (Right) distribution]{Proportion of sample means less than 95 mm on a $N(98.06,4.84)$ (Left) and $N(98.06,5.406)$ (Right) distribution.}\label{fig:NormTL95}
\end{figure}


\end{knitrout}

Consider another question -- ``what is the probability of observing a sample mean of more than 95 mm in a sample of $n=40$ from Square Lake?''  At first glance it may appear that this question can be answered from the work done for the previous question. However, the sample sizes differ between the two questions and, because the sampling distribution depends on the sample size, a different sampling distribution is used here. Because $n>30$ the sampling distribution will be $\bar{x}\sim N(98.06,\frac{34.19}{\sqrt{40}})$ or $\bar{x}\sim N(98.06,5.406)$ (Note the different value of the SE). Thus, the answer to this question is the area to the right of 95 on a $N(98.06,5.406)$ or 0.7143 (\figref{fig:NormTL95}-Right).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(95,mean=98.06,sd=34.19/sqrt(40),lower.tail=FALSE) )
[1] 0.714319
\end{verbatim}
\end{kframe}
\end{knitrout}

\warn{Always check what sample size is being used -- if the sample size changes, then the sampling distribution changes.}

Consider two more Square Lake example questions. First, ``what is the probability of observing a sample mean of more than 95 mm in a sample of $n=$10 from Square Lake?'' This question is again about a statistic, but because $n<15$ and the population is not known to be normal it is not known that the sampling distribution will be normal. Thus, this questions cannot be answered. Second, ``What is the probability that a fish will have a length less than 85 mm?''  This question is about an individual, not a statistic as in the previous questions. Thus, the population distribution, NOT the sampling distribution, is appropriate here. However, this question also cannot be answered because the population distribution is not known to be normally distributed.

Two points are illustrated with the last two questions. First, population distributions are used for questions about individuals and sampling distributions are used for questions about statistics. Second, if the distribution is not known to be normal, no matter which distribution is used, then the probability cannot be computed.\footnote{At least with the techniques in this course.}

One issue you may have noticed is that these calculations require knowing the mean, standard deviation, and shape (if $n<30$) of the population. However, the population usually cannot be ``seen'' (recall \modref{chap:WhyStatsImportant}) and, thus, it is uncomfortable to assume so much is known about the population. The only appropriate response to this concern is that we are building towards being able to make inferences with statements based on probabilities that take into account sampling variability. To make these probabilistic statements we need to fully understand sampling distributions. These questions, while not yet realistic, will help you to better understand sampling distibutions for when they are needed to make inferences in later modules.


\section{Accuracy and Precision}
\textbf{Accuracy} and \textbf{precision} are often used to describe characteristics of a sampling distribution. Accuracy refers to how closely a statistic estimates the intended parameter. If, \textbf{on average}, a statistic is approximately equal to the parameter it was intended to estimate, then the statistic is considered \textbf{accurate}. Unbiased statistics are also accurate statistics. Precision refers to the repeatability of a statistic. A statistic is considered to be \textbf{precise} if multiple samples produce similar statistics. The standard error is a measure of precision; i.e., a high SE means low precision and a low SE means high precision.

The concepts of accuracy and precision are illustrated in \figref{fig:AccPrec}. The targets in \figref{fig:AccPrec} provide an intuitive interpretation of accuracy and precision, whereas the sampling distributions (i.e., histograms) are what statisticians look at to identify accuracy and precision. Targets in which the blue plus (i.e., mean of the means) is close to the bullseye are considered accurate (i.e., unbiased). Similarly, sampling distributions where the observed center (i.e., blue vertical line) is very close to the actual parameter (i.e., black tick labeled with a ``T'') are considered accurate. Targets in which the red dots are closely clustered are considered precise. Similarly, sampling distributions that exhibit little variability (low dispersion) are considered precise.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/AccPrec-1} 

}

\caption[Accuracy and precision model]{Model used to demonstrate accuracy, precision, and bias. The center of each target (i.e., the bullseye) and the point marked with a ``T'' (for ``truth'') represent the parameter of interest. Each dot on the target represents a statistic computed from a single sample and, thus, the many red dots on each target represent repeated samplings from the same population. The center of the samples (analogous to the center of the sampling distribution) is denoted by a blue plus-sign on the target and a blue vertical line on the histogram.}\label{fig:AccPrec}
\end{figure}


\end{knitrout}



\chapter{Hypothesis Tests} \label{chap:HypothesisTests}

\vspace*{-24pt}
\minitoc
\vspace*{24pt}

\lettrine{A}{ statistic is an imperfect estimate} of a parameter because of sampling variability. There are two calculations using the results of a single sample that recognize this imperfection and allow conclusions to be made about a parameter. First, a researcher may form an \emph{a priori} hypothesis about a parameter and then use the information in the sample to make a judgment about the ``correctness'' of that hypothesis. Second, a researcher may form, from the information in the sample, a range of values that is likely to contain the parameter. The first method is called \emph{hypothesis testing} and is the subject of this module. The second method consists of constructing a \emph{confidence region}, which is introduced in \modref{chap:ConfidenceRegions}. Specific applications of these two techniques are described in Modules \ref{chap:ZTest}-\ref{chap:GOF}.


\section{Hypothesis Testing \& The Scientific Method} \label{sect:SciMethod}
In its simplest form, the scientific method has four steps:

\vspace*{-6pt}
\begin{Enumerate}
  \item Observe and describe a natural phenomenon.
  \item Formulate a hypothesis to explain the phenomenon.
  \item Use the hypothesis to predict new observations.
  \item Experimentally test the predictions.
\end{Enumerate}
\vspace*{-6pt}

If the results of the experiment do not match the predictions, then the hypothesis is rejected and an alternative hypothesis is proposed. If the results of the experiment closely match the predictions, then belief in the hypothesis is gained, though the hypothesis will likely be subjected to further experimentation.

Statistical hypothesis testing is key to using the scientific method in many fields of study and, in fact, closely follows the scientific method in concept. Statistical hypothesis testing begins by formulating two competing statistical hypotheses from a research hypothesis. One of these hypotheses (the null) is used to predict the parameter of interest. Data is then collected and statistical methods are used to determine whether the observed statistic closely matches the prediction made from the null hypothesis or not. Probability \modrefp{chap:ProbIntro} is used to measure the degree of matching with sampling variability taken into account. This process and the theory underlying statistical hypothesis testing is explained in detail in this module.


\section{Statistical Hypotheses} \label{sec:Hypotheses}
Hypotheses are classified into two types: (1) research hypothesis and (2) statistical hypotheses. A research hypothesis is a ``wordy'' statement about the question or phenomenon that the researcher is testing. Four example research hypotheses are:

\vspace*{-8pt}
\begin{Enumerate}
  \item A medical researcher is concerned that a new medicine may change patients' mean pulse rate (from the ``known'' mean pulse rate of 82 bpm for individuals in the study population not using the new medicine).
   \item A chemist has invented an additive to car batteries that she thinks will extend the current 36 month average life of a battery.
  \item An engineer wants to determine if a new type of insulation will reduce the average heating costs of a typical house (which are currently \$145 per month).
  \item A researcher is concerned whether, on average, Alzheimer's caregivers at a particular facility are clinically depressed (as suggested by a mean Beck Depression Inventory (BDI) score greater than 25)
\end{Enumerate}
\vspace*{-8pt}

Research hypotheses are converted to statistical hypotheses that are mathematical and more easily subjected to statistical methods. There are two types of statistical hypotheses: (1) the null hypothesis and (2) the alternative hypothesis. The \textbf{null hypothesis}, abbreviated as $H_{0}$, is a specific statement of no difference between a parameter and a specific value or between two parameters. The $H_{0}$ ALWAYS contains an equals sign because it always represents ``no difference.''  The \textbf{alternative hypothesis}, abbreviated as $H_{A}$, always states that there is some sort of difference between a parameter and a specific value or between two parameters. The type of difference comes from the research hypothesis and will require use of a less than (\verb"<"), greater than (\verb">"), or not equals ($\neq$) sign. Null and alternative hypotheses that correspond to the four research hypotheses above are:

\vspace*{-8pt}
\begin{Enumerate}
  \item $H_{A}:\mu\neq82$ and $H_{0}:\mu=82$ (where $\mu$ represents the mean pulse rate for individuals in the study population that take the new medicine; thus, the alternative hypothesis represents a change from the ``normal'' pulse rate).
 \item $H_{A}:\mu>36$ and $H_{0}:\mu=36$ (where $\mu$ represents the mean life of batteries with the new additive; thus, this alternative hypothesis represents an extension of the current battery life).
  \item $H_{A}:\mu<145$ and $H_{0}:\mu=145$ (where $\mu$ represents the mean monthly heating bill for houses that receive the new type of insulation; thus, this alternative hypothesis represents a decline in heating bills from the previous ``normal'' amount).
 \item $H_{A}:\mu>25$ and $H_{0}:\mu=25$ (where $\mu$ represents the mean BDI score; thus, this alternative hypothesis represents a mean score that indicates clinical depression).
\end{Enumerate}
\vspace*{-8pt}

The sign used in the alternative hypothesis comes directly from the wording of the research hypothesis \tabrefp{tab:HAwords}. An alternative hypothesis that contains the $\neq$ sign is called a \textbf{two-tailed alternative}, as the value can be ``not equal'' to another value in two ways; i.e., less than or greater than. Alternative hypotheses with the $<$ or the $>$ signs are called \textbf{one-tailed alternatives}. The null hypothesis is easily constructed from the alternative hypothesis by replacing the sign in the alternative hypothesis with an equals sign.

\begin{table}[htbp]
  \caption{Common words that indicate which sign to use in the alternative hypothesis.}
  \label{tab:HAwords}
  \centering
  \begin{tabular}{ccc}
\hline\hline
$>$ & $<$ & $\neq$ \\
\hline
is greater than & is less than & is not equal to \\
is more than & is below & is different from \\
is larger than & is lower than & has changed from \\
is longer than & is shorter than & is not the same as \\
is bigger than & is smaller than &  \\
is better than & is reduced from &  \\
is at least & is at most &  \\
is not less than & is not more than &  \\
\hline\hline
  \end{tabular}
\end{table}


\subsection{Hypothesis Testing Concept}
\vspace{-6pt}
Statistical hypothesis testing begins by using the null hypothesis to predict what value one should expect for the mean in a sample. So, for the Square Lake example (from \modref{chap:WhyStatsImportant}), if $H_{0}:\mu=105$ and $H_{A}:\mu<105$, then one would expect, if the null hypothesis is true, that the observed sample mean would be 105. If the observed sample mean was NOT equal to 105 and sampling variability did not exist, then the prediction based on the null hypothesis would not be supported and one would conclude that the null hypothesis was incorrect. In other words, one would conclude that the population mean was not equal to 105.

Of course, sampling variability does exist and it complicates matters. The simple interpretation of not supporting $H_{0}$ because the observed sample mean did not equal the hypothesized population mean canNOT be made because, with sampling variability, one would not expect a statistic to exactly equal the parameter in the population from which the sample was extracted. For example, even if the null hypothesis was correct, one would not expect, with sampling variability, the observed sample mean to exactly equal 105; rather, one would expect the observed sample mean to be \textbf{reasonably} close to 105.

Thus, hypothesis testing is a process to determine if the difference between the observed statistic and the expected statistic based on the null hypothesis is ``large'' \textbf{relative to sampling variability}. For example, the standard error of $\bar{x}$ for samples of $n=50$ in the Square Lake example is $\frac{\sigma}{\sqrt{n}}=$$\frac{31.5}{\sqrt{50}}$$=4.45$. With this sampling variability, an observed sample mean of 103 would be considered reasonably close to 105 and one would have more belief in $H_{0}:\mu=105$ \figrefp{fig:HOSLExample}. However, an observed sample mean of 90 is further away from 105 than one would expect based on sampling variability alone and belief in $H_{0}:\mu=105$ would lessen \figrefp{fig:HOSLExample}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.53\linewidth]{Figs/HOSLExample-1} 

}

\caption[Sampling distribution of samples means of n=50 from the Square Lake population ASSUMING that $\mu$=105]{Sampling distribution of samples means of n=50 from the Square Lake population ASSUMING that $\mu$=105.}\label{fig:HOSLExample}
\end{figure}


\end{knitrout}

While the above procedure is intuitively appealing, the conclusions are not as clear when the examples chosen (i.e., samples means of 103 and 90) are not as extremely close or distant from the null hypothesized value. For example, what would one concludeif the observed sample mean was 97?. A first step in creating a more objective decision criteria is to compute the ``p-value.'' A p-value is the probability of the observed statistic or a value of the statistic more extreme assuming that the null hypothesis is true. The p-value is described in more detail below given its centrality to making conclusions about statistical hypotheses.

The meaning of the phrase ``or more extreme'' in the p-value definition is derived from the sign in $H_{A}$ \figrefp{fig:HOtails}. If $H_{A}$ is the ``less than'' situation, then ``or more extreme'' means ``less than'' or ``shade to the left'' for the probability calculation. The ``greater than'' situation is defined similarly but would result in shading to the ``right.''  In the ``not equals'' situation, ``or more extreme'' means further into the tail AND the exact same size of tail on the other side of the distribution. It is clear from \figref{fig:HOtails} why ``less than'' and ``greater than'' are one-tailed alternatives and ``not equals'' is a two-tailed alternative.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.8\linewidth]{Figs/HOtails-1} 

}

\caption[Depiction of ``or more extreme'' (red areas) in p-values for the three possible alternative hypotheses]{Depiction of ``or more extreme'' (red areas) in p-values for the three possible alternative hypotheses.}\label{fig:HOtails}
\end{figure}


\end{knitrout}

The ``assuming that the null hypothesis is true'' phrase is used to define a $\mu$ for the sampling distribution on which the p-value will be calculated. This sampling distribution is called the \textbf{null distribution} because it depends on the value of $\mu$ from the null hypothesis. One must remember that the null distribution represents the distribution of all possible sample means assuming that the null hypothesis is true; it does NOT represent the actual sample means.\footnote{Of course, unless the null hypothesis happens to be perfectly true.}  The null distribution in the Square Lake example is thus $\bar{x}\sim N(105,4.45)$ because $n=50>30$ (so the Central Limit Theorem holds), $H_{0}:\mu=105$, and SE=$\frac{31.49}{\sqrt{50}}$=$4.45$.

The p-value is computed with a ``forward'' normal distribution calculation on the null sampling distribution. For example, suppose that a sample mean of 100 was observed with $n=50$ from Square Lake (as it was in \tabref{tab:SquareLakeSample1}). The p-value in this case would be ``the probability of observing $\bar{x}=100$ or a smaller value assuming that $\mu=105$.''  This probability is computed by finding the area to the left of 100 on a $N(105,4.45)$ null distribution and is the exact same type of calculation as that made in \sectref{sect:sdprob}. Thus, this p-value of $p=0.1308$ is computed as below and shown in \figref{fig:SLpvalue1}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(100,mean=105,sd=31.49/sqrt(50)) )
[1] 0.1307722
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/SLpvalue1-1} 

}

\caption[Depiction of the p-value for the Square Lake example where $\bar{x}=100$ and $H_{A}]{Depiction of the p-value for the Square Lake example where $\bar{x}=100$ and $H_{A}:\mu<105$.}\label{fig:SLpvalue1}
\end{figure}


\end{knitrout}

Interpreting the p-value requires critically thinking about the p-value definition and how it is calculated. Small p-values appear when the observed statistic is ``far'' from the null hypothesized value. In this case there is a small probability of seeing the observed statistic ASSUMING that $H_{0}$ is true. Thus, the assumption is likely wrong and $H_{0}$ is likely incorrect. In contrast, large p-values appear when the observed statistic is close to the null hypothesized value suggesting that the assumption about $H_{0}$ may be correct.

The p-value serves as a numerical measure on which to base a conclusion about $H_{0}$. To do this objectively requires an objective definition of what it means to be a ``small'' or ``large'' p-value. Statisticians use a cut-off value, called the rejection criterion and symbolized with $\alpha$, such that p-values less than $\alpha$ are considered small and would result in rejecting $H_{0}$ as a viable hypothesis. The value of $\alpha$ is typically small, usually set at $0.05$, although $\alpha=0.01$ and $\alpha=0.10$ are also commonly used.

The choice of $\alpha$ is made by the person conducting the hypothesis test and is based on how much evidence a researcher demands before rejecting $H_{0}$. Smaller values of $\alpha$ require a larger difference between the observed statistic and the null hypothesized value and, thus, require ``more evidence'' of a difference for the $H_{0}$ to be rejected. For example, if rejection of the null hypothesis will be heavily scrutinized by regulatory agencies, then the researcher may want to be very sure before claiming a difference and should then set $\alpha$ at a smaller value, say $\alpha=0.01$. The actual choice for $\alpha$ MUST be made before collecting any data and canNOT be changed once the data has been collected. In other words, once the data are in hand, a researcher cannot lower or raise $\alpha$ to achieve a desired outcome regarding $H_{0}$.

\warn{The value of the rejection criterion ($\alpha$) is set by the researcher BEFORE data is collected.}

The null hypothesis in the Square Lake example is not rejected because the p-value (i.e., $0.1308$) is larger than any of the common values of $\alpha$. Thus, the conclusion in this example is that it is possible that the mean of the entire population is equal to 105 and it is not likely that the population mean is less than 105. In other words, observing a sample mean of 100 is likely to happen based on random sampling variability alone and it is unlikely that the null hypothesized value is incorrect.


\newpage
\section{Test Statistics and Effect Sizes}
Instead of reporting the observed statistic and the resulting p-value, it may be of interest to know how ``far'' the observed statistic was from the hypothesized value of the parameter. This is easily calculated with
\[ \text{Observed Statistic}-\text{Hypothesized Parameter} \]
where ``Hypothesized Parameter'' represents the specific value in $H_{0}$. However, the meaning of this difference is difficult to interpret without an understanding of the standard error of the statistic. For example, a difference of 10 between the observed statistic and the hypothesized parameter seems ``very different'' if the standard error is 3 but does not seem ``different'' if the standard error is 15 \figrefp{fig:EffectSizeSE}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.53\linewidth]{Figs/EffectSizeSE-1} 

}

\caption[Sampling distribution of samples means with SE=3 (Left) and SE=15 (Right)]{Sampling distribution of samples means with SE=3 (Left) and SE=15 (Right). A single observed sample mean of 90 (a difference of 10 from the hypothesized mean of 100) is shown by the red dot and arrow.}\label{fig:EffectSizeSE}
\end{figure}


\end{knitrout}

The difference between the observed statistic and the hypothesized parameter is  standardized to a common scale by dividing by the standard error of the statistic. The result is called a \emph{test statistic} and is generalized with

\begin{equation}  \label{eqn:zTestStatGeneral}
  \text{Test Statistic} = \frac{\text{Observed Statistic}-\text{Hypothesized Parameter}}{SE_{\text{Statistic}}}
\end{equation}

Thus, the test statistic \eqref{eqn:zTestStatGeneral} measures how many standard errors the observed statistic is away from the hypothesized parameter. A relatively large value is indicative of a difference that is likely not due to randomness (i.e., sampling variability) and suggests that the null hypothesis should be rejected.

The test statistic in the Square Lake Example is $\frac{100-105}{\frac{31.49}{\sqrt{50}}}$=$-1.12$. Thus, the observed mean total length of 100 mm is 1.12 standard errors below the null hypothesized mean of 105 mm. From our experience, a little over one SE from the mean is not ``extreme'' and, thus, it is not surprising that the null hypothesis was not rejected.

There are other forms for calculating test statistics, but all test statistics retain the general idea of scaling the difference between what was observed and what was expected from the null hypothesis in terms of sampling variability. Even though there is a one-to-one relationship between a test statistic and a p-value, a test statistic is often reported with a hypothesis test to give another feel for the magnitude of the difference between what was observed and what was predicted.


\section{Hypothesis Testing Concept Summary}

In summary, hypotheses are statistically examined with the following procedure.
\vspace{-8pt}
\begin{Enumerate}
  \item Construct null and alternative hypotheses from the research hypothesis.
  \item Construct an expected value of the statistic based on the null hypothesis (i.e., assume that the null hypothesis is true).
  \item Calculate an observed statistic from the individuals in a sample.
  \item Compare the difference between the observed statistic and the expected statistic based on the null hypothesis in relation to sampling variability (i.e., calculate a test statistic and p-value).
  \item Use the p-value to determine if this difference is ``large'' or not.
  \begin{Itemize}
    \item If this difference is ``large'' (i.e., p-value$<\alpha$), then reject the null hypothesis.
    \item If this difference is not ``large'' (i.e., p-value$>\alpha$), then ``Do Not Reject'' the null hypothesis.
  \end{Itemize}
\end{Enumerate}

Statisticians say ``do not reject H$_{0}$'' rather than ``accept H$_{0}$ as true'' when the p-value $>\alpha$ for two reasons. First, there are several other possible values, besides the specific value in the null hypothesis, that would lead to ``do not reject'' conclusions. For example, if a null hypothesized value of 105 was not rejected, then values of 104.99, 104.98, etc. would also likely not be rejected.\footnote{In fact, for example, the values in a 95\% confidence interval -- see \modref{chap:ConfRegions} -- represent all possible hypothesized values that would not be rejected with a two-tailed $H_{A}$ using $\alpha=0.05$.}  So, we don't say that we ``accept'' a particular hypothesized value when we know many other values would also be ``accepted.''

Second, the null hypothesis is almost always not true. Consider the null hypothesis of the Square Lake example (i.e., ``that the mean length is 105 mm''). The mean length of fish in Square Lake is undoubtedly not exactly equal to 105. It may be 104.9, 105.01, or some other more disparate value. The point is that the specific value of the hypothesis is likely never true, especially for a continuous variable. The problem is that it takes large amounts of data to be able to distinguish means that are very close to the true population mean (i.e., it is difficult to distinguish between 104.9 and 105 when sampling variability is present). Very often we will not take a sample size large enough to distinguish these subtle differences. Thus, we will say that we ``do not reject H$_{0}$'' because there simply was not enough data to reject it.




\section{Errors and Power}
The goal of hypothesis testing is to make a decision about $H_{0}$. Unfortunately, because of sampling variability, there is always a risk of making an incorrect decision. Two types of incorrect decisions can be made \tabrefp{tab:DMerrs}. A Type I error occurs when a true $H_{0}$ is falsely rejected. In other words, even if $H_{0}$ is true, there is a chance that a rare sample will occur and $H_{0}$ will be deemed incorrect. The probability of making a Type I error is set when $\alpha$ is chosen. A Type II error occurs when a false $H_{0}$ is not rejected. The probability of a Type II error is denoted by $\beta$.

\begin{table}[htbp]
  \caption{Types of decisions that can be made from a hypothesis test.}
  \label{tab:DMerrs}
  \centering
  \begin{tabular}{cc|c|c|}
    \multicolumn{1}{c}{\widen{-2}{7}{}} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Decision from Data} \\
    \cline{3-4}
    \multicolumn{1}{c}{\widen{-2}{7}{}} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Reject} & \multicolumn{1}{c|}{Not Reject} \\
    \cline{2-4}
    \multicolumn{1}{c|}{\widen{-2}{7}{Truth About}} & \multicolumn{1}{c|}{$H_{0}$} & \multicolumn{1}{c|}{Type I} & \multicolumn{1}{c|}{Correct} \\
    \cline{2-4}
    \multicolumn{1}{c|}{\widen{-2}{7}{Population}} & \multicolumn{1}{c|}{$H_{A}$} & \multicolumn{1}{c|}{Correct} & \multicolumn{1}{c|}{Type II} \\
    \cline{2-4}
  \end{tabular}
\end{table}

The decision in the Square Lake example above produced a Type II error because $H_{0}:\mu=105$ was not rejected even though we know that $\mu=98.06$ \tabrefp{tab:SquareLakePopn}. Unfortunately, in real life, it will never be known exactly when a Type I or a Type II error has been made because the true $\mu$ is not known. However, it is known that a Type I error will be made $100\alpha$\% of the time. The probability of a type II error ($\beta$), though, is never known because this probability depends on the true $\mu$. Decisions can be made, however, that affect the magnitude of $\beta$ (discussed below with power).

A concept that is very closely related to decision-making errors is the idea of \textbf{power}. Power is the probability of correctly rejecting a false $H_{0}$. In other words, it is the probability of detecting a difference from the hypothesized value if a difference really exists. Power is used to demonstrate how sensitive a hypothesis test is for identifying a difference. High power related to a $H_{0}$ that is not rejected implies that the $H_{0}$ really should not have been rejected. Conversely, low power related to a $H_{0}$ that was not rejected implies that the test was very unlikely to detect a difference, so not rejecting $H_{0}$ is not surprising nor particularly conclusive.

Power is equal to $1-\beta$ and, thus, like $\beta$ it cannot be computed directly. However, a researcher can make decisions that will positively affect power \figrefp{fig:SLPowerRelations}. For example, a researcher can increase power by increasing $\alpha$ or $n$. Increasing $n$ is more beneficial because it does not result in an increase in Type I errors as would occur with increasing $\alpha$.

In addition, power decreases as the difference between the hypothesized mean ($\mu_{0}$) and the actual mean ($\mu_{A}$) decreases \figrefp{fig:SLPowerRelations}. This means that the ability to detect increasingly smaller differences decreases. In addition, power decreases with an increasing amount of natural variability (i.e., $\sigma$; \figref{fig:SLPowerRelations}). In other words, the ability to detect a difference decreases with increasing amounts of variability among individuals. A researcher cannot control the difference between $\mu_{0}$ and $\mu_{A}$ or the value of $\sigma$. However, it is important to know that if a situation with a ``large'' amount of variability is encountered or the difference to be detected is small, the researcher will need to increase $n$ to gain power.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.55\linewidth]{Figs/SLPowerRelations-1} 

}

\caption{The relationship between one-tailed (lower) power and $\alpha$, $n$, actual mean ($\mu_{A}$), and $\sigma$. In all situations where the variable does not vary, $\mu_{0}=105$, $\mu_{A}=98.06$, $\sigma=31.49$, $n=50$, and $\alpha=0.05$. }\label{fig:SLPowerRelations}
\end{figure}


\end{knitrout}

Power cannot usually be calculated because the actual mean ($\mu_{A}$) is not known. However, in the Square Lake example, $\mu_{A}$ is known and power can be calculated in four steps:
\begin{Enumerate}
 \item Draw the sampling distribution assuming the $H_{0}$ is true (called the null distribution).
 \begin{itemize}
   \item The null distribution is $N(105,\frac{31.49}{\sqrt{50}})$ because $H_{0}:\mu=105$, $\sigma=31.49$, and $n=50$.
 \end{itemize}
 \item Find the rejection region borders (based on $\alpha$ and $H_{A}$) in terms of the value of the statistic (a ``reverse'' calculation on the null distribution).
 \begin{itemize}
   \item The rejection region is delineated by the $\bar{x}$ that has $\alpha=0.10$ to the left (because $H_{A}$ is a ``less than''). This reverse calculation on the null distribution gives $\bar{x}$=99.2928.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( rejreg <- distrib(0.10,mean=105,sd=31.49/sqrt(50),type="q") )
[1] 99.29279
\end{verbatim}
\end{kframe}
\end{knitrout}
 \end{itemize}
 \item Draw the sampling distribution corresponding to the ``actual'' parameter value (SE is the same as that for the null distribution).
 \begin{itemize}
   \item The actual $\mu$ is 98.06. Thus, the actual sampling distribution is $N(98.06,\frac{31.49}{\sqrt{50}})$.
 \end{itemize}
 \item Compute the portion of the ``actual'' sampling distribution in the REJECTION region of the null distribution (i.e., a ``forward'' calculation on the actual distribution).
 \begin{itemize}
   \item This computation is to find the area to the left of $\bar{x}$=99.2928 on $N(98.06,\frac{31.49}{\sqrt{50}})$. The area to the left of this Z is 0.6090.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(rejreg,mean=98.06,sd=31.49/sqrt(50)) )
[1] 0.6090419
\end{verbatim}
\end{kframe}
\end{knitrout}
 \end{itemize}
\end{Enumerate}

Thus, the power to detect a $\mu_{A}=98.06$ was 0.6090. This means that in only about 61\% of the samples will the false $H_{0}:\mu=105$ be correctly rejected. Thus, it is not too surprising that $H_{0}$ was not rejected in this example. If $n$ could be doubled to 100, however, the power to correctly reject $H_{0}:\mu=105$ would increase to approximately 0.82 \figrefp{fig:SLPowerRelations}.



\chapter{Confidence Regions} \label{chap:ConfidenceRegions}

\vspace*{-24pt}
\minitoc



\vspace*{24pt}
\lettrine{T}{he final result from a hypothesis test} \modrefp{chap:HypothesisTests} can feel uneventful -- i.e., either conclude that the parameter may be equal to or different from the hypothesized value.\footnote{Depending on the $H_{A}$ it may be known if the parameter is more or less than the hypothesized value.} If the parameter is thought to be different from the hypothesized value we might then say that our best guess at the parameter is the observed statistic. However, as shown in \modref{chap:SamplingDist}, a statistic is an imperfect estimate of the unknown parameter because of sampling variability. This imperfectness can be recognized by computing a range of values that is likely to contain the parameter. For example, we may make a statement such as this -- ``Our best guess for the true population mean length of fish in Square Lake is the sample mean of 98.5 mm; however, we are 95\% confident that the mean of ALL fish in the lake is between 95.9 and 101.1 mm.''  The range in the last phrase acknowledges sampling variability and is called a confidence region. In this module, the concept, calculation, and interpretation of confidence regions is explored.

\section{Confidence Concept}\label{sect:CIconcept}
An understanding of what it means to be ``95\% confident'' requires examination of multiple samples from a population, as was done in \modref{chap:SamplingDist} when considering sampling variability. In this initial discussion, only 95\% confidence intervals (CI), where a range (i.e., bounded on both ends) is computed, are considered. These simplifying restrictions and unrealistically knowing population values are used here only so that the \textbf{concept} of confidence intervals can be more easily explained. General methods for constructing other types of confidence regions with other levels of confidence are in \sectref{sec:CIConstruct}.

In the Square Lake example (introduced in \modref{chap:WhyStatsImportant}), it was known that $\mu$=98.06 and $\sigma$=31.49 \tabrefp{tab:SquareLakePopn}. Additionally, $\bar{x}=100.04$ was obtained from the first sample of $n$=50 \tabrefp{tab:SquareLakeSample1}. A 95\% CI for $\mu$ is defined as $\bar{x}\pm2SE_{\bar{x}}$. The 95\% CI for the mean total length for the Square Lake population, computed from this one sample, is $100.04\pm2\frac{31.49}{\sqrt{50}}$, $100.04\pm8.91$, or (91.13,108.95). This interval contains $\mu$ (i.e., 98.06 is between 91.13 and 108.95). In other words, this particular CI accomplished what it was intended to do; i.e., provide a range that contains $\mu$.

Despite the success observed in this first sample, not all confidence intervals will contain $\mu$. For example, four of 100 95\% confidence intervals shown in \figref{fig:CIex100} did not contain $\mu$. Thus, the researcher would have concluded that $\mu$ was in an incorrect interval four times in these 100 samples. The concept of ``confidence'' in confidence regions is related to determining how often the intervals correctly contain the parameter.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.5\linewidth]{Figs/CIex100-1} 

}

\caption[Sampling distribution of the sample mean (top) and 100 95\% confidence intervals (horizontal lines) from samples of $n$=50 from the Square Lake population]{Sampling distribution of the sample mean (top) and 100 95\% confidence intervals (horizontal lines) from samples of $n$=50 from the Square Lake population. Confidence intervals that do NOT contain $\mu$=98.06 are shown in red and with an open circle. The green shaded area represents 95\% of the sample means. See text for more explanation.}\label{fig:CIex100}
\end{figure}


\end{knitrout}

\vspace*{-12pt}
From the Central Limit Theorem, the sampling distribution of $\bar{x}$ for samples of $n$=50 is $N(98.06,\frac{31.49}{\sqrt{50}})$ or $N(98.06,4.45)$ for this known population. According to the 68-95-99.7\% Rule, it is known that 95\% of the sample means in this sampling distribution will be between $\mu\pm2SE$ or, in this specific case, between $98.06\pm2(4.45)$. The sampling distribution and this range of expected sample means is shown at the top of \figref{fig:CIex100}. Note that any sample that produced a mean (dot on the CI line) inside the expected range of sample means (light green area) also produced a 95\% CI that contained $\mu$ (i.e., black CI line with a solid circle). Because 95\% of the sample means will be within the expected range of sample means, 95\% of the 95\% CIs will contain $\mu$. So, ``95\% confident'' means that 95\% of all 95\% CIs will contain the parameter and 5\% will not. In other words, the mistake identified above will be made with 5\% of all 95\% CIs.

The specifics for constructing confidence regions with different levels of confidence is described below. However, at this point, it should be noted that the number of CIs expected to contain the parameter of interest is set by the level of confidence used to construct the CI. For example, 80\% of 80\% CIs and 90\% of 90\% CIs will contain the parameter of interest. In either case, a particular CI either does or does not contain the interval and, in real-life, we will never know whether it does or does not (i.e., we won't know the value of the parameter). However, we do know that the technique (i.e., the construction of the CI) will ``work'' (i.e., contain the parameter) a set percentage of the time. To reiterate this point, examine the 100 90\% CIs (\figref{fig:CI9080Ex}-Left) and 100 80\% CIs (\figref{fig:CI9080Ex}-Right) for the Square Lake fish length data.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.92\linewidth]{Figs/CI9080Ex-1} 

}

\caption[Sampling distribution of the sample mean (\textbf{tops}) and 100 random 90\% (\textbf{Left}) and 80\% (\textbf{Right}) confidence intervals (horizontal lines) from samples of $n$=50 from the Square Lake population]{Sampling distribution of the sample mean (\textbf{tops}) and 100 random 90\% (\textbf{Left}) and 80\% (\textbf{Right}) confidence intervals (horizontal lines) from samples of $n$=50 from the Square Lake population. Confidence intervals that do NOT contain $\mu$ are shown in red.}\label{fig:CI9080Ex}
\end{figure}


\end{knitrout}

The concept of confidence regions can be difficult to grasp at first. Thus, one should consider the following subtleties about the concept of a confidence region:

\vspace*{-8pt}
\begin{Itemize}
  \item A CI is a random variable like any other statistic. That is, each sample results in a different 95\% CI (see CI lines in Figures \ref{fig:CIex100} and \ref{fig:CI9080Ex}) just like it results in a different $\bar{x}$ (see dots on CI lines in Figures \ref{fig:CIex100} and \ref{fig:CI9080Ex}).
  \item Any CI either contains the parameter (e.g., $\mu$) or not. However, on average, 95\% of 95\% CIs will contain the parameter and 5\% will not. That is, 95\% of all possible 95\% CIs wil contain the parameter.
  \item A 95\% CI is a technique that ``works correctly'' 95\% of the time. In other words, 95\% of all 95\% CI ``capture'' the unknown parameter.
\end{Itemize}

Because of these subtleties, confidence regions are often misinterpreted. Common misinterpretations are listed below with an explanation for the misinterpretation in parentheses. These misinterpretations should be studied, compared to the interpretations discussed above, and avoided.

\vspace*{-8pt}
\begin{Enumerate}
  \item ``There is a 95\% probability that the population mean is contained in the confidence interval.'' [\textit{This is incorrect because the population mean is constant (not random), it either is or is not in a particular CI, and it will never change whether it is or is not in that CI. The CI, not the parameter, is random.}]
  \item ``There is a 95\% probability that the sample mean is contained in the confidence interval.`` [\textit{This is incorrect for the simple fact that CI are not used to estimate sample means (or, generally, statistics); they are used to estimate population means (or parameters). Furthermore, the sample mean has to be exactly in the middle of the CI (see next section).}]
  \item ``95\% of all 95\% confidence intervals are contained in the confidence interval.'' [\textit{First, this is physically impossible because each CI is the same width (if $n$ and the level of confidence stay constant). Second, it is not important how many CI are contained in a CI; interest is in whether the parameter is in the interval or not.}]
\end{Enumerate}

\vspace{-12pt}
\warn{Confidence intervals are constructed for parameters, not statistics.}

\vspace{-12pt}
\warn{Care and specificity must be used when interpreting and describing confidence intervals.}


\section{Constructing Confidence Regions} \label{sec:CIConstruct}
Not all confidence regions are designed to contain the parameter 95\% ``of the time,'' are intervals, or are computed to contain $\mu$. Confidence regions can be constructed for any level of confidence, as intervals or bounds, and for nearly all \textbf{parameters}.

The level of confidence ($C$) used will be determined by the $\alpha$ chosen for the hypothesis test; specifically, $C=100(1-\alpha)$\%. For example, if $\alpha$ is set at 0.05, then the level of confidence will be $100(1-0.05)$\% \tabrefp{tab:Calpha}. Thus, if $\alpha$ is decreased such that fewer Type I errors are made, then the confidence level will increase and more of the confidence regions will contain the parameter of interest (i.e., fewer errors). In this manner the proportion of Type I errors in hypothesis testing is linked to the proportion of errors made with confidence regions.

\begin{table}[htbp]
\caption{Several common confidence levels ($C$) and the corresponding probability of a Type I error ($\alpha$).}
\label{tab:Calpha}
\centering
\begin{tabular}{cc}
\hline\hline
$\alpha$ & $C$ \\
\hline
0.01 & 99\% \\
0.05 & 95\% \\
0.10 & 90\% \\
\hline\hline
\end{tabular}
\end{table}

The type of confidence region depends on the type of alternative hypothesis \tabrefp{tab:HAsCI}. If the alternative hypothesis is two-tailed (i.e., $\neq$), then the confidence region will be an interval (i.e., a range will be computed, as in \sectref{sect:CIconcept}). However, if the alternative hypothesis is one-tailed, then a confidence bound is used. For example, if the alternative hypothesis is a ``less than'', then interest lies in determining what is the ``largest possible value'' for the parameter (rather than a range of possible values). In other words, if the alternative hypothesis is a ``less than'', then an upper confidence bound for the parameter is constructed. In contrast, if the alternative hypothesis is a ``greater than'', then a lower confidence bound is constructed to estimate the ``smallest possible value'' for the parameter.

\begin{table}[htbp]
\caption{Confidence regions and their interpretation in relation to alternative hypotheses ($H_{A}$) types.}
\label{tab:HAsCI}
\centering
\begin{tabular}{ccc}
\hline\hline
$H_{A}$ & Confidence Region & Interpretation \\
\hline
$\neq$ & Interval & Parameter in interval \\
$<$ & Upper Bound & Parameter less than upper bound \\
$>$ & Lower Bound & Parameter greater than lower bound \\
\hline\hline
\end{tabular}
\end{table}


Fortunately, most confidence regions follow the same basic form of
  \[ \text{``Statistic''} + \text{``scaling factor''} * SE_{statistic} \]
where ``Statistic'' represents the statistic used to estimate the parameter, $SE_{statistic}$ is the standard error of that statistic, and $\text{``scaling factor''}*SE_{statistic}$ is called the margin-of-error. The scaling factor is computed from a known distribution. When $\sigma$ is known, the scaling factor is computed from a $N(0,1)$ and is called $Z^{*}$. Thus, in the case when a confidence interval is being constructed for $\mu$ and $\sigma$ is known, the specific formula for the confidence region is
  \[ \bar{x} + z^{*}\frac{\sigma}{\sqrt{n}} \]

\vspace{-8pt}
The ``scaling factor'' serves to control the width and type of confidence region. The magnitude of the scaling factor controls the relative width of the region such that the parameter is contained in the region at a rate according to the level of confidence. For example, the scaling factor for a 99\% confidence region will be set such that 99\% of the confidence regions will contain the parameter.

The sign of the scaling factor controls whether an interval, upper bound, or lower bound is computed. For example, if the alternative hypothesis is two-tailed, then $Z^{*}$ is the two values such that an area equal to the level of confidence is contained between them (\figref{fig:CIboundsZ}-Left). The two values that delineate these boundaries will be the same value but with different signs because the $N(0,1)$ is symmetric about zero. Thus, a confidence interval is computed with a scaling factor of $\pm Z^{*}$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.8\linewidth]{Figs/CIboundsZ-1} 

}

\caption[Areas (yellow) that define $Z^{*}$ for confidence regions of a parameter in a hypothesis test]{Areas (yellow) that define $Z^{*}$ for confidence regions of a parameter in a hypothesis test.}\label{fig:CIboundsZ}
\end{figure}


\end{knitrout}

\vspace{-12pt}
In contrast, if the alternative hypothesis is a ``less than'', then an upper confidence bound is desired and $Z^{*}$ has an area equal to the level of confidence LESS THAN it (\figref{fig:CIboundsZ}-Middle). As the level of confidence will always be greater than 50\%, this definition will produce a positive $Z^{*}$ so that the scaling factor will be $+Z^{*}$. Similarly, if the alternative hypothesis is a ``greater than'', then a lower confidence bound is desired and $Z^{*}$ has an area equal to the level of confidence GREATER THAN it (\figref{fig:CIboundsZ}-Right). This definition produces a negative $Z^{*}$ so that the scaling factor will be $-Z^{*}$.

\warn{When finding $Z^{*}$ for a confidence bound, the level of confidence always represents an area shaded in the same direction as the sign in $H_{A}$.}

Constructing a proper confidence region should follow the five steps below. These steps are illustrated in three examples further below.

\vspace*{-12pt}
\begin{Enumerate}
  \item Identify the level of confidence (i.e., $C=100(1-\alpha)$\%; \tabref{tab:Calpha}).
  \item Identify the type of confidence regions -- interval, lower bound, or upperbound \tabrefp{tab:HAsCI}.
  \item Determine the scaling factor.
  \item Compute the actual confidence region.
  \item Interpret the confidence region.
\end{Enumerate}
\vspace*{-4pt}

Consider the Square Lake example where $H_{A}:\mu < 105$, $\alpha=0.05$, and $\bar{x}$=100.04 from $n=50$ \tabrefp{tab:SquareLakeSample1}.

\vspace*{-8pt}
\begin{Enumerate}
  \item $C=95$\% ($100(1-0.05)$).
  \item Upper confidence bound because $H_{A}$ is a ``less than.''
  \item $Z^{*}=+$1.645 as found with [Note that \R{mean=0} and \R{sd=1} are the default settings for \R{distrib()} and can, thus, be omitted when finding a $Z^{*}$.]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(0.95,type="q") )
[1] 1.644854
\end{verbatim}
\end{kframe}
\end{knitrout}
\vspace*{6pt}
  \item $100.04+1.645\frac{31.49}{\sqrt{50}}$, $100.04+7.33$, or $107.37$.
  \item One is 95\% confident that the mean total length of ALL fish in Square Lake is less than 107.4 mm. By confident, it is meant that 95\% of all 95\% confidence regions will contain $\mu$.
\end{Enumerate}

Second, suppose that the Lake Superior ice cover data from \tabref{tab:LSIStats} (note that $\bar{x}$=107.8 and $n$=42) was tested with $H_{A}:\mu \neq 100$, $\sigma=22$, and $\alpha=0.01$.

\vspace*{-8pt}
\begin{Enumerate}
  \item $C=99$\% ($100(1-0.01)$).
  \item Confidence interval because $H_{A}$ is a ``not equals.''
  \item $Z^{*}=\pm$2.576 as found with
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(0.995,type="q") )
[1] 2.575829
\end{verbatim}
\end{kframe}
\end{knitrout}
\vspace*{6pt}
  \item $107.8\pm2.576\frac{22}{\sqrt{42}}$, $107.8\pm8.74$, or $(99.06,116.54)$.
  \item One is 95\% confident that the mean annual number of days of ice cover on Lake Superior is between 99.1 and 116.5 days. By confident, it is meant that 95\% of all 95\% confidence regions will contain $\mu$.
\end{Enumerate}

Finally, suppose that the second example hypothesis test in \modref{chap:HypothesisTests} about battery life (i.e., $H_{A}:\mu>36$ vs $H_{0}:\mu=36$) is being tested with $\alpha=0.10$. Further suppose that $\sigma=$7 and that $\bar{x}=$45 from $n=$40.

\vspace*{-8pt}
\begin{Enumerate}
  \item $C=90$\% ($100(1-0.10)$).
  \item Lower confidence bound because $H_{A}$ is a ``greater than.''
  \item $Z^{*}=$-1.282 as found with
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(0.90,type="q",lower.tail=FALSE) )
[1] -1.281552
\end{verbatim}
\end{kframe}
\end{knitrout}
\vspace*{6pt}
  \item $45-1.282\frac{7}{\sqrt{40}}$, $45-1.42$, or $43.58$
  \item One is 90\% confident that the mean life for ALL batteries with the additive is more than 43.58 months. By confident, it is meant that 95\% of all 95\% confidence regions will contain $\mu$.
\end{Enumerate}


\section[Inference Type Relationship]{Hypothesis Tests and Confidence Region Relationship}
An alternative conceptualization of confidence intervals can show how confidence regions and hypothesis tests are related. This conceptualization rests on considering the sample means that would be ``reasonable to see'' from populations with various values of $\mu$. A graphic is contructed below using the Square Lake population as an example and assuming that $\sigma$ is known (=31.49), $n=50$, and 95\% CIs are used.

First, compute the most common 95\% of sample means assuming that $\mu=70$; i.e., $70 \pm 1.960\frac{31.49}{\sqrt{50}}$ or $(61.27,78.73)$. This range is plotted as a vertical rectangle centered on $\mu=70$ (left-most rectangle) in \figref{fig:CIAlt1}-Left. Next, compute and plot the same range for a slightly larger $\mu$ (e.g., with $\mu=71$, plot $(62.27,78.73)$). Then repeat these steps for sequentially larger values of $\mu$ until a plot similar to \figref{fig:CIAlt1}-Left is constructed.

Consider very carefully what \figref{fig:CIAlt1}-Left represents. The vertical rectangles represent the ranges of the most common 95\% of sample means (values read from the y-axis) that will be produced for a particular population mean (value read from the x-axis). In essence, each vertical line represents the sample means that are likely to be observed from a population with a given population mean (x-axis).

Now suppose that $\bar{x}$=100.04 \tabrefp{tab:SquareLakeSample1}. Draw a horizontal line across \figref{fig:CIAlt1} at this value and then draw vertical lines down from where the horizontal line first enters and last leaves the band of possible sample means (\figref{fig:CIAlt1}-Right). The x-axis values that these vertical lines intercept are an approximate 95\% CI for $\mu$. The approximation is only as close as the intervals used to construct the rectangles (i.e., 1.0 mm were used here). However, the results from this graphical approach (i.e., $(92,108)$) compare favorably to the previous results from using the CI formula (i.e., $(91.27,108.73)$).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/CIAlt1-1} 
\includegraphics[width=.4\linewidth]{Figs/CIAlt1-2} 

}

\caption{Range (95\%) of sample means that would be produced by particular population means in the Square Lake fish length example (\textbf{Left}) and the ranges intercepted by $\bar{x}=100.04$ mm (\textbf{Right}).}\label{fig:CIAlt1}
\end{figure}


\end{knitrout}

Surely, the CI formula (\sectref{sec:CIConstruct}) is a more efficient and precise way to construct confidence intervals. However, this conceptualization illustrates that a confidence interval (or region, more generally) consists of population means that are likely to produce the observed sample mean. Thus, a confidence region represents possible null hypothesized population means that WOULD NOT BE rejected during hypothesis testing.

\warn{A confidence region represents null hypothesized values that would NOT be rejected.}


\section{Precision and Sample Size}
The width of a confidence interval explains how precisely the parameter is estimated. For example, narrow intervals represent precise estimates of the parameter. The width of a confidence interval is directly related to the margin-of-error which depends on (1) the standard error and (2) the scaling factor. As either of these two items gets smaller (while holding the other constant), the width of the confidence interval gets smaller.

A small standard error means that sampling variability is low and the parameter is precisely estimated by the statistic. Smaller standard errors are obtained only by increasing the sample size. A smaller standard deviation would also result in a smaller SE, but the standard deviation cannot be made smaller (i.e., it is an inherent characteristic of the population).

A smaller scaling factor is obtained by reducing the level of confidence. For example, a 90\% confidence interval uses a $Z^{*}=\pm1.645$ whereas a 95\% confidence interval uses a $Z^{*}=\pm1.960$. Thus, decreasing the confidence level narrows the CI. However, reducing the level of confidence will also increase the number of confidence intervals that do not contain the parameter. Thus, reducing the level of confidence may not be the best choice for narrowing the confidence interval.

The margin-of-error formula can be solved for $n$.
\[
  \begin{split}
    m.e. &= Z^{*}\frac{\sigma}{\sqrt{n}} \\
    \sqrt{n} &= \frac{Z^{*}\sigma}{m.e.} \\
    n &= \left(\frac{Z^{*}\sigma}{m.e}\right)^{2} \\
  \end{split}
\]

This formula can be used to find the $n$ required to estimate $\mu$ within $\pm m.e.$ units with C\% confidence assuming that $\sigma$ is known. For example, suppose that one wants to determine $n$ required to estimate the mean length of fish in Square Lake to within 5 mm with 90\% confidence knowing that the population standard deviation is 34.91. From this, $m.e.$=5, $\sigma$=34.91, and $Z^{*}$=1.645 (found previously for 90\% confidence).\footnote{Strictly, $Z^{*}\pm$=1.645, but the sign is inconsequential due to squaring in the sample size formula.} Thus, $n = \left(\frac{1.645*34.91}{5}\right)^{2} = 131.91$. Therefore, a sample of at least 132 fish from Square Lake should be taken to meet these constraints. Note that sample size calculations are always rounded up to the next integer because rounding down would produce a sample size that does not meet the desired criteria.

\warn{Always round sample size calculations up to the next integer.}

The margin-of-error and confidence level in these calculations need to come from the researcher's beliefs about how much error they can live with (i.e., chance that a confidence interval does not contain the parameter) and how precise their estimate of the mean needs to be. Values for $\sigma$ are rarely known in practice (because it is a parameter) and estimates from preliminary studies, previous similar studies, similar populations, or best guesses are often used instead. In practice, a researcher will often prepare a graph with varying values of $\sigma$ to make an informed decision of what sample size to choose.





\chapter{1-Sample Z-Test} \label{chap:ZTest}

\vspace*{-48pt}
\minitoc
\vspace*{12pt}

\lettrine{A}{ foundation for making statistical inferences} was provided in Modules \ref{chap:SamplingDist}-\ref{chap:ConfidenceRegions}. Most of the material in Modules \ref{chap:HypothesisTests} and \ref{chap:ConfidenceRegions} is related to a 1-Sample Z-test, which is formalized in this module. Other specific hypothesis tests are in Modules \ref{chap:tTest1}-\ref{chap:GOF}.

\section{11-Steps of Hypothesis Testing} \label{sec:11Steps}
\vspace*{-4pt}
Hypothesis testing is a rigorous and formal procedure for making inferences about a parameter from a statistic. The 11 steps listed below will help make sure that all aspects important to hypothesis testing are completed. These steps should be used for all hypothesis tests in this and ensuing modules.

\vspace*{-8pt}
\begin{Enumerate}
  \item State the rejection criterion ($\alpha$).
  \item State the null and alternative hypotheses to be tested and define the parameter(s).
  \item Identify (and explain why!) the hypothesis test to use (e.g., 1-Sample t, 2-sample t, etc.).
  \item Collect the data (address study type and if randomization occurred).
  \item Check all necessary assumptions (describe how you tested the validity).
  \item Calculate the appropriate statistic(s).
  \item Calculate the appropriate test statistic.
  \item Calculate the p-value.
  \item State your rejection decision about $H_{0}$.
  \item Summarize your findings in terms of the problem.
  \item \textbf{If $H_{0}$ was rejected}, compute and interpret an appropriate confidence region for the parameter.
\end{Enumerate}
\vspace*{-6pt}

The order of some of these steps is arbitrary. However Steps 1-3 \textbf{MUST} be completed before collecting data (Step 4). Further note that Step 11 will be completed only to provide a more definitive statement about the value of the parameter when the $H_{0}$ was rejected (i.e., if the parameter differs from the hypothesized value, then provide a range for which the actual parameter may exist).


\section{1-Sample Z-Test Specifics} \label{sect:ZTest}
\vspace*{-4pt}
A 1-Sample Z-Test tests $H_{0}:\mu=\mu_{0}$, where $\mu_{0}$ represents a specific value of $\mu$, when $\sigma$ is known. Other specifics of this test were discussed in previous modules and are summarized in \tabref{tab:1Zspec}.

\vspace*{-3pt}
\begin{table}[h]
\centering
\colorbox{ltgray}{
\begin{minipage}{.75\textwidth}
  \centering
	\caption{Characteristics of a 1-Sample Z-Test.}\label{tab:1Zspec}
  \begin{Itemize}
  \vspace*{-6pt}
      \item \textbf{Hypothesis:} $H_{0}:\mu=\mu_{0}$
      \item \textbf{Statistic:} $\bar{x}$
      \vspace{4pt}
      \item \textbf{Test Statistic:} $Z=\frac{\bar{x}-\mu_{0}}{\frac{\sigma}{\sqrt{n}}}$
      \vspace{4pt}
      \item \textbf{Confidence Region:} $\bar{x}+Z^{*}\frac{\sigma}{\sqrt{n}}$
      \vspace{4pt}
      \item \textbf{Assumptions:}
        \begin{Enumerate}
          \item $\sigma$ is known
          \item $n\geq30$, $n\geq15$ and the \textbf{population} is not strongly skewed, OR the \textbf{population} is normally distributed.
        \end{Enumerate}
      \item \textbf{When to Use:} Quantitative response, one population, $\sigma$ is known.
  \end{Itemize}
\end{minipage}}
\end{table}
\vspace*{-3pt}

The only test that can possibly be confused with a 1-Sample Z-Test is a 1-Sample t-Test \modrefp{chap:tTest1}, which tests the same null hypothesis but when $\sigma$ is unknown.

\vspace*{-4pt}
\subsection{Example - Intra-class Travel}
\vspace{-2pt}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:
\vspace{-4pt}
\begin{quote}
\textsl{A dean is interested in the average amount of time it takes for students to get from one class to another. In particular, she wants to determine if it takes more than 10 minutes, on average, to go between classes. In an effort to test this hypothesis, she collected a random sample of 100 intra-class travel times and found the mean to be 10.12 mins. Assume that it is known from previous studies that the distribution of intra-class times is symmetric with a standard deviation of 1.60 mins. Test the dean's hypothesis with $\alpha=0.10$.}
\end{quote}

\vspace{-12pt}
\begin{Enumerate}
  \item $\alpha$=0.10.
  \item $H_{0}:\mu=10$ mins vs. $H_{A}:\mu>10$ mins, where $\mu$ is the mean time for ALL intra-class travel events.
  \item A 1-Sample Z-Test is required because (i) a quantitative variable (intra-class travel time) was measured, (ii) individuals from one population were sampled (students at the Dean's school), and (iii) $\sigma$ is thought to be known (=1.60 mins).
  \item The data appear to be part of an observational study (the dean did not impart any conditions on the students) with a random selection of individuals.
  \item (i) $n=100\geq30$ and (ii) $\sigma$ is thought to be known (=1.60 mins).
  \item $\bar{x}$=10.12.
  \item $Z=\frac{10.12-10}{\frac{1.60}{\sqrt{100}}}$=$\frac{0.12}{0.16}$=$0.75$.
  \item p-value=$0.2266$.
  \item $H_{0}$ is not rejected because the p-value $>\alpha$=0.10.
  \item It appears that the mean time for \textbf{all} intra-class travel events is not more than 10 minutes.
  \item The confidence region is not computed when $H_{0}$ is not rejected.
\end{Enumerate}

\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\vspace*{-6pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
( distrib(10.12,mean=10,sd=1.60/sqrt(100),lower.tail=FALSE) )
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}


\section{1-Sample Z-Test in R}
\vspace*{-4pt}
If raw data exist, the calculations for a 1-Sample Z-test can be efficiently computed with \R{z.test()}. This function requires the vector of quantitative data as the first argument, the hypothesized value for $\mu$ in \R{mu=}, and the known $\sigma$ in \R{sd=}. Additionally, the type of alternative hypothesis may be declared in \R{alt=}, where \R{alt="two.sided"} (the default), \R{alt="less"}, and \R{alt="greater"} correspond to the ``not equals'', ``less than'', and ``greater than'' hypotheses, respectively. Finally, the level of confidence may be given as a proportion (between 0 and 1) in \R{conf.level=} (which defaults to 0.95). The \R{z.test()} results may be assigned to an object and submitted to \R{plot()} to visualize the test statistic and p-value.

\vspace*{-4pt}
\subsection{Body Temperature}
\vspace*{-4pt}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:
\vspace*{-8pt}
\begin{quote}
\textsl{Machowiak et al. (1992) critically examined the belief that the mean body temperature is 98.6$^{o}$F by measuring the body temperatures in a sample of healthy humans. Use their data in \href{https://raw.githubusercontent.com/droglenc/NCData/master/BodyTemp.csv}{BodyTemp.csv}, with a supposedly known $\sigma=0.63^{o}$F and $\alpha=0.01$ to determine if the mean body temperature differs from 98.6$^{o}$F.}
\end{quote}




\vspace*{-12pt}
\begin{Enumerate}
  \item $\alpha$=0.01.
  \item $H_{0}:\mu=98.6^{o}$F vs. $H_{A}:\mu\neq98.6^{o}$F, where $\mu$ is the mean body temperature for ALL healthy humans. [\textit{Note that not equals was used because the researchers want to determine if the temperature is \textbf{different from} $98.6^{o}$F.}]
  \item A 1-Sample Z-Test is required because (i) a quantitative variable (i.e., body temperature) was measured, (ii) individuals from one population were sampled (i.e., healthy humans), and (iii) $\sigma$ is thought to be known ($=0.63^{o}$F).
  \item The data appear to be part of an observational study although this is not made clear in the background information. There is also no evidence that randomization was used.
  \item (i) $n=130\geq30$ and (ii) $\sigma$ is thought to be known ($=0.63^{o}$F).
  \item $\bar{x}$ = 98.25$^{o}$F \tabrefp{tab:1Zbtex}.
  \item $Z=-6.35$ \tabrefp{tab:1Zbtex}.
  \item p-value$<0.00005$ \tabrefp{tab:1Zbtex}.
  \item Reject $H_{0}$ because p-value$<\alpha=0.01$.
  \item It appears that the mean body temperature of ALL healthy humans is less than 98.6$^{o}$F. [\textit{Note that the test was for a difference but because $\bar{x}<98.6$ this more specific conclusion can be made.}]
  \item I am 99\% confident that the mean body temperature ($\mu$) for ALL healthy humans is between 98.1 and 98.4$^{o}$F \tabrefp{tab:1Zbtex}.
\end{Enumerate}

\begin{table}[htbp]
\vspace*{-8pt}
  \caption{Results from 1-Sample Z-Test for mean body temperature.}
  \label{tab:1Zbtex}
\vspace*{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
z = -6.3482, n = 130.000, Std. Dev. = 0.630, Std. Dev. of the sample mean =
0.055, p-value = 2.178e-10
99 percent confidence interval:
 98.10690 98.39156 
sample estimates:
mean of bt$temp 
       98.24923 
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\vspace*{-6pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
bt <- read.csv("data/BodyTemp.csv")
( bt.z <- z.test(bt$temp,mu=98.6,sd=0.63,conf.level=0.99) )
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}



\chapter{1-Sample t-Test} \label{chap:tTest1}

\minitoc
\vspace*{48pt}

\lettrine{P}{rior to this module}, hypothesis testing methods required knowing $\sigma$, which is a parameter that is seldom known. When $\sigma$ is replaced by its estimator, $s$, the test statistic follows a Student's t rather than a standard normal (Z) distribution. In this module, the t-distribution is described and a 1-Sample t-Test for testing that the mean from one population equals a specific value is discussed.

\vspace*{12pt}
\section{t-distribution}\label{sect:tDist}
A t-distribution is similar to a standard normal distribution (i.e., N(0,1)) in that it is centered on 0 and is bell shaped \figrefp{fig:tvsZ}. The t-distribution differs from the standard normal distribution in that it is heavier in the tails, flatter near the center, and its exact dispersion is dictated by a quantity called the degrees-of-freedom (df). The t-distribution is ``flatter and fatter'' because of the uncertainty surrounding the use of $s$ rather than $\sigma$ in the standard error calculation.\footnote{Recall that the sample standard deviation is a statistic and is thus subject to sampling variability.}  The degrees-of-freedom are related to $n$ and generally come from the denominator in the standard deviation calculation. As the degrees-of-freedom increase, the t-distribution becomes narrower, taller, and approaches the standard normal distribution \figrefp{fig:tvsZ}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}




























\begin{figure}[hbtp]

{\centering \animategraphics[width=.4\linewidth,controls,loop,autoplay]{1}{Figs/tvsZ-}{1}{30}

}

\caption[Standard normal (black) and t-distributions (red) with varying degrees-of-freedom]{Standard normal (black) and t-distributions (red) with varying degrees-of-freedom.}\label{fig:tvsZ}
\end{figure}


\end{knitrout}

\newpage
\vspace*{12pt}
Proportional areas on a t-distribution are computed using \R{distrib()} similar to what was described for a normal distribution in Modules \ref{chap:NormDist} and \ref{chap:SamplingDist}. The major exceptions for using \R{distrib()} with a t-distribution is that \R{distrib="t"} must be used and the degrees-of-freedom must be given in \R{df=} (how to find df is discussed in subsequent sections). For example, the area right of $t=-1.456$ on a t-distribution with 9 df is 0.9103 \figrefp{fig:tarea1}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(-1.456,distrib="t",df=9,lower.tail=FALSE) )
[1] 0.9103137
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/tarea1-1} 

}

\caption[Depiction of the area to the right of $t=-1.456$ on a t-distribution with 9 df]{Depiction of the area to the right of $t=-1.456$ on a t-distribution with 9 df.}\label{fig:tarea1}
\end{figure}


\end{knitrout}

Similarly, the $t$ with an upper-tail area of 0.95 on a t-distribution with 19 df is -1.729 \figrefp{fig:tstar1}.\footnote{This ``reverse'' calculation would be $t^{*}$ for a 95\% lower confidence bound.}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(0.95,distrib="t",type="q",df=19,lower.tail=FALSE) )
[1] -1.729133
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/tstar1-1} 

}

\caption[Depiction of the value of t with an area to the right of 0.95 on a t-distribution with 19 df]{Depiction of the value of t with an area to the right of 0.95 on a t-distribution with 19 df.}\label{fig:tstar1}
\end{figure}


\end{knitrout}


\section{1-Sample t-Test Specifics} \label{sect:t1test}
A 1-Sample t-Test is similar to a 1-Sample Z-test in that both test the same $H_{0}$. The difference, as discussed above, is that when $\sigma$ is replaced by $s$, the test statistic becomes $t$ and the scaling factor for confidence regions becomes a $t^{*}$. Other aspects are similar between the two tests as shown in \tabref{tab:1tspec}.\footnote{Compare \tabref{tab:1tspec} to \tabref{tab:1Zspec}.}

\begin{table}[h]
\centering
\colorbox{ltgray}{
\begin{minipage}{.75\textwidth}
  \centering
	\caption{Characteristics of a 1-Sample t-Test.}\label{tab:1tspec}
  \begin{Itemize}
    \vspace*{-6pt}
      \item \textbf{Hypothesis:} $H_{0}:\mu=\mu_{0}$
      \item \textbf{Statistic:} $\bar{x}$
      \vspace{4pt}
      \item \textbf{Test Statistic:} $t=\frac{\bar{x}-\mu_{0}}{\frac{s}{\sqrt{n}}}$
      \vspace{4pt}
      \item \textbf{Confidence Region:} $\bar{x}+t^{*}\frac{s}{\sqrt{n}}$
      \item \textbf{df}: $n-1$
      \item \textbf{Assumptions:}
        \begin{Enumerate}
          \item $\sigma$ is UNknown
          \item $n\geq40$, $n\geq15$ and the \textbf{sample} (i.e., histogram) is not strongly skewed, OR the \textbf{sample} is normally distributed.
        \end{Enumerate}
      \item \textbf{When to Use:} Quantitative response, one population, $\sigma$ is UNknown.
  \end{Itemize}
\end{minipage}}
\end{table}

\subsection{Example - Purchase Catch of Salmon?}
\vspace*{-2pt}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:
\vspace*{-14pt}
\begin{quote}
\textsl{A prospective buyer will buy a catch of several thousand salmon if the mean weight of all salmon in the catch is at least 19.9 lbs. A random selection of 50 salmon had a mean of 20.1 and a standard deviation of 0.76 lbs. Should the buyer accept the catch at the 5\% level?}
\end{quote}
\vspace*{-12pt}

\begin{Enumerate}
    \item $\alpha$=0.05.
    \item $H_{0}:\mu=19.9$ lbs vs. $H_{A}:\mu >19.9$ lbs where $\mu$ is the mean weight of ALL salmon in the catch.
    \item A 1-Sample t-Test is required because (1) a quantitative variable (weight) was measured, (ii) individuals from one population were sampled (this catch of salmon), and (iii) $\sigma$ is \textbf{UN}known.\footnote{If $\sigma$ is given, then it will appear in the background information to the question and will be in a sentence that uses the words ``population'', ``assume that'', or ``suppose that.''}
    \item The data appear to be part of an observational study with random selection.
    \item (i) n=50 $\geq$ 40 and (ii) $\sigma$ is unknown.
    \item $\bar{x}$ = 20.1 lbs (and $s$ = 0.76 lbs).
    \item $t$ = $\frac{20.1-19.9}{\frac{0.76}{\sqrt{50}}}$ = $\frac{0.2}{0.107}$ = 1.87 with df = 50-1 = 49.
    \item p-value = $0.0337$.
    \item $H_{0}$ is rejected because the p-value $< \alpha$.
    \item The average weight of ALL salmon in this catch appears to be greater than 19.9 lbs; thus, the buyer should accept this catch of salmon.
    \item I am 95\% confident that the mean weight of ALL salmon in the catch is greater than 19.92 lbs (i.e., $20.1-1.677\frac{0.76}{\sqrt{50}}$ = $20.1-0.18$ = $19.92$).
\end{Enumerate}

\vspace{-6pt}
\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\vspace{-6pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
( pval <- distrib(1.87,distrib="t",df=49,lower.tail=FALSE) )
( zstar <- distrib(0.95,distrib="t",type="q",df=49,lower.tail=FALSE) )
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}


\section{1-Sample t-Test in R}
\vspace*{-3pt}
If raw data exist, the calculations for a 1-Sample t-test can be efficiently computed with \R{t.test()}. The arguments to \R{t.test()} are the same as those for \R{z.test()}, with the exception that \R{sd=} is not used with \R{t.test()}. Thus, \R{t.test()} requires the vector of quantitative data as the first argument, the null hypothesized value for $\mu$ in \R{mu=}, the type of alternative hypothesis in \R{alt=} (again, can be \R{alt="two.sided"} (the default), \R{alt="less"}, or \R{alt="greater"}), and the level of confidence as a proportion in \R{conf.level=} (defaults to 0.95). The use of \R{t.test()} is illustrated in the following example.

\subsection{Example - Crab Body Temperature}
\vspace*{-6pt}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:

\vspace*{-12pt}
\begin{quote}
\textsl{A marine biologist wants to determine if the body temperature of crabs exposed to ambient air temperature is different than the ambient air temperature. The biologist exposed a sample of 25 crabs to an air temperature of 24.3$^{o}$C for several minutes and then measured the body temperature of each crab (shown below). Test the biologist's question at the 5\% level.}
\end{quote}

\vspace*{-18pt}
\begin{Verbatim}[xleftmargin=15mm]
22.9,22.9,23.3,23.5,23.9,23.9,24.0,24.3,24.5,24.6,24.6,24.8,24.8,
25.1,25.4,25.4,25.5,25.5,25.8,26.1,26.2,26.3,27.0,27.3,28.1
\end{Verbatim}
\vspace*{-18pt}




\begin{Enumerate}
    \item $\alpha$ = 0.05.
    \item $H_{0}:\mu$ = 24.3$^{o}$C vs. $H_{A}:\mu \neq$ 24.3$^{o}$C, where $\mu$ is the mean body temperature of ALL crabs.
    \item A 1-Sample t-Test is required because (1) a quantitative variable (temperature) was measured, (ii) individuals from one population were sampled (an ill-defined population of crabs), and (iii) $\sigma$ is \textbf{UN}known.
    \item The data appear to be part of an experimental study (the temperature was controlled) with no suggestion of random selection of individuals.
    \item (i) n = 25 $\geq$ 15 and the sample distribution of crab temperatures appears to be only slightly right-skewed \figrefp{fig:CrabTempHist} and (ii) $\sigma$ is \textbf{UN}known.
    \item $\bar{x}$ = 25.0$^{o}$C \tabrefp{tab:1tcrabs}.
    \item $t$ = 2.713 with 24 df \tabrefp{tab:1tcrabs}.
    \item p-value = $0.0121$ \tabrefp{tab:1tcrabs}.
    \item $H_{0}$ is rejected because the p-value $<\alpha$.
    \item It appears that the average body temperature of ALL crabs is greater than the ambient temperature of 24.3$^{o}$C.
    \item I am 95\% confident that the mean body temperature of ALL crabs is between 24.5$^{o}$C and 25.6$^{o}$C \tabrefp{tab:1tcrabs}.
\end{Enumerate}

\vspace*{-4pt}
\begin{table}[h]
  \caption{Results from 1-Sample t-Test for body temperature of crabs.}
  \label{tab:1tcrabs}
  \vspace*{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
t = 2.7128, df = 24, p-value = 0.01215
95 percent confidence interval:
 24.47413 25.58187 
sample estimates:
mean of x 
   25.028 
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/CrabTempHist-1} 

}

\caption[Histogram of the body temperatures of crabs exposed to an ambient temperature of $24.3^{o}$C]{Histogram of the body temperatures of crabs exposed to an ambient temperature of $24.3^{o}$C.}\label{fig:CrabTempHist}
\end{figure}


\end{knitrout}

\vspace*{-4pt}
\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\vspace*{-6pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
df <- read.csv("data/CrabTemps.csv")
hist(~ct,data=df,xlab="Crab Body Temp (C)")
( ct.t <- t.test(df$ct,mu=24.3,conf.level=0.95) )
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}



\chapter{2-Sample t-Test} \label{chap:tTest2}

\vspace*{-12pt}
\minitoc
\vspace*{24pt}

\lettrine{W}{hile it is often useful to test whether a population mean} differs from a specific value (i.e., with the 1-Sample t-Test of \modref{chap:tTest1}), there are many instances where interest is in whether means from two populations differ. For example, is there a difference in mean income between males and females, in mean test scores between students from high- and low-income families, in mean percent body fat between raccoons from southern and northern Wisconsin, or in mean amount of milk produced from cows provided with a hormone or a placebo. In all of these situations, interest is identifying if a difference in population means exists between two populations (males and females, students from high- and low-income families, raccoons from southern and northern Wisconsin, cows given a hormone or a placebo). A \textbf{2-Sample t-Test} is used in these situations and is the subject of this module.


\section{2-Sample t-Test Specifics}
\vspace*{-3pt}
In a 2-Sample t-Test, $H_{0}:\mu_{1}=\mu_{2}$ states that the two population means are equal. This can be rewritten as $H_{0}:\mu_{1}-\mu_{2}=0$, because the difference between two population means should be zero if the two population means are equal. With this $H_{0}$, the ``parameter'' is $\mu_{1}-\mu_{2}$ and the corresponding statistic is $\bar{x}_{1}-\bar{x}_{2}$. Thus, a 2-Sample t-Test is focused on the difference in population means.

When looking at the ``general'' test statistic formula (i.e., Equation \eqref{eqn:zTestStatGeneral}) of

\vspace*{-6pt}
\[ \text{Test Statistic} = \frac{\text{Observed Statistic}-\text{Hypothesized Parameter}}{SE_{\text{Statistic}}} \]
\vspace*{-6pt}

it is apparent that the SE of $\bar{x}_{1}-\bar{x}_{2}$ (i.e., the statistic) is needed. Unfortunately, the calculation of this standard error depends on whether the two population variances are equal or not. When the variances are approximately equal (discussed in \sectref{sect:LevenesTest}), the standard error of $\bar{x}_{1}-\bar{x}_{2}$ is

\[ SE_{\bar{x}_{1}-\bar{x}_{2}}= \sqrt{s_{p}^{2}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}} \right)}  \]

where $n_{1}$ and $n_{2}$ are the sample sizes for the two populations and $s_{p}^{2}$ is the ``pooled sample variance'' computed as a weighted average of the two sample variances ($s_{1}^{2}$ and $s_{2}^{2}$), or

\[s_{p}^{2}=\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2} \]

The degrees-of-freedom for the 2-Sample t-Test with equal variances come from the denominator of the pooled variance calculation; i.e., $df=n_{1}+n_{2}-2$. The specifics of the 2-Sample t-Test are in \tabref{tab:2tspec}.

\vspace*{12pt}
\begin{table}[h]
\centering
\colorbox{ltgray}{
\begin{minipage}{.8\textwidth}
  \centering
	\caption{Characteristics of a 2-Sample t-Test with equal variances.}\label{tab:2tspec}
  \begin{Itemize}
      \item \textbf{Hypothesis:} $H_{0}:\mu_{1}-\mu_{2}=0$
      \item \textbf{Statistic:} $\bar{x}_{1}-\bar{x}_{2}$
      \item \textbf{Test Statistic:} $t=\frac{\bar{x}_{1}-\bar{x}_{2}-0}{\sqrt{s_{p}^{2}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}} \right)}}$ where $s_{p}^{2}=\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}$.
      \vspace{6pt}
      \item \textbf{Confidence Region:} $(\bar{x}_{1}-\bar{x}_{2})+t^{*}\sqrt{s_{p}^{2}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}} \right)}$
      \vspace{6pt}
      \item \textbf{df:} $n_{1}+n_{2}-2$
      \item \textbf{Assumptions:} $n_{1}+n_{2}\geq40$, $n_{1}+n_{2}-2\geq15$ and \textbf{each sample} (i.e., histogram) is not strongly skewed, OR \textbf{each sample} is normally distributed.
      \item \textbf{When to Use:} Quantitative response, two populations, individuals are independent between populations.
  \end{Itemize}
\end{minipage}}
\end{table}
\vspace*{12pt}

\warn{The $s_{p}^{2}$ calculation can be ``checked'' by determining if the value of $s_{p}^{2}$ is between $s_{1}^{2}$ and $s_{2}^{2}$ or if the value of $\sqrt{s_{p}^{2}}$ is between $s_{1}$ and $s_{2}$.}

A 2-Sample t-Test is often used to test an alternative hypothesis of simply finding a difference between the two populations. However, if the null hypothesis is rejected in these instances (thus, identifying a significant difference between the two populations), then care should be taken to specifically describe how the two populations differ. If the statistic is negative, then the mean of the first population is lower than the mean of the second population and, if the statistic is positive, then the mean of the first population is larger than the mean of the second population. The values of the confidence region should be used to identify how much larger or smaller the mean from one population is compared to the mean of the other population.


\newpage
\section{Testing for Equal Variances}\label{sect:LevenesTest}
As noted above, the methods of a 2-Sample t-Test differ depending on whether the two population variances are equal or not. This should present a problem to you because the population variances are parameters and are typically not known.\footnote{Actually, the population variances don't have to be known, it just needs to be known whether they are equal or not.} The question of whether these parameters are equal or not is answered with a hypothesis test, as has been done with all other questions about parameters.

A Levene's Test is used to determine whether two population variances are equal. The specifics of the Levene's test are not examined in detail here, rather you only need to know that $H_{0}:\sigma_{1}^{2}=\sigma_{2}^{2}$ is tested against $H_{A}:\sigma_{1}^{2}\neq\sigma_{2}^{2}$. We will use computer software to compute the p-value for this test (without further detail). If the Levene's Test p-value $< \alpha$, then $H_{0}$ is rejected and the population variances are considered unequal. If the p-value $> \alpha$, then $H_{0}$ is not rejected and the population variances are considered equal.


\subsection{Example - Corn and Fertilizers}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:
\vspace{-4pt}
\begin{quote}
\textsl{An agricultural researcher thought that corn plants grown in pots exposed to a certain type of synthetic fertilizer would grow taller than plants exposed to an organic fertilizer. To collect data to test this idea, he grew 50 corn plants in individual pots -- 25 were treated with organic fertilizer and 25 were treated with synthetic fertilizer. Each pot contained soil from a well-mixed common source and was planted in the same greenhouse. Each plant was similar in all regards (similar genetics, age, etc.). Use the results (heights of individual plants) in \tabref{tab:CornFert} to test the researcher's hypothesis at the 5\% level.}
\end{quote}

\vspace*{-4pt}
\begin{table}[htbp]
  \caption{Summary statistics of the corn plant height in two treatments.}
  \label{tab:CornFert}
\vspace*{-5pt}
  \begin{center}
    \begin{minipage}{4.5in}
      \begin{Verbatim}
        Synthetic  Organic
means:    51.46     47.49
SD:       5.975     6.721          Levene's Test:  p=0.1341
      \end{Verbatim}
    \end{minipage}
  \end{center}
\end{table}
\vspace*{-18pt}

\begin{Enumerate}
  \item $\alpha$ = 0.05.
  \item $H_{0}:\mu_{s}-\mu_{o} = 0$ vs $H_{A}:\mu_{s}-\mu_{o} > 0$, where $\mu$ is the mean plant height, $s$ represents synthetic fertilizer, and $o$ represents organic fertilizer. [\textit{Note that positive differences represent larger values for synthetic fertilizer; thus, $H_{A}$ represents synthetic fertilizer producing taller plants.}]
  \item A 2-Sample t-Test is required because (i) a quantitative variable (height) was measured, (ii) two populations were sampled (synthetic and organic fertilizers), and (iii) plants in the two populations were \textbf{IN}dependent as the plants were not paired, plants were not tested over time, etc.
  \item The data appear to be part of an experiment (the researcher imposed the treatments on the plants) with no clear indication of random selection of plants or random allocation of plants to the two treatments.
  \item (i) $n_{s}+n_{o}$ = 50 $> 40$, (ii) individuals in the two populations are independent as discussed above, and (iii) the population variances appear to be equal because the Levene's Test p-value (0.1341) is $> \alpha$.
  \item $\bar{x}_{s}-\bar{x}_{0}$ = 51.46 $-$ 47.49 = 3.97. Additionally,
    \[s_{p}^{2}=\frac{(25-1)5.975^{2}+(25-1)6.721^{2}}{25+25-2} = 40.44 \]
and
    \[ SE_{\bar{x}_{s}-\bar{x}_{o}}=\sqrt{40.44\left(\frac{1}{25}+\frac{1}{25} \right)} = 1.799  \]
  \item $t$ = $\frac{3.97-0}{1.799}$ = $\frac{3.97}{1.799}$ = 2.207 with 25+25-2 = 48 df.
  \item p-value = $0.0161$.
  \item The $H_{0}$ is rejected because the p-value $< \alpha$.
  \item The average height of the corn plants appears to be greater for plants grown with synthetic fertilizer than for plants grown with organic fertilizer.
  \item I am 95\% confident that plants grown with synthetic fertilizer are more than 0.95 cm taller, on average, than plants grown with the organic fertilizer. [Note $3.97-1.677*1.799$ = 3.97 $-$ 3.02 = 0.95.]
\end{Enumerate}

\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\vspace{-6pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
( pval <- distrib(2.207,distrib="t",df=48,lower.tail=FALSE) )
( tstar <- distrib(0.95,distrib="t",df=48,type="q",lower.tail=FALSE) )
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}


\vspace*{12pt}
\subsection{Example - Music and Anxiety}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:
\vspace{-4pt}
\begin{quote}
\textsl{An oral surgeon conducted an experiment to determine if background music decreased the anxiety level of patients during tooth extraction. Over a one-month period, 32 patients had a tooth removed while listening to music and 36 had a tooth removed without listening to music. Each patient was given a questionnaire following the extraction. Answers to the questionnaire were converted to a numeric scale to measure the patient's level of anxiety (larger numbers mean more anxiety). For those given background music, the mean anxiety level was 4.2 (with a standard deviation of 1.2), while the group without music had a mean of 5.9 (with a standard deviation of 1.9). The surgeon also reported a Levene's test p-value of 0.089. Test the surgeon's hypothesis at the 5\% level.}
\end{quote}

\begin{Enumerate}
  \item $\alpha$ = 0.05.
  \item $H_{0}:\mu_{w}-\mu_{wo} = 0$ vs $H_{A}:\mu_{w}-\mu_{wo}<0$, where $\mu$ is the mean anxiety level, $w$ represents patients ``with'', and $wo$ represents ``without'' music. [\textit{Note that negative numbers represent lower anxiety values in patients in the ``with music'' treatment. Thus, $H_{A}$ suggests lower anxiety in paients with music.}]
  \item A 2-Sample t-Test is required because (i) a quantitative variable (anxiety level) was measured, (ii) two populations were sampled (music or no music), and (iii) individuals in the two populations are independent (i.e., they were not paired, were not otherwise related, etc.).
  \item The data appear to be an experiment as the music treatment was imparted by the surgion, but there is no obvious random selection or allocation in this study.
  \item (i) $n_{w}+n_{wo}$ = 68 $>$ 40, (ii) individuals in the two populations are independent as described above, and (iii)the two population variances appear to be equal because the Levene's Test p-value of 0.089 is greater than $\alpha$.
  \item $\bar{x}_{w}-\bar{x}_{wo}$ = 4.2 $-$ 5.9 = -1.7. Additionally,
    \[s_{p}^{2}=\frac{(32-1)1.2^{2}+(36-1)1.9^{2}}{32+36-2} = 2.59 \]
and
    \[ SE_{\bar{x}_{w}-\bar{x}_{wo}}=\sqrt{2.59\left(\frac{1}{32}+\frac{1}{36} \right)} = 0.391  \]
  \item $t$ = $\frac{-1.7-0}{0.391}$ = -4.348 with 32+36-2 = 66 df.
  \item p-value $<0.00005$.
  \item $H_{0}$ is rejected because the p-value $< \alpha$.
  \item The mean anxiety level appears to be lower when music was played for the patients.
  \item I am 95\% confident that the mean anxiety level is more than 1.05 points lower, on average, when music is played than when it is not. [Note -1.7+1.668*0.391 = -1.7+0.65 = -1.05.
\end{Enumerate}

\vspace{-8pt}
\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\vspace*{-6pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
( pval <- distrib(-4.348,distrib="t",df=66) )
( tstar <- distrib(0.95,distrib="t",df=66,type="q") )
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}


\vspace*{12pt}
\section{2-Sample t-Tests in R}\label{sect:2tData}
\vspace*{-6pt}
\subsection{Data Format}
\vspace*{-4pt}
Data must be in stacked format (as described in \sectref{sect:REnterData}) for a 2-Sample t-Test. Stacked data has measurements in one column and group labels for the measurement in another column. Thus, each row corresponds to a measurement and the group for a single individual. As an example, BOD measurements from either the inlet or outlet to an aquaculture facility are shown below. These data are stacked because each row corresponds to one individual (a water sample) with one column of (BOD) measurements and another column for which group the individual belongs.
\vspace*{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
   BOD    src
 6.782  inlet
 5.809  inlet
 8.063 outlet
 8.001 outlet
\end{verbatim}
\end{kframe}
\end{knitrout}

\vspace*{-8pt}
\subsection{Levene's Test}
\vspace*{-4pt}
Before conducting a 2-Sample t-Test, the assumption of equal population variances must be tested with Levene's test. The Levene's test is computed with \R{levenesTest()}, where the first argument is a model formula of the form \R{response\TILDE group}, where \R{response} represents the quantitative measurements and \R{group} represents the group factor variable.\footnote{This is the same model formula introduced in \sectref{sect:MultGroups} for summarizing multiple groups of data.} The data.frame containing \R{response} and \R{group} is given in \R{data=}.

\vspace*{-8pt}
\subsection{2-Sample t-Test}
\vspace*{-4pt}
A 2-Sample t-Test is computed with \R{t.test()}, where the first argument is the same formula as in \R{levenesTest()} (and, thus, same \R{data=}). Additionally, the following arguments may need to be specified.
\vspace*{-8pt}
\begin{Itemize}
  \item \R{mu=}: The specific value in $H_{0}$. For a 2-Sample t-Test this is usually 0, which is the default.
  \item \R{alt=}: A string that indicates the type of $H_{A}$ (i.e., \R{"two.sided"} (default), \R{"greater"}, or \R{"less"}).
  \item \R{conf.level=}: The level of confidence (default is \R{0.95}) used for the confidence region of $\mu_{1}-\mu_{2}$.
  \item \R{var.equal=}: A logical value that indicates whether the two population variances should be considered equal or not. If \R{TRUE}, then the pooled sample variance is calculated and used in the standard error. The default \R{FALSE}, to assume UNequal variances.
\end{Itemize}

\vspace*{-8pt}
\warn{\R{var.equal=TRUE} must be in \R{t.test()} to assume equal variances. This is NOT the default.}

R computes the difference among populations as the alphabetically ``first'' level minus the alphabetically ``second'' level. For example, if the two levels are \var{inlet} and \var{outlet}, then R will compute $\bar{x}_{outlet}-\bar{x}_{inlet}$. If this is not the order you want, then you need to change the order of the levels by using \R{levels=} in \R{factor()} (as described in Modules \ref{chap:UnivEDACat} and \ref{chap:BivEDACat}). For example, the order of the levels of \var{src} in the \var{aqua} data.frame is changed below.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> aqua$src <- factor(aqua$src,levels=c("outlet","inlet"))
> levels(aqua$src)
[1] "outlet" "inlet" 
\end{verbatim}
\end{kframe}
\end{knitrout}


\vspace*{12pt}
\subsection{Example - BOD in Aquaculture Water}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:
\vspace*{-6pt}
\begin{quote}
\textsl{An aquaculture farm takes water from a stream and returns it to the stream after it has circulated through the fish tanks. The owner has taken steps to reduce the level of organic matter in the water released back into the stream. However, he is still concerned that water returned to the stream may contain heightened levels of organic matter. To determine if this is true, he took samples of water at the intake and, at other times, downstream from the outlet and recorded the biological oxygen demand (BOD) as a measure of the organics in the effluent (a higher BOD at the outlet would imply heightened levels of organics are being released to the stream). The owner's data are recorded in \href{https://raw.githubusercontent.com/droglenc/NCData/master/BOD.csv}{BOD.csv}}. Test for any evidence (i.e., at the 10\% level) to support the owner's concern.
\end{quote}
\vspace*{-12pt}




\begin{Enumerate}
  \item $\alpha$ = 0.10.
  \item $H_{0}:\mu_{outlet}-\mu_{inlet}=0$ vs $H_{A}:\mu_{outlet}-\mu_{inlet}>0$, where $\mu$ is the mean BOD, $outlet$ represents the outlet source, and $inlet$ represents the inlet source. [\textit{Positive differences represent larger values at the outlet, which implies that BOD is higher in the water released from the facility. Thus, $H_{A}$ represents the owner's concern. Further note that the order of subtraction could have been reversed such that the owner's concern would require a ``less than'' $H_{A}$. This is simply a matter of choice. However, note that the order of the levels has to be changed in R to use my choice of hypotheses.}]
  \item A 2-Sample t-Test is required because (i) a quantitative variable (BOD level) was measured, (ii) two populations were sampled (outlet and inlet), and (ii) the sets of samples were \textbf{IN}dependent (note that it said that the outlet samples came from different times then the inlet samples).
  \item The data appear to be part of an observational study with no obvious randomization.
  \item (i) n = 20 $>$15 and the histograms \figrefp{fig:AquaHist} are inconclusive about the shape because of the small sample size in each group (it appears that the \var{inlet} data is not strongly skewed, whereas the \var{outlet} data is skewed, which may invalidate the results of this hypothesis test; however, I continued to make a complete example), (ii) individuals in the two samples are independent as discussed above, and (iii) the variances appear to be equal because the Levene's test p-value (=$0.5913$) is greater than $\alpha$.
  \item $\bar{x}_{outlet}-\bar{x}_{inlet}$ = 8.69 $-$ 6.65 = 2.03 \tabrefp{tab:2tBOD}.
  \item $t$ = 8.994 with 18 df \tabrefp{tab:2tBOD}.
  \item p-value $<0.00005$ \tabrefp{tab:2tBOD}.
  \item $H_{0}$ is rejected because the p-value $<\alpha$.
  \item The average BOD is greater at the outlet than at the inlet to the aquaculture facility. Thus, the aquaculture facility appears to add to the biological oxygen demand of the water and the farmer's concern is warranted.
  \item I am 90\% confident that the mean BOD measurement at the outlet is AT LEAST 1.73 GREATER than the mean BOD measurement at the inlet \tabrefp{tab:2tBOD}.
\end{Enumerate}

\begin{table}[h]
  \caption{Results from the 2-Sample t-Test for differences in BOD between the inlet and outlet of an aquaculture facility.}
  \label{tab:2tBOD}
\vspace*{-8pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
t = 8.994, df = 18, p-value = 2.224e-08
90 percent confidence interval:
 1.732704      Inf 
sample estimates:
mean in group outlet  mean in group inlet 
              8.6873               6.6538 
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{figure}[hbtp]

{\centering \includegraphics[width=.7\linewidth]{Figs/AquaHist-1} 

}

\caption[Histogram of the BOD measurements at the outlet and inlet of the aquaculture facility]{Histogram of the BOD measurements at the outlet and inlet of the aquaculture facility.}\label{fig:AquaHist}
\end{figure}


\end{knitrout}

\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
aqua <- read.csv("data/BOD.csv")
aqua$src <- factor(aqua$src,levels=c("outlet","inlet"))
hist(BOD~src,data=aqua,xlab="BOD Measurement")
levenesTest(BOD~src,data=aqua)
( aqua.t <- t.test(BOD~src,data=aqua,var.equal=TRUE,alt="greater",conf.level=0.90) )
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}



\chapter[Chi-Square Test]{Chi-Square Test} \label{chap:ChiSquare}

\minitoc

\lettrine{S}{ituations where a categorical} response variable is recorded would be summarized with a frequency or percentage table (see Modules \ref{chap:UnivEDACat} and \ref{chap:BivEDACat}). The appropriate test statistic in these situations is a chi-square rather than a t. The Chi-Square Test test statistic follows a chi-square distribution, which is introduced below. The rest of this module is dedicated to the general Chi-Square Test where the distribution of a categorical response variable is compared between two or more populations. The related goodness-of-fit test for a categorical response recorded for only one population is introduced in \modref{chap:GOF}.


\section{Chi-Square Distribution}\label{sect:ChiDist}
A chi-square ($\chi^2$) distribution is generally right-skewed \figrefp{fig:chiDist}, with the exact shape dictated by the degrees-of-freedom (df; as df increase, the sharpness of the skew decreases; \figref{fig:chiDist}). In its simplest form, the $\chi^2$ distribution arises as a sampling distribution for the $\chi^2$ test statistic,

\[ \chi^{2} = \Sum_{cells}\frac{(Observed-Expected)^{2}}{Expected} \]

where ``Observed'' and ``Expected'' represent the observed and expected individuals in the cells of frequency tables (see \modref{chap:UnivEDACat} and \modref{chap:BivEDACat}) and ``cells'' generically represents the number of cells in one of these tables. Thus, the $\chi^2$ distribution arises from comparing frequencies in two tables.\footnote{Subsequent sections demonstrate how this test statistic is used to compare observed frequencies (i.e., from a sample) to a table of expected frequencies (i.e., from a null hypothesis).}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}








\begin{figure}[hbtp]

{\centering \animategraphics[width=.3\linewidth,controls,loop,autoplay]{1}{Figs/chiDist-}{1}{10}

}

\caption[$\chi^2$ distributions with varying degrees-of-freedom]{$\chi^2$ distributions with varying degrees-of-freedom.}\label{fig:chiDist}
\end{figure}


\end{knitrout}

Unlike the normal and t distributions, the $\chi^2$ distribution always represents the two-tailed situation, although the ``two tails'' will appear as one tail on the right side of the distribution. The simplest explanation for this characteristic is that the ``squaring'' in the calculation of the $\chi^{2}$ test statistic results in what would be a ``negative tail'' being ``folded over'' onto what is the ``positive tail.''  Thus, all probability (i.e., area) calculations on a $\chi^{2}$ distribution represent the two-tailed alternative hypotheses.

Proportional areas on a $\chi^2$ distribution are computed with \R{disrib()},  similarly to what was described for normal and t distributions in Modules \ref{chap:NormDist}, \ref{chap:SamplingDist}, and \ref{chap:tTest1}. The major difference for using \R{distrib()} with a $\chi^2$ distribution is that \R{distrib="chisq"} must be used and the degrees-of-freedom must be given to \R{df=} (how to find the df will be discussed in subsequent sections). In addition, if calculating a p-value, then \R{lower.tail=FALSE} is always used because the upper-tail probability represents the two-tailed alternative hypothesis inherent to all Chi-Square Tests. For example, the area right of $\chi^2=6.456$ on a $\chi^2$ distribution with 2 df is 0.0396 \figrefp{fig:chiarea1}.
\vspace{-3pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( distrib(6.456,distrib="chisq",df=2,lower.tail=FALSE) )
[1] 0.03963669
\end{verbatim}
\end{kframe}\begin{figure}[hbtp]

{\centering \includegraphics[width=.4\linewidth]{Figs/chiarea1-1} 

}

\caption[Depiction of the area to the right of $\chi^2=6.456$ on a $\chi^2$ distribution with 2 df]{Depiction of the area to the right of $\chi^2=6.456$ on a $\chi^2$ distribution with 2 df.}\label{fig:chiarea1}
\end{figure}


\end{knitrout}


\section{Chi-Square Test Specifics}
\vspace{-3pt}
Researchers commonly want to compare the distribution of individuals into the levels of a categorical variable among two or more populations. For example, researchers may want to determine if the disribution of failing students differs between males and females, if the distribution of kids playing sports differs between kids from high- or low-income families, if the distribution of four major plant species differs between two locations, or if the distribution of responses to a five-choice question differs between respondents from neighboring counties. All of these questions have a categorical response variable (fail or not, play sport or not, plant species, answer to five-choice question) compared among two or more populations (gender, income category, two locations, neighboring counties). The Chi-Square Test, the subject of this module, can be used for each of these situations.\footnote{The Chi-Square Test is quite flexible and can be derived from different types of hypotheses than those described here.}

\subsection{Hypotheses}
The statistical hypotheses for a Chi-Square Test are ``wordy.''  To explore this, let's first assume that a two-way frequency table (see \modref{chap:BivEDACat}) will summarize the data where the rows correspond to separate populations and the columns correspond to levels of the response variable. In this organization, the Chi-Square Test null hypothesis is that the row percentages are equal -- i.e., ``the percentage distribution of individuals into the levels of the response variable is the same for all populations.''  The alternative hypothesis states that there is some difference among the row percentages -- i.e., ``the percentage distribution of individuals into the levels of the response variable is NOT the same for all populations.''

As one example (more are shown below), consider the following:
\vspace{-6pt}
\begin{quote}
\textsl{An association of Christmas tree growers in Indiana sponsored a survey of Indiana households to help improve the marketing of Christmas trees. Of the 261 rural households, 64 had a natural tree (as compared to an artificial tree). Of the 160 urban households, 89 had a natural tree. Use these results to determine, at the 10\% level, if the distribution of households with a natural tree differed between rural and urban households.}
\end{quote}
\vspace{-3pt}

The hypotheses for this situation are,
\[ \begin{split}
  H_{0}&: \text{``the distrubution of households into the tree types is the same for urban and rural households''} \\
  H_{A}&: \text{``the distrubution of households into the tree types is NOT the same for urban and rural households''}
\end{split} \]


\subsection{Tables}
As noted above, all two-way frequency tables used for a Chi-Square Test will be organized such that the response variable forms the columns and the populations form the rows. With this organization, the row-percentage table becomes the table of primary interest because it relates directly to the hypotheses described above. The question of a Chi-Square Test then becomes one of determining whether each row of the row-percentage table is equal, given sampling variability.

The observed raw data must be organized into a two-way frequency table as described in \modref{chap:BivEDACat}. For example, the Christmas tree data is summarized as in \tabref{tab:ChiTreeObs}. The actual calculations for a Chi-Square Test are performed on this observed table. However, the hypothesis test, as described above, is best viewed as a method to determine if each row of the row-percentage table is statistically equivalent or not. Thus, the row-percentage table computed from the frequency table is useful when interpreting the results of a Chi-Square Test \tabrefp{tab:ChiTreeObsProp}.

\begin{table}[htbp]
  \centering
  \caption{Frequency of individuals in urban and rural households that have a natural or an artificial Christmas tree.}\label{tab:ChiTreeObs}
    \begin{tabular}{c|c|c|c|}
      \multicolumn{1}{c}{} & \multicolumn{2}{c}{Tree Type} & \multicolumn{1}{c}{} \\
      \cline{2-3}
      Household & Natural & Artificial & \multicolumn{1}{c}{} \\
      \hline
      \multicolumn{1}{|c|}{Urban} & 89 & 172 & \textbf{261} \\
      \hline
      \multicolumn{1}{|c|}{Rural} & 64 & 96 & \textbf{160} \\
      \hline
       & \textbf{153} & \textbf{268} & \textbf{421} \\
      \cline{2-4}
    \end{tabular}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Percentage of individuals within urban and rural households that have a natural or an artificial Christmas tree.}\label{tab:ChiTreeObsProp}
    \begin{tabular}{c|c|c|c|}
      \multicolumn{1}{c}{} & \multicolumn{2}{c}{Tree Type} & \multicolumn{1}{c}{} \\
      \cline{2-3}
      Household & Natural & Artificial & \multicolumn{1}{c}{} \\
      \hline
      \multicolumn{1}{|c|}{Urban} & 34.1 & 65.9 & \textbf{100.0} \\
      \hline
      \multicolumn{1}{|c|}{Rural} & 40.0 & 60.0 & \textbf{100.0} \\
      \hline
       & \textbf{36.3} & \textbf{63.7} & \textbf{100.0} \\
      \cline{2-4}
    \end{tabular}
\end{table}

The Chi-Square Test requires constructing a table of expected values that are derived from the null hypothesis. Specifically, the ``expected'' table contains the expected frequency of individuals in each level of the response variable for each population assuming that the distribution of responses does not differ among populations. These expected table are computed from the margins of the observed table, but are best explained with an illustrative example.

In the Christmas tree example, the null hypothesis states that there is no difference in the distribution of households with a natural tree between the rural and urban areas. Thus, under this null hypothesis, one would expect the proportion (or percentage) of households with a natural tree to be the same in both groups. The proportion of households with a natural tree, regardless of location, is $\frac{153}{421}$=$0.363$. Thus, under the null hypothesis, the proportion of rural AND the proportion of urban households with a natural tree is $0.363$. Because there is a different number of urban and rural households in the study, the actual NUMBER (rather than proportion) of households expected to have a natural tree will differ. The NUMBER of urban households expected to HAVE a natural tree is found by multiplying the number of urban households by the common proportion computed above -- i.e., $261*0.363$=$94.743$. The remaining urban households would be expected to NOT have a natural tree -- i.e., $261-94.743$=$261(1-0.363)$=$166.257$. Similar calculations are made for the rural households (i.e., $160*0.363=58.080$ expected to have a natural tree and $160*(1-0.363)=101.920$ expected to NOT have a natural tree.

These expected frequencies are computed directly and easily from the marginal totals of the observed frequency table \tabrefp{tab:ChiTreeObs}. For example, substituting the fractional representation of the decimal proportions into the calculation of the expected number of urban households with a natural tree gives $261*\frac{153}{421}$=$\frac{261*153}{421}$=$94.853$\footnote{Note a slight difference here because 0.363 was rounded to three decimals, whereas the fraction is not rounded.}. A close examination of this formula and the marginal totals in \tabref{tab:ChiTreeObs} shows that this value is equal to the product of the corresponding row and column marginal totals in the observed table divided by the total number of individuals. The other expected values follow a similar pattern as follows,
\begin{Itemize}
  \item $261*\frac{268}{421}=\frac{261*268}{421}=166.147$ urban households to NOT have a natural tree.
  \item $160*\frac{153}{421}=\frac{160*153}{421}=58.147$ rural households to have a natural tree.
  \item $160*\frac{268}{421}=\frac{160*268}{421}=101.853$ rural households to NOT have a natural tree.
\end{Itemize}

Thust, all expected values in a Chi-Square Test are calculated by multiplying the row and column totals of the frequency table and dividing by the total number of individuals. These expected values are summarized in a two-way table, called the expected frequencies table \tabrefp{tab:ChiTreeExp}.

\begin{table}[htbp]
  \centering
  \caption{The expected frequency of individuals in urban and rural households that have a natural or an artificial Christmas tree.}\label{tab:ChiTreeExp}
    \begin{tabular}{c|c|c|c|}
      \multicolumn{1}{c}{} & \multicolumn{2}{c}{Tree Type} & \multicolumn{1}{c}{} \\
      \cline{2-3}
      Household & Natural & Artificial & \multicolumn{1}{c}{} \\
      \hline
      \multicolumn{1}{|c|}{Urban} & 94.853 & 166.147 & \textbf{261} \\
      \hline
      \multicolumn{1}{|c|}{Rural} & 58.147 & 101.853 & \textbf{160} \\
      \hline
       & \textbf{153} & \textbf{268} & \textbf{421} \\
      \cline{2-4}
    \end{tabular}
\end{table}

\subsection{Specifics}
\vspace{-3pt}
The Chi-Square Test is characterized by a categorical response variable recorded for two or more populations. The specifics of the Chi-Square Test are in \tabref{tab:Chispec}.

\begin{table}[h]
\centering
\colorbox{ltgray}{
\begin{minipage}{.8\textwidth}
  \centering
	\caption{Characteristics of a Chi-Square Test.}\label{tab:Chispec}
\vspace{-3pt}
  \begin{Itemize}
      \item \textbf{Null Hypothesis:} ``The distribution of individuals into the levels of the response variable is the same for all populations''
      \item \textbf{Alternative Hypothesis:} ``The distribution of individuals into the levels of the response variable is NOT the same for all populations.''
      \item \textbf{Statistic:} Observed frequency table.
      \item \textbf{Test Statistic:} $\chi^{2} = \Sum_{cells}\frac{(Observed-Expected)^{2}}{Expected}$
      \item \textbf{df:} $(r-1)(c-1)$ where $r=$ number of rows and $c=$ number of columns
      \item \textbf{Assumptions:} Expected value for each category is $\geq5$.
      \item \textbf{When to Use:} Categorical response, two or more populations.
  \end{Itemize}
\end{minipage}}
\end{table}

In general, a confidence region is not constructed for a Chi-Square Test because of the complexity of the parameter (i.e., same size as the observed table). Thus, in this course, Step 11 for a hypothesis test will not be computed for a Chi-Square Test.

\vspace{-3pt}
\subsection{Example -- Christmas Trees}
\vspace{-3pt}
Below are the 11-steps \sectrefp{sec:11Steps} for a full hypothesis test for the Christmas tree example.
\vspace{-6pt}
\begin{Enumerate}
  \item $\alpha$=0.10.
  \item $H_{0}$: ``distribution of households by type of tree is the same for urban and rural households'' vs. $H_{A}$: ``distribution of households by type of tree is NOT the same for urban and rural households.''
  \item A Chi-Square Test is required because (i) a categorical response variable was recorded (type of tree) and (ii) two populations were sampled (urban and rural households).
  \item The data appear to be part of an observational study with no clear indication of randomization.
  \item The expected frequency in each of the four cells is greater than five \tabrefp{tab:ChiTreeExp}.
  \item The observed frequency table is in \tabref{tab:ChiTreeObs}.
  \item $\chi^{2}$ = $\frac{(89-94.853)^{2}}{94.853}$ + $\frac{(172-166.147)^{2}}{166.147}$ + $\frac{(64-58.147)^{2}}{58.147}$ + $\frac{(96-101.853)^{2}}{101.853}$ = $0.3611+0.2062+0.5891+0.3363$ = $1.4927$ with 1 df.
  \item p-value=$0.2218$.
  \item $H_{0}$ is not rejected because the p-value is $>\alpha$.
  \item There does not appear to be a significant difference in the distribution of Christmas tree types among rural and urban households.
  \item Not performed for Chi-Square Test.
\end{Enumerate}

\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\vspace{-6pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
( distrib(1.4927,distrib="chisq",df=1,lower.tail=FALSE) )
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}


\vspace{6pt}
\section{Chi-Square test in R}
\vspace{-3pt}
\subsection{Data Format}
The data for a Chi-Square Test may be entered from summarized data or computed from raw data on individuals. Raw data must be in stacked format where one column in the data.frame represents the response variable and another column represents the populations (see Sections \ref{sect:REnterData} and \ref{sect:2tData}). Raw data must be summarized into a two-way frequency table with \R{xtabs()} as described in \modref{chap:BivEDACat}. The two-way table must contain frequencies, not proportions or percentages (don't use \R{percTable()}), without marginal totals (don't use \R{addMargins()}).

Summarized data must be entered into a two-dimensional matrix. The frequencies must first be entered into a vector with the first row of values followed by the second row and so on. This vector is then the first argument to \R{matrix()}, which will also include the number of rows in the frequency table in \R{nrow=} and \R{byrow=TRUE} (which causes the values in the vector to be entered into the matrix in a row-wise manner). The process of entering summarized data into a matrix is better explained by example.

Suppose that you are given this observed frequency table.

\begin{center}
  \begin{tabular}{c|c|c|c|c|c|c|c|}
    \multicolumn{1}{c}{} & \multicolumn{6}{c}{Species} & \multicolumn{1}{c}{} \\
    \cline{2-7}
    Location & A & B & C & D & E & F & \multicolumn{1}{c}{} \\
    \hline
    \multicolumn{1}{|c|}{DI} & 34 & 22 & 14 & 13 & 12 & 5 & \textbf{100} \\
    \hline
    \multicolumn{1}{|c|}{BP} & 62 & 12 & 8 & 7 & 6 & 5 & \textbf{100} \\
    \hline
    & \textbf{96} & \textbf{34} & \textbf{22} & \textbf{20} & \textbf{18} & \textbf{10} & \textbf{200} \\
    \cline{2-8}
  \end{tabular}
\end{center}

The observed frequencies, ignoring the marginal sums, are first entered into a vector called \R{freq} below, which is then transformed into a two-row matrix called \R{obstbl}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( freq <- c(34,22,14,13,12,5,62,12,8,7,6,5) )
 [1] 34 22 14 13 12  5 62 12  8  7  6  5
> ( obstbl <- matrix(freq,nrow=2,byrow=TRUE) )
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]   34   22   14   13   12    5
[2,]   62   12    8    7    6    5
\end{verbatim}
\end{kframe}
\end{knitrout}

The matrix is more informative if the rows and columns are named with \R{rownames()} and \R{colnames()} as shown below.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> rownames(obstbl) <- c("DI","BP")
> colnames(obstbl) <- c("A","B","C","D","E","F")
> obstbl
    A  B  C  D  E F
DI 34 22 14 13 12 5
BP 62 12  8  7  6 5
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{Chi-Square Test}
The Chi-Square Test is performed with \R{chisq.test()}, which takes an observed frequency table either entered through \R{matrix()} or summarized with \R{xtabs()} as the first argument. The only other argument needed is \R{correct=FALSE} so that the continuity correction is not used.\footnote{The continuity correction is not used here simply so that the results using R will match hand-calculations. The continuity correction should usually be used.} The results of \R{chisq.test()} should be assigned to an object. The Chi-Square test statistic and p-value are extracted by simply printing the saved object. The expected frequency table is returned by appending \R{\$expected} to the saved object.

\subsection{Post-Hoc Analysis}
Rejecting the null hypothesis in a Chi-Square Test indicates that there is some difference in the distribution of individuals into the levels of the response variable among some of the populations. However, rejecting the null hypothesis does not indicate which populations are different. In addition, as mentioned previously, confidence intervals are generally not performed with a Chi-Square Test. A post-hoc method for helping determine which populations differ is obtained by observing the Pearson residuals.

A Pearson residual is computed for each cell in the table as,

\[ \frac{Observed-Expected}{\sqrt{Expected}} \]

which is the appropriately signed square root of the parts in the $\chi^2$ test statistic calculation. Therefore, cells that have Pearson residuals far from zero contributed substantially to the large $\chi^2$ test statistic that resulted in a small p-value and the ultimate rejection of $H_{0}$. Patterns in where the large Pearson residuals are found may allow one to qualitatively determine which populations differ and, thus, which levels of the response differ the most. This process will be illustrated more fully in the examples and review exercises. The Pearson residuals are obtained from the saved \R{chisq.test()} object by appending \R{\$residuals}.

\subsection{Example - Father Present at Birth}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:
\vspace{-8pt}
\begin{quote}
\textsl{Daniel Weiss (in ``100\% American'') reported the results of a survey of 300 first-time fathers from four different hospitals (labeled as A, B, C, and D). Each father was asked if he was present (or not) in the delivery room when his child was born. The results of the survey are in \href{https://raw.githubusercontent.com/droglenc/NCData/master/FatherPresent.csv}{FatherPresent.csv}. Use these data to determine if there is a difference, at the 5\% level, in the proportion of fathers present in the delivery room among the four hospitals.}
\end{quote}




\vspace{-6pt}
\begin{Enumerate}
  \item $\alpha$=0.05.
  \item $H_{0}$:``distribution of fathers presence (or not) during the birth of their child is the same for all four hospitals'' vs. $H_{A}:$ ``the distribution of fathers presence during the birth of their child is NOT the same for all four hospitals.''
  \item A Chi-Square Test is required because (i) a categorical variable (present or absent) was recorded and (ii) four populations were sampled (the hospitals).
  \item The data appear to be part of an observational study with no clear indication of randomization (likely a voluntary response survey).
  \item There are at least five individuals in each cell of the expected table \tabrefp{tab:ChiFPexp}.
  \item The statistic is the observed frequency table \tabrefp{tab:ChiFPobs}.
  \item $\chi^{2}$=5.000 with 3 df \tabrefp{tab:ChiFPchi}.
  \item p-value=$0.1718$ \tabrefp{tab:ChiFPchi}.
  \item $H_{0}$ is not rejected because the p-value is $>\alpha$.
  \item The distribution of father's presence (or not) at their child's birth does not seem to differ significantly among hospitals where that birth occurred. For comparative purposes, the row-percentage table is in \tabref{tab:ChiFProw}.
\end{Enumerate}

\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\vspace{-4pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
setwd("c:/data/")
fp <- read.csv("FatherPresent.csv")
( fp.obs <- xtabs(~hospital+father,data=fp) )
( fp.chi <- chisq.test(fp.obs) )
fp.chi$expected
percTable(fp.obs,margin=1,digits=1)
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}

\begin{table}[h]
  \caption{Expected frequency table for father's presence (or absence) during child birth among four hospitals.}
  \label{tab:ChiFPexp}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
        father
hospital Absent Present
       A  15.25   59.75
       B  15.25   59.75
       C  15.25   59.75
       D  15.25   59.75
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Observed frequency table for father's presence (or absence) during child birth among four hospitals.}
  \label{tab:ChiFPobs}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
        father
hospital Absent Present
       A      9      66
       B     15      60
       C     18      57
       D     19      56
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Results from the Chi-Square Test for differences in father's presence during child birth among four hospitals.}
  \label{tab:ChiFPchi}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
X-squared = 5.0003, df = 3, p-value = 0.1718
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Percentage of father's presence (or absence) during child birth among four hospitals.}
  \label{tab:ChiFProw}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
        father
hospital Absent Present   Sum
       A   12.0    88.0 100.0
       B   20.0    80.0 100.0
       C   24.0    76.0 100.0
       D   25.3    74.7 100.0
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}





\subsection{Example - Apostle Islands Plants}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:
\vspace{-4pt}
\begin{quote}
\textsl{In her Senior Capstone project a Northland College student recorded the dominant (i.e., most abundant) plant species in 100 randomly selected plots on both Devil's Island and the Bayfield Peninsula (i.e., the mainland). There were a total of six ``species'' (one group was called ``other'') recorded (labeled as A, B, C, D, E, and F). The results are shown in the table below. Determine, at the 5\% level, if the frequency of dominant species differs between the two locations.}
\end{quote}

\begin{center}
  \begin{tabular}{c|c|c|c|c|c|c|c|}
    \multicolumn{1}{c}{} & \multicolumn{6}{c}{Species} & \multicolumn{1}{c}{} \\
    \cline{2-7}
    Location & A & B & C & D & E & F & \multicolumn{1}{c}{} \\
    \hline
    \multicolumn{1}{|c|}{DI} & 34 & 22 & 14 & 13 & 12 & 5 & \textbf{100} \\
    \hline
    \multicolumn{1}{|c|}{BP} & 62 & 12 & 8 & 7 & 6 & 5 & \textbf{100} \\
    \hline
    & \textbf{96} & \textbf{34} & \textbf{22} & \textbf{20} & \textbf{18} & \textbf{10} & \textbf{200} \\
    \cline{2-8}
  \end{tabular}
\end{center}

  \begin{Enumerate}
    \item $\alpha$=0.05.
    \item $H_{0}$: ``the distribution of dominant plants species is the same between Devil's Island and the Bayfield Peninsula'' vs. $H_{A}$: ``the distribution of dominant plants species is NOT the same between Devil's Island and the Bayfield Peninsula.''
    \item A Chi-Square Test is required because (i) a categorical variable with six levels (plant species) was recorded and (ii) two populations were sampled (Devil's Island and Bayfield Peninsula).
    \item The data appear to be part of an observational study where the plots were randomly selected.
    \item There are more than five individuals in each cell of the expected table \tabrefp{tab:ChiAIexp}.
    \item The statistic is the observed frequency table given in the background.
    \item $\chi^{2}$=16.54 with 5 df \tabrefp{tab:ChiAIchi}.
    \item p-value=$0.0055$ \tabrefp{tab:ChiAIchi}.
    \item $H_{0}$ is rejected because the p-value is $<\alpha$.
    \item There does appear to be a significant difference in the distribution of the dominant plants between the two sites. A look at the Pearson residuals \tabrefp{tab:ChiAIres} and the row-percentage table \tabrefp{tab:ChiAIrow} both suggest that the biggest difference between the two locations is due to ``plant A.''\footnote{When ``Plant A'' is removed from the observed table, the Chi-Square Test performed on the remaining plant species showed no difference in the distribution of the remaining plants between the two locations ($p=0.9239$). Thus, most of the difference in plant distributions between Devil's Island and the Bayfield Peninsula appears to be due primarily to ``plant A'' with more of ``plant A'' found on the Bayfield Peninsula than on Devil's Island.}
  \end{Enumerate}

\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
freq <- c(34,22,14,13,12,5,62,12,8,7,6,5)
ai.obs <- matrix(freq,nrow=2,byrow=TRUE)
rownames(ai.obs) <- c("DI","BP")
colnames(ai.obs) <- c("A","B","C","D","E","F")
( ai.chi <- chisq.test(ai.obs) )
ai.chi$expected
ai.chi$residuals
percTable(ai.obs,margin=1,digits=1)
ai.obs1 <- ai.obs[,-1]
( ai.chi1 <- chisq.test(ai.obs1) )
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}

\begin{table}[h]
  \caption{Expected frequency table for dominant plant species on Devil's Island and the Bayfield Peninsula.}
  \label{tab:ChiAIexp}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
    A  B  C  D E F
DI 48 17 11 10 9 5
BP 48 17 11 10 9 5
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Results from the Chi-Square Test for differences in the distribution of dominant plant species between Devil's Island and the Bayfield Peninsula.}
  \label{tab:ChiAIchi}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
X-squared = 16.5442, df = 5, p-value = 0.00545
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Pearson residuals from the Chi-Square Test for differences in the distribution of dominant plant species between Devil's Island and Bayfield Peninsula.}
  \label{tab:ChiAIres}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
           A         B         C          D  E F
DI -2.020726  1.212678  0.904534  0.9486833  1 0
BP  2.020726 -1.212678 -0.904534 -0.9486833 -1 0
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Percentage of dominant plant species within each location (Devil's Island and Bayfield Peninsula).}
  \label{tab:ChiAIrow}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
    A  B  C  D  E F Sum
DI 34 22 14 13 12 5 100
BP 62 12  8  7  6 5 100
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}



\chapter[Goodness-of-Fit]{Goodness-of-Fit Test} \label{chap:GOF}

\vspace{36pt}
\minitoc
\vspace{36pt}

\lettrine{I}{t is common to determine if the frequency} of individuals in the levels of a categorical response variable follow frequencies suggested by a particular theory or distribution. The simplest of these situations occurs when a researcher is making a hypothesis about the percentage or proportion of individuals in one of two categories. The ``distribution'' of individuals in two categories comes from the proportion in the hypothesis for one group and one minus the proportion in the hypothesis for the other group. In situations with more than two levels, the ``distribution'' of individuals into the categories likely comes from the hypothesis that a particular theoretical distribution holds true. For example, a researcher may want to determine if frequencies predicted from a certain genetic theory are upheld by the observed frequencies found in a breeding experiment, if the frequency that a certain animal uses habitats in proportion to the availability of those habitats, or if the frequency of consumers that show a preference for a certain product (over other comparable products) is non-random.

In each of these cases, the theoretical distribution articulated in the research hypothesis must be converted to statistical hypotheses that will then be used to generate expected frequencies for each level. These expected frequencies will then be statistically compared to the observed frequencies to determine if the theoretical distribution represented in the null hypothesis is supported by the data. The method used for comparing the observed to expected frequencies, where the expected frequencies come from a hypothesized theoretical distribution, is a Goodness-of-Fit Test, the subject of this module.

\section{Goodness-of-Fit Test Specifics}
\subsection{The Hypotheses}
A Goodness-of-Fit Test is used when a single categorical variable has been recorded and the frequency of individuals in the levels of this variable are to be compared to a theoretical distribution. In its most general form the statistical hypotheses for the Goodness-of-Fit Test will be ``wordy,'' relating whether the ``distribution'' of individuals into the levels of the response variable follows a specific theoretical distribution or not. The null hypothesis will generally be like $H_{0}:$ ``the distribution of individuals into the levels follows the `theoretical distribution' '', where `theoretical distribution' will likely be replaced with more specific language. For example, the research hypothesis that states that ``50\% of students at Northland are from Wisconsin, 25\% are from neighboring states, and 25\% are from other states'' would be converted to $H_{0}$: ``the proportion of students from Wisconsin, neighboring states, and other states is 0.50, 0.25, and 0.25, respectively'' with an $H_{A}$: ``the proportion of students from Wisconsin, neighboring states, and other states is NOT 0.50, 0.25, and 0.25, respectively.''

\warn{The statistical hypotheses for a Goodness-of-Fit Test are ``wordy'' and relate the observed distribution of individuals into levels of the categorical variable to those expected from a theoretical distribution.}

The hypotheses are simpler, but you must be more careful, when there are only two levels of the response variable. For example, a research hypothesis of ``less than 40\% of new-born bear cubs are female'' would be converted to $H_{0}$: ``the proportion of bear cubs that are female and male is 0.40 and 0.60, respectively'' with an $H_{A}$: ``the proportion of bear cubs that are female and male is NOT 0.40 and 0.60, respectively.''  However, these hypotheses are often simplified to focus on only one level as the other level is implied by subtraction from one. Thus, these hypotheses are more likely to be written as $H_{0}$: ``the proportion of bear cubs that are female is 0.40'' with an $H_{A}$: ``the proportion of bear cubs that are female is NOT 0.40.''

\warn{The statistical hypotheses for a Goodness-of-Fit Test with only two levels of the categorical variable often relate only to the proportion or percentage of individuals in one level.}

One may also have expected, from the wording of the research hypothesis about the sex of bear cubs, that the alternative hypothesis would have been $H_{A}$: ``the proportion of bear cubs that are female is LESS THAN 0.40.''  Recall from \sectref{sect:ChiDist}, however, that the chi-square test statistic always represents a two-tailed situation. Thus, the $H_{A}$ here reflects that constraint. The researcher will ultimately be able to determine if the proportion is less than 0.40 if the p-value from the Goodness-of-Fit Test indicates a difference and the observed proportion of female bear cubs is less than 0.40.

\subsection{The Tables}
For a Goodness-of-Fit Test, the data are summarized in an observed frequency table as in \modref{chap:UnivEDACat}. Additionally, a table of expected frequencies must be constructed from the theoretical distribution in the null hypothesis and the total number of observed individuals ($n$). Specifically, the expected frequencies are found by multiplying the expected proportions from the theoretical distribution in the null hypothesis by $n$. For example, consider this situation,

\begin{quote}
\cite{BathBuchanan1989} surveyed residents of Wyoming by distributing a mailing to random residents and collecting voluntarily returned surveys. One question asked of the respondents was, ``Do you strongly agree, agree, neither agree or disagree, disagree, or strongly disagree with this statement? -- `Wolves would have a significant impact on big game hunting opportunities near Yellowstone National Park'.''  The researchers hypothesized that more than 50\% of Wyoming residents would either disagree or strongly disagree with the statement. Of the 371 residents that returned the survey, 153 disagreed and 43 strongly disagreed with the statement.
\end{quote}

At first glance it may seem that this variable has five levels -- i.e., the levels of agreement offered in the actual survey. However, the researcher's hypothesis collapsed the results of the survey question into two levels: (1) strongly disagree or disagree combined and (2) all other responses. Thus, the statistical hypotheses for this situation are $H_{0}$: ``the proportion of respondents that disagreed or strongly disagreed is 0.50'' and $H_{A}$: ``the proportion of respondents that disagreed or strongly disagreed is NOT 0.50.''

The expected frequencies in each level are derived from the total number of individuals examined and the specific null hypothesis. For example, if the null hypothesis is true, then 50\% of the 371 respondents would be expected to disagree or strongly disagree with the statement. In other words, $371*0.50$=$185.5$ individuals would be expected to disagree or strongly disagree. Furthermore, the other 50\%, or $371*(1-0.50)$=$185.5$ would be expected to ``not'' disagree or strongly disagree. The expectations for the two levels of this variable are summarized in \tabref{tab:WYOtable}.

\begin{table}[htbp]
  \centering
  \caption{Expected and observed frequency of respondents that disagreed or strongly disagreed (i.e., labeled as ``Disagree'') with the given statement in the Wyoming survey example.}\label{tab:WYOtable}
  \begin{tabular}{ccc}
    \hline\hline
    \multicolumn{1}{c}{\widen{0}{5}{}} & \multicolumn{2}{c}{Frequency} \\
    \widen{-2}{0}{Category} & Expected & Observed\\
    \hline
    \widen{-1}{6}{``Disagree''} & 185.5 & 196 \\
    \widen{-1}{6}{not ``Disagree''} & 185.5 & 175 \\
    \hline
    \widen{-2}{7}{Total} & 371 & 371 \\
    \hline\hline
  \end{tabular}
\end{table}

\warn{The expected table should maintain at least one decimal in each cell even though the values represent frequencies.}

Consider the following situation where construction of expected frequencies is bit more complex.

\begin{quote}
\textsl{Mendel's law of independent assortment predicts that the genotypes (i.e., how they look) of the offspring from mating the offspring of a dihybrid cross of homozygous dominant and homozygous recessive parents should follow a 9:3:3:1 ratio. In an experiment to test this, Mendel crossed a pea plant that produces round, yellow seeds (i.e., all dominant alleles, YYWW) with a pea plant that produces green, wrinkled seeds (i.e., all recessive alleles, yyww) such that only round, yellow heterozygous offspring (i.e., YyWw) were produced. Pairs of these offspring were then bred. Mendel's theory says that $\frac{9}{16}$ of these offspring should be round, yellow; $\frac{3}{16}$ should be round, green; $\frac{3}{16}$ should be wrinkled, yellow; and $\frac{1}{16}$ should be wrinkled, green. Of 566 seeds studied in this experiment, Mendel found that 315 were round, yellow; 108 were round, green; 101 were wrinkled, yellow; and 32 were wrinkled, green. Use these results to determine, at the 5\% level, if Mendel's law of independent assortment is supported by these results.}
\end{quote}

The statistical hypotheses are as follows,
\[ \begin{split}
  H_{0}&: \text{``the proportion of RY, RG, WY, and WG individuals will be $\frac{9}{16}$, $\frac{3}{16}$, $\frac{3}{16}$, and $\frac{1}{16}$, respectively''} \\
  H_{A}&: \text{``the proportion of RY, RG, WY, and WG individuals will NOT be $\frac{9}{16}$, $\frac{3}{16}$, $\frac{3}{16}$, and $\frac{1}{16}$, respectively''}
\end{split} \]
where RY=``round, yellow'', RG=``round, green'', WY=``wrinkled, yellow'', and WG=``wrinkled, green''. If these proportions are applied to the $n=566$ observed offspring, then the following frequencies for each genotype would be expected:
\begin{Itemize}
  \item $\frac{9}{16}\cdot566 = 318.375$ would be expected to be round, yellow.
  \item $\frac{3}{16}\cdot566 = 106.125$ would be expected to be round, green.
  \item $\frac{3}{16}\cdot566 = 106.125$ would be expected to be wrinkled, yellow.
  \item $\frac{1}{16}\cdot566 = 35.375$ would be expected to be wrinkled, green.
\end{Itemize}
These expected frequencies, along with the observed frequencies, are summarized in \tabref{tab:PEAtable}.

\begin{table}[htbp]
  \centering
  \caption{Expected and observed frequency of 566 pea seeds in four types.}\label{tab:PEAtable}
  \begin{tabular}{ccc}
    \hline\hline
    \multicolumn{1}{c}{\widen{0}{5}{}} & \multicolumn{2}{c}{Frequency} \\
    \widen{-1}{0}{Category} & Expected & Observed\\
    \hline
    \widen{-1}{5}{round, yellow} & 318.375 & 314 \\
    \widen{-1}{5}{round, green} & 106.125 & 108 \\
    \widen{-1}{5}{wrinkled, yellow} & 106.125 & 101 \\
    \widen{-1}{5}{wrinkled, green} & 35.375 & 32 \\
    \hline
    \widen{-2}{6}{Total} & 566 & 566 \\
    \hline\hline
  \end{tabular}
\end{table}

The hypothesis test method developed in the following sections will be used to determine if the differences between the expected and observed frequencies is ``large'' enough to suggest that the observed frequencies do not support the distribution represented in the null hypothesis.

\subsection{Specifics}
The Goodness-of-Fit Test is characterized by a single categorical response variable. The hypotheses tested usually cannot be converted to mathematical symbols and are thus ``wordy.''  Specifics of the Goodness-of-Fit Test are in \tabref{tab:ChiGOFspec}.

\begin{table}[h]
\centering
\colorbox{ltgray}{
\begin{minipage}{.8\textwidth}
  \centering
	\caption{Characteristics of a Goodness-of-Fit Test.}\label{tab:ChiGOFspec}
  \begin{Itemize}
      \item \textbf{Hypotheses:} $H_{0}:$``the observed distribution of individuals into the levels follows the `theoretical distribution' ''\\ $H_{A}:$``the observed distribution of individuals into the levels DOES NOT follow the `theoretical distribution' ''
      \item \textbf{Statistic:} Observed frequency table.
      \vspace{6pt}
      \item \textbf{Test Statistic:} $\chi^{2} = \Sum_{cells}\frac{(Observed-Expected)^{2}}{Expected}$
      \vspace{6pt}
      \item \textbf{df:} Number of levels minus 1.
      \item \textbf{Assumptions:} Expected value in each level is $\geq5$.
      \vspace{6pt}
      \item \textbf{Confidence Interval (for one level):} $\hat{p} \pm Z^{*}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$
      \vspace{6pt}
      \item \textbf{When to Use:} Categorical response, one population, comparing to a theoretical distribution.
  \end{Itemize}
\end{minipage}}
\end{table}

It is cumbersome to produce a confidence interval in a Goodness-of-Fit Test because there generally is not a single parameter (i.e., there are as many parameters as levels in the response variable). Confidence intervals can be calculated for the proportion in each level as shown below. However, confidence intervals will only be ``hand''-calculated when there are two levels. When using R (as discussed in a subsequent section), confidence intervals will be computed for all levels, no matter the number of levels.

Let $p$ be the population proportion in a particular level and $\hat{p}$ be the sample proportion in the same interval. The $\hat{p}$ is computed by dividing the frequency of individuals in this level by the total number of individuals in the sample (i.e., $n$). The $\hat{p}$ is a statistic that is subject to sampling variable with that sampling variability measured by $SE_{\hat{p}}=\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$ for ``large'' values of $n$. For ``large'' values of $n$ the $\hat{p}$ will follow a normal distribution such that a confidence interval for $p$ is computed using the general confidence interval formula found in \sectref{sec:CIConstruct} and repeated below:

  \[ \text{``Statistic''} + \text{``scaling factor''} * SE_{statistic} \]

where the scaling factor is the familiar $Z^{*}$. Thus, the confidence interval for $p$ is constructed with

  \[ \hat{p} \pm Z^{*}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \]

Note that one does not need to worry about lower and upper bounds, only confidence intervals will be computed, because of the two-tailed nature of the chi-square test statistic.

In the Wyoming survey example, the proportion of respondents in the sample that either disagreed or strongly disagreed was $\hat{p}$=$\frac{196}{371}$=$0.528$. The standard error for this sample proportion is $\sqrt{\frac{0.528(1-0.528)}{371}}$=$0.026$. For a 95\% confidence interval, $Z^{*}$=$\pm$1.960.\footnote{This $Z^{*}$ is computed with \R{distrib(0.975,type="q")}} Thus, the confidence interval is $0.528 \pm 1.960*0.026$ or $0.528 \pm 0.051$ or $(0.477,0.579)$. Therefore, one is 95\% confident that the population proportion that either disagreed or strongly disagreed is between 0.477 and 0.579. Because there are only two levels in this example it can also be said with 95\% confidence that the population proportion that did not either disagree or strongly disagree is between 0.421 and 0.523.

\subsection{Example - \$1 Coins}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:
\vspace{-4pt}
\begin{quote}
\textsl{USA Today (June 14, 1995) reported that 77\% of the population opposes replacing \$1 bills with \$1 coins. To test if this claim holds true for the residents of Ashland a student selected a sample of 80 Ashland residents and found that 54 were opposed to replacing the bills with coins. Develop a hypothesis test (at the 10\% level) to determine if the proportion of Ashland residents that are opposed to replacing bills with coins is different from the proportion opposed for the general population.}
\end{quote}

\begin{Enumerate}
    \item $\alpha$=0.10.
    \item $H_{0}$: ``the proportion of Ashland residents that oppose replacing the \$1 bill with the \$1 coin is 0.77'' vs. $H_{A}$: ``The proportion of Ashland residents that oppose replacing the \$1 bill with the \$1 coin is NOT 0.77.''
    \item A Goodness-of-Fit Test is required because (a) a single categorical variable was recorded (opinion about \$1 coing), (ii) a single population was sampled (Ashland residents), and (iii) the frequency of responses is being compared to a hypothesized distribution in the null hypothesis.
    \item The data appear to be part of an observational study with no clear indication of random selection of individuals.
    \item The expected number in the ``oppose'' level is $80*0.77$=$61.6$. The expected number in the ``do not oppose category is $80*0.23$=$18.4$. These expectations are shown in the table in the next step. The assumption of more than five individual in all cells of the expected table has been met.
    \item The observed table is shown below (along with the expected table).
    \vspace{12pt}
    \begin{center}
      \begin{tabular}{ccc}
        \hline\hline
        \multicolumn{1}{c}{\widen{0}{5}{}} & \multicolumn{2}{c}{Frequency} \\
        \widen{-2}{0}{Level} & Expected & Observed\\
        \hline
        \widen{-1}{6}{``Oppose''} & 61.6 & 54 \\
        \widen{-1}{6}{``Do Not Oppose''} & 18.4 & 26 \\
        \hline
        \widen{-2}{7}{Total} & 80 & 80 \\
        \hline\hline
      \end{tabular}
    \end{center}
    \vspace{12pt}
    \item $\chi^{2}$=$\frac{(61.6-54)^{2}}{55} + \frac{(18.4-26)^{2}}{25}$=$0.938 + 3.139$ = $4.077$ with $2-1=1$ df.
    \item p-value=$0.0435$.
    \item $H_{0}$ is rejected because the $p-value <\alpha=0.10$).
    \item The proportion of Ashland residents that oppose replacing the \$1 bill with the \$1 coin does appear to be different from the proportion (0.77) reported for the general population.
    \item I am 90\% confident that the proportion of all Ashland residents opposed to the \$1 coin is between 0.596 and 0.767. [$\frac{54}{80}$$\pm1.645*\sqrt{\frac{0.68125*0.31875}{80}}$=$0.68125\pm1.645*0.0521$=$0.68125\pm0.0857$=$(0.5956,0.7670)$.]
\end{Enumerate}

\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
( distrib(4.077,distrib="chisq",df=1,lower.tail=FALSE) )
( distrib(0.95,type="q") )
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}


\section{Goodness-of-Fit Test in R}
\subsection{Data Format}
A Goodness-of-Fit Test is conducted in R with \R{chisq.test()}, which requires an observed table as the first argument. This observed table is entered from summarized data using \R{c()} or raw data is summarized to a frequency table with \R{xtabs()} as in \modref{chap:UnivEDACat}.

For example, suppose that the frequencies of shrike observations in the ``mid-successional'', ``open'', ``scattered trees'', ``woods'', and ``wetland'' habitats shown previously are known to be 43, 1456, 112, 44 and 6, respectively. These summarized values are entered directly into a named vector below
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( obs <- c(MidSucc=43,Open=1456,ScatTree=112,Woods=6,Wetland=44) )
 MidSucc     Open ScatTree    Woods  Wetland 
      43     1456      112        6       44 
\end{verbatim}
\end{kframe}
\end{knitrout}


However, instead of having summarized frequencies, suppose that the recorded habitats in a variable called \R{hab.use} in the \R{df} data.frame. These raw data must be summarized into a frequency table.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ( obs <- xtabs(~hab.use,data=shrike.raw) )
hab.use
 MidSucc     Open ScatTree  Wetland    Woods 
      43     1456      112        6       44 
\end{verbatim}
\end{kframe}
\end{knitrout}

\warn{If raw un-summarized data are in a data.frame, then the variable in that data.frame must be summarized with \R{xtabs()} and assigned to an object before performing the Goodness-of-Fit Test.}


\subsection{Goodness-of-Fit Test}
The Goodness-of-Fit Test is computed with \R{chisq.test()} with a observed frequencies as the first argument and the following arguments:
\begin{Itemize}
  \item \R{p=}: a vector of expected proportions for the levels of the theoretical distribution.
  \item \R{rescale.p=TRUE}: rescales the values in \R{p=} to sum to 1. Rescaling is useful if the proportions in \R{p=} were rounded or are expected frequencies.
  \item \R{correct=FALSE}: indicates to not use a ``continuity correction''.\footnote{Some statisticians argue that small chi-square tables with small sample sizes should be corrected for the fact that the chi-square distribution is a continuous distribution. This correction is applied by simply subtracting 0.5 from each observed-expected calculation. We will not use the continuity correction in this course so that R calculations will match hand calculations.}
\end{Itemize}

The results from \R{chisq.test()} should be assigned to an object so that useful information can be extracted. The chi-square test statistics and p-value are extracted by typing the name of the saved object, the expected values are extracted by appending \R{\$expected} to the object, and a visual of the p-value is obtained by submitting the object to \R{plot()}. In addition, confidence intervals for the proportions of individuals in each level are constructed by submitting the saved object to \R{gofCI()}.

\warn{The results from \R{chisq.test()} should be assigned to an object.}

\subsection{Example - Loggerhead Shrikes}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:
\vspace{-4pt}
\begin{quote}
\textsl{\cite{BohallWood1987} constructed 24 random 16-km transects along roads in counties near Gainesville, FL. Two observers censused each transect once every 2 weeks from 18 October 1981 to 30 October 1982, by driving 32 km/h and scanning both sides of the road for perched and flying shrikes (\textbf{Lanius ludovicianus}). The habitat, whether the bird was on the roadside or actually in the habitat, and the perch type were recorded for each shrike observed. Habitats were grouped into five categories. The number of shrikes observed in each habitat was 1456 in open areas, 43 in midsuccessional, 112 in scattered trees, 44 in woods, and 6 in wetlands. Separate analyses were used to construct the proportion of habitat available in each of the five habitat types. These results were as follows: 0.358 open, 0.047 midsuccessional, 0.060 scattered trees, 0.531 woods, and 0.004 wetlands. Use these data to determine, at the 5\% level, if shrikes are using the habitat in proportion to its availability.}
\end{quote}



\begin{Enumerate}
  \item $\alpha$=0.05.
  \item $H_{0}$:``distribution of habitat use by shrikes is the same as the proportions of available habitat'' vs. $H_{A}$:``distribution of habitat use by shrikes is NOT the same as the proportions of available habitat.''
  \item A Goodness-of-Fit Test is required because (i) a categorical variable was recorded (habitat use), (ii) a single population was sampled (shrikes in this area), and (iii) the observed distribution is compared to a theoretical distribution.
  \item The data appear to be part of an observational study where the individuals were not randomly selected but the transects upon which they were observed were.
  \item There are more than five individuals expected in each habitat level \tabrefp{tab:GOFshrikesRes}.
  \item The statistic is the observed frequency table in \tabref{tab:GOFshrikesRes}.
  \item $\chi^2$=2345 with 4 df \tabrefp{tab:GOFshrikesChi}.
  \item p-value$<0.00005$ \tabrefp{tab:GOFshrikesChi}.
  \item $H_{0}$ is rejected because the p-value$<\alpha$.
  \item The shrikes do not appear to use habitats in the same proportions as the availability of the habitat.
  \item The 95\% confidence intervals for the proportion of use in each habitat level are in \tabrefp{tab:GOFshrikesCIs}. From these results it appears that the shrikes use the ``open'' habitat much more often and the ``woods'' habitat mush less often than would be expected if they used all habitats in proportion to their availability.
\end{Enumerate}

\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
( obs <- c(Open=1456,MidSucc=43,ScatTree=112,Woods=6,Wetland=44) )
( p.exp <- c(Open=0.358,MidSucc=0.047,ScatTree=0.060,Woods=0.531,Wetland=0.004) )
( shrike.chi <- chisq.test(obs,p=p.exp,rescale.p=TRUE) )
data.frame(obs=shrike.chi$observed,exp=shrike.chi$expected)
gofCI(shrike.chi,digits=3)
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}

\begin{table}[h]
  \caption{Observed and expected frequencies for the Goodness-of-Fit Test for shrike habitat use.}
  \label{tab:GOFshrikesRes}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
          obs     exp
Open     1456 594.638
MidSucc    43  78.067
ScatTree  112  99.660
Woods       6 881.991
Wetland    44   6.644
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Results from the Goodness-of-Fit Test for shrike habitat use.}
  \label{tab:GOFshrikesChi}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
X-squared = 2345.071, df = 4, p-value < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Observed proportions, 95\% condidence intervals for the proprtions, and expected proportions for shrike habitat use.}
  \label{tab:GOFshrikesCIs}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
         p.obs p.LCI p.UCI p.exp
Open     0.877 0.860 0.892 0.358
MidSucc  0.026 0.019 0.035 0.047
ScatTree 0.067 0.056 0.081 0.060
Woods    0.004 0.002 0.008 0.531
Wetland  0.026 0.020 0.035 0.004
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}


\newpage
\subsection{Example - Modes of Fishing}
Consider the following:
\begin{quote}
\textsl{\cite{HerrigesKing1999} examined modes of fishing for a large number of recreational saltwater users in southern California. One of the questions asked in their Southern California Sportfishing Survey was what ``mode'' they used for fishing -- ``from the beach'', ``from a fishing pier'', ``on a private boat'', or ``on a chartered boat.''  The results to this question, along with other data not used here, are found in \href{https://raw.githubusercontent.com/droglenc/NCData/master/FishingModes.csv}{FishingModes.csv}. One hypothesis of interest states that two-thirds of the users will fish from a boat, split evenly between private and charter boats, while the other one-third will fish from land, also split even between those fishing on the beach and those from a pier. Use the data in the \var{mode} variable of the data file to determine if this hypothesis is supported at the 10\% level.}
\end{quote}



The 11-steps \sectrefp{sec:11Steps} for a hypothesis test for this example is below:
\begin{Enumerate}
  \item $\alpha$=0.10.
  \item $H_{0}$:``The distribution will follow the proportions of $\frac{1}{3}$, $\frac{1}{3}$, $\frac{1}{6}$, and $\frac{1}{6}$ for private boat, charter boat, beach, and pier modes of fishing, respectively'' vs. $H_{A}$:``The distribution will NOT follow the proportions of $\frac{1}{3}$, $\frac{1}{3}$, $\frac{1}{6}$, and $\frac{1}{6}$ for private boat, charter boat, beach, and pier modes of fishing, respectively.''  [\textit{These fractions were found with the following thought process -- the two-thirds for ``boat'' fishing is split in half for one-third each for private and charter boats; the one-third, or two-sixths, for ``land'' fishing is split in half for one-sixth each for beach and pier fishing.}]
  \item A Goodness-of-Fit Test is required because (i) a categorical variable was recorded (mode), (ii) a single population was sampled (Southern California Sportfishers), and (iii) the obseved distribution is compared to a theoretical distribution.
  \item The data appear to be part of an observational study where the individuals were not obviously (probably were not) randomly selected.
  \item There are more than five individuals expected in each mode \tabrefp{tab:GOFfishRes}.
  \item The statistic is the observed frequency table in \tabref{tab:GOFfishRes}.
  \item $\chi^2$=32 with 3 df \tabrefp{tab:GOFfishChi}.
  \item p-value$<0.00005$ \tabrefp{tab:GOFfishChi}.
  \item $H_{0}$ is rejected because the p-value$<\alpha$.
  \item The modes of fishing do not appear to match the distribution outlined in the null hypothesis.
  \item The 95\% confidence intervals for the proportion of use of each mode is in  \tabrefp{tab:GOFfishCIs}. From these results it is apparent that the users use the beach slightly less than expected and use the charter boats slightly more than expected. The use of the pier and private boats are not different from what was expected.
\end{Enumerate}

\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
sf <- read.csv("https://raw.githubusercontent.com/droglenc/NCData/master/FishingModes.csv")
obs <- xtabs(~mode,data=sf)
p.exp <- c(beach=1/6,boat=1/3,charter=1/3,pier=1/6)
( sf.chi <- chisq.test(obs,p=p.exp,rescale.p=TRUE) )
data.frame(obs=sf.chi$observed,exp=sf.chi$expected)
gofCI(sf.chi,digits=3)
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}

\begin{table}[h]
  \caption{Observed and expected frequencies for the Goodness-of-Fit Test for modes of fishing.}
  \label{tab:GOFfishRes}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
        obs.mode obs.Freq exp
beach      beach      134 197
boat        boat      418 394
charter  charter      452 394
pier        pier      178 197
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Results from the Goodness-of-Fit Test for modes of fishing.}
  \label{tab:GOFfishChi}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
X-squared = 31.9797, df = 3, p-value = 5.285e-07
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Observed proportions, 95\% condidence intervals for the proprtions, and expected proportions for modes of fishing.}
  \label{tab:GOFfishCIs}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
        p.obs p.LCI p.UCI p.exp
beach   0.113 0.097 0.133 0.167
boat    0.354 0.327 0.381 0.333
charter 0.382 0.355 0.410 0.333
pier    0.151 0.131 0.172 0.167
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}


\subsection{Example - Mendelian Genetics II}
Below are the 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for the following situation:
\vspace{-4pt}
\begin{quote}
\textsl{Geneticists hypothesized that three of every four progeny from a cross between two parent fruitflies known to possess both a dominant and recessive allele would have red eyes. In a carefully controlled experiment, 82 of 151 randomly selected progeny had red-eyes. Test at the 1\% level if the percentage of red-eyed progeny in the population of progeny is different than what the researchers hypothesized.}
\end{quote}



\begin{Enumerate}
  \item $\alpha$=0.01.
  \item $H_{0}$:``The proportion of progeny with red-eyes is 0.75'' vs. $H_{A}$:``The proportion of progeny with red-eyes is NOT 0.75.''
  \item A Goodness-of-Fit Test is required because (i) a categorical variable was recorded (eye color), (ii) a single population was used in the experiment, and (iii) the observed distribution is compared to a theoretical distribution.
  \item The data appear to be quasi-experimental in that a specific cross was made but there are very little controls. Selected progeny were randomly selected.
  \item There are more than five individuals expected in each eye level \tabrefp{tab:GOFgenRes}.
  \item The appropriate statistic is the observed frequency table in \tabref{tab:GOFgenRes}.
  \item $\chi^2$=34.49 with 1 df \tabrefp{tab:GOFgenCHI}
  \item p-value$<0.00005$ \tabrefp{tab:GOFgenCHI}.
  \item $H_{0}$ is rejected because the $p-value<\alpha$.
  \item The proportion of red-eyed progeny appears to be different than 0.75. Thus, the Mendelian theory is not supported by these results.
  \item The 95\% confidence intervals for the proportion of progeny in each eye level is in \tabrefp{tab:GOFgenCIs} From these results it appears that the proportion of progeny with red-eyes was between 0.464 and 0.620, which indicates that there were many fewer red-eyed progeny than would be expected from the Mendelian theory.
\end{Enumerate}

\begin{minipage}{\textwidth}
\textbf{R Appendix:}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{verbatim}
obs <- c(red=82,nonred=151-82)
p.exp <- c(red=0.75,nonred=0.25)
( m.chi <- chisq.test(obs,p=p.exp,rescale.p=TRUE) )
data.frame(obs=m.chi$observed,exp=m.chi$expected)
gofCI(m.chi,digits=3)
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{minipage}

\begin{table}[h]
  \caption{Observed and expected frequencies for the Goodness-of-Fit Test for the genetic cross experiment.}
  \label{tab:GOFgenRes}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
       obs    exp
red     82 113.25
nonred  69  37.75
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Results from the Goodness-of-Fit Test for the genetic cross experiment.}
  \label{tab:GOFgenChi}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
X-squared = 34.4923, df = 1, p-value = 4.279e-09
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

\begin{table}[h]
  \caption{Observed proportions, 95\% condidence intervals for the proprtions, and expected proportions for eye colors in the genetic cross experiment.}
  \label{tab:GOFgenCIs}
  \vspace{-12pt}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
       p.obs p.LCI p.UCI p.exp
red    0.543 0.464 0.620  0.75
nonred 0.457 0.380 0.536  0.25
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{table}

%    \cleardoublepage
%    \phantomsection
%    \addcontentsline{toc}{part}{Appendix}
%    \chapter*{Appendices}
%    \appendix


%  \backmatter
%    \input{FBMatter/BMatter}

\end{document}
