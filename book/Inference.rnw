<<echo=FALSE, cache=FALSE, results='hide'>>=
set_parent('IntroStats.Rnw')
@

\chapter{Inference Concepts} \label{chap:Inference}
\begin{ChapObj}{\boxwidth}
  \textbf{Chapter Objectives:}
  \begin{Enumerate}
    \item Describe the concept underlying confidence intervals.
    \item Construct confidence intervals for parameters.
    \item Use the confidence interval formula to estimate desired sample sizes.
    \item Describe the relationship between the scientific method and statistical hypothesis testing.
    \item Properly construct statistical hypotheses.
    \item Describe the concept underlying significance testing.
    \item Describe possible errors in statistical decision making.
    \item Understand the specifics of a one-sample Z-test.
    \item Perform the 11-steps of a significance test in a one-sample Z-test situation.
  \end{Enumerate}
\end{ChapObj}

\minitoc
\newpage

\lettrine{A}{ statistic is, because of sampling variability,} an imperfect estimate of the unknown parameter.  Thus conclusions about the parameter from the statistics could be in error and should include a measure of the precision of the estimate.  There are two possible calculations using the results of a single sample that recognize this imperfectness and allow conclusions about parameters.  First, a researcher may form an \emph{a priori} hypothesis about the parameter and then use the information in the sample to make a judgment about the ``correctness'' of this hypothesis.  Second, a researcher may form, from the information in the sample, a range of values that is likely to contain the parameter.  The methods of the first calculation are called \emph{hypothesis testing} whereas the second method consists of constructing a \emph{confidence region}.  In this chapter, the foundational concepts of these two methods will be explained and explored.  Specific applications are taken up in ensuing chapters.


\section{Hypothesis Testing} \label{sect:HypTest}
In its simplest form, the scientific method has four steps:\index{Scientific Method}

\begin{Enumerate}
  \item Observation and description of a natural phenomenon.
  \item Formulation of a hypothesis to explain the phenomenon.
  \item Use of the hypothesis to predict new observations.
  \item Performance of experimental tests of the predictions.
\end{Enumerate}

If the predictions do not match the results of the experiment, then the hypothesis is rejected and an alternative hypothesis is proposed.  If the predictions do closely match the results of the experiment, then belief in the hypothesis is gained, but the hypothesis will likely be subjected to further scrutiny.

Statistical hypothesis testing is key to performing the scientific method and, in fact, closely follows the scientific method in concept.  Statistical hypothesis testing begins by taking a research hypothesis and formulating it into competing statistical hypotheses.  The null hypothesis is used to make a prediction about a parameter of interest.  Data is then collected and statistical methods are used to determine whether the observed statistic closely matches the predictions made from the null hypothesis or not.  Probability is used in the measure of matching and sampling variability is taken into account.  This process and the theory underlying statistical hypothesis testing is further explained and illustrated in this section.

\subsection{The Hypotheses} \label{sec:Hypotheses}
The hypotheses in a research study are classified into two types: (1) research hypothesis and (2) statistical hypotheses.  A research hypothesis is usually a general statement, stated in prose, about the phenomenon that the researcher is interested in investigating.\index{Hypothesis!Research}  The research hypothesis must be transferred into statistical hypotheses that are more mathematical and, thus, more easily subjected to statistical methods.

\defn{Research Hypothesis}{A very generally stated hypothesis about the phenomenon of interest.}

\vspace{-12pt}
\defn{Statistical Hypothesis}{A specific mathematically stated hypothesis about the phenomenon of interest.}

There are two types of statistical hypotheses: (1) the null hypothesis and (2) the alternative hypothesis.  The \textbf{null hypothesis}, typically abbreviated with $H_{0}$, is a specific statement of no difference between a parameter and a specific value or between two parameters.\index{Hypothesis!Null}  Because $H_{0}$ is always the ``no difference'' situation it always contains the equals sign.  The \textbf{alternative hypothesis}, usually abbreviated as $H_{A}$ always states that there is a difference of some sort between a parameter and a specific value or between two parameters.\index{Hypothesis!Alternative}  The exact type of difference comes from the researcher's question of interest and results in the use of a less than (\verb"<"), greater than (\verb">"), or not equals ($\neq$) sign.  The phenomenon described in the research hypothesis is most often mathematically formalized in the $H_{A}$.

\defn{Null Hypothesis}{A statistical hypothesis that states specifically that there is no difference between a parameter and a specific value or between two parameters; typically abbreviated with H$_{0}$.}

\vspace{-12pt}
\warn{Null hypotheses always represent the ``no difference'' situation and, thus, always contain an equals sign.}

\vspace{-12pt}
\defn{Alternative Hypothesis}{A statistical hypothesis that states a specific difference between a parameter and a specific value or between two parameters; typically abbreviated with $H_{A}$.}

\vspace{-12pt}
\warn{Alternative hypotheses always represent some sort of difference and, thus, always contain one of these three directional symbols ($\neq$, $>$, and $<$).}

The relationships between the research, null, and alternative hypotheses are illustrated with the following examples (the research hypothesis is listed first):

\begin{Enumerate}
  \item A medical researcher wants to determine if a new medicine has any undesirable side effects, particularly does it change the patients' mean pulse rate.
  \begin{Itemize}
    \item $H_{A}:\mu\neq82$ and $H_{0}:\mu=82$ (where $\mu$ represents the mean pulse rate and 82 is the ``known'' mean pulse rate in the population under study; thus, this alternative hypothesis represents a change from the ``normal'' pulse rate).
  \end{Itemize}
   \item A chemist has invented an additive to automobile batteries that is intended to extend the average life of the battery.
  \begin{Itemize}
    \item $H_{A}:\mu>36$ and $H_{0}:\mu=36$ (where $\mu$ represents the mean battery life and 36 is the ``known'' mean battery life of batteries without the new additive; thus, this alternative hypothesis represents an extension of the current battery life).
  \end{Itemize}
  \item An engineer wants to determine if a new type of insulation will reduce the average heating costs of a typical house.
   \begin{Itemize}
    \item $H_{A}:\mu<78$ and $H_{0}:\mu=78$ (where $\mu$ represents the mean monthly heating bill and 78 is the ``known'' mean monthly heating bill without the new insulation; thus, this alternative hypothesis represents a decline in heating bills from the previous ``normal'' amount).
  \end{Itemize}
\end{Enumerate}

\warn{There are always two competing statistical hypotheses -- null and alternative hypotheses.}

The sign used in the alternative hypothesis comes directly from the wording of the research hypothesis \tabrefp{tab:HAwords}.  An alternative hypothesis that contains the $\neq$ sign is called a \textbf{two-tailed alternative} as the value can be ``not equal'' to another value in two ways; i.e., less than or greater than the value.  Alternative hypotheses with the $<$ or the $>$ signs are called \textbf{one-tailed alternatives}.  The null hypothesis is easily constructed from the alternative hypothesis by replacing the sign in the alternative hypothesis with an equals sign.

\begin{table}[htbp]
  \caption{Common words that indicate which sign to use in the alternative hypothesis.}
  \label{tab:HAwords}
  \centering
  \begin{tabular}{ccc}
\hline\hline
$>$ & $<$ & $\neq$ \\
\hline
is greater than & is less than & is not equal to \\
is more than & is below & is different from \\
is larger than & is lower than & has changed from \\
is longer than & is shorter than & is not the same as \\
is bigger than & is smaller than &  \\
is better than & is reduced from &  \\
is at least & is at most &  \\
is not less than & is not more than &  \\
\hline\hline
  \end{tabular}
\end{table}

\warn{The ``not-equals'' alternative is called a two-tailed alternative, whereas the other two alternative hypotheses are called one-tailed alternatives.}

\begin{exsection}
  \item \label{revex:HypTCactus} A researcher is investigating the mean growth of a certain cactus under a variety of environmental conditions. Under the current environmental conditions, he hypothesizes that mean growth is no more than 4 cm. What is $H_{0}$ and $H_{A}$ in this situation? \ansref{ans:HypTCactus}
  \item \label{revex:HypTBodyTemp} \cite{Machowiaketal1992} critically examined the belief that the mean body temperature differed from 98.6$^{0}$F by measuring the body temperatures of 93 healthy humans.  What is $H_{0}$ and $H_{A}$ in this situation? \ansref{ans:HypTBodyTemp}
  \item \label{revex:HypTPain} A study by \cite{Cheshireetal1994} reported on six patients with chronic myofascial pain syndrome. The authors were examining the hypothesis that the mean pain length was greater than 2.5 years.  What is $H_{0}$ and $H_{A}$ in this situation? \ansref{ans:HypTPain}
\end{exsection}


\subsection{Concept}\index{Hypothesis Testing!Concept}
Statistical hypothesis testing begins by using the null hypothesis to make a prediction of what value one should expect for the mean in a sample.  So, for the Square Lake example, if $H_{0}:\mu=105$ and $H_{A}:\mu<105$, then one would expect, if the null hypothesis is true, that the observed mean in a sample would be 105.  If sampling variability did not exist and the observed sample mean was NOT equal to 105, then the prediction based on the null hypothesis would not be supported and the conclusion would be that the null hypothesis is incorrect.  In other words, one could conclude that the population mean was not equal to 105.

Of course, sampling variability does exist and its existence complicates matters.  The simple interpretation of not supporting $H_{0}$ because the observed sample mean did not equal the hypothesized population mean canNOT be made because, with sampling variability, one would not expect a statistic to exactly equal the parameter in the population from which the sample was extracted.  For example, even if the null hypothesis was correct, then one would not expect, with sampling variability, the observed sample mean to exactly equal 105; rather, one would expect the observed sample mean to be \textbf{reasonably} close to 105.

Thus, hypothesis testing is a procedure for determining if the difference between the observed statistic and the expected statistic based on the null hypothesis is ``large'' \textbf{relative to sampling variability}.  For example, the standard error of $\bar{x}$ in samples of $n=50$ in the Square Lake example is equal to $\frac{\sigma}{\sqrt{n}}=$$\frac{31.5}{\sqrt{50}}$$=4.45$.  Thus, with this amount of sampling variability, an observed sample mean of 103 would be considered reasonably close to 105 and one would have more belief in $H_{0}:\mu=105$.  However, an observed sample mean of 70 would be considered further away from 105 than one would expect based on sampling variability alone and the belief in $H_{0}:\mu=105$ would lessen.

While the above procedure is intuitively appealing, it loses some of its objectivity when the examples chosen (i.e., samples means of 103 and 70) are not as extremely close or distant from the null hypothesized value (e.g., what would the ``conclusion'' be if the observed sample mean was 97?).  A first step in creating a more objective decision criteria is to compute a value called the ``p-value.''\index{p-value}  A p-value is defined as the probability of the observed statistic or a value of the statistic more extreme assuming that the null hypothesis is true.  A more detailed description of the p-value is warranted given the centrality of the p-value to making conclusions about statistical hypotheses.

\defn{p-value}{The probability of the observed statistic or a value of the statistic more extreme assuming the null hypothesis is true.}

The meaning of the phrase ``or more extreme'' is derived from the sign in $H_{A}$ \figrefp{fig:HOtails}.  If $H_{A}$ is the ``less than'' situation, then ``or more extreme'' means ``less than'' or ``shade to the left'' for the probability calculation.  The ``greater than'' situation is defined similarly but would result in shading to the ``right.''  In the ``not equals'' situation, ``or more extreme'' means further into the tail AND the exact same size of tail on the other side of the distribution.  It should be clear from \figref{fig:HOtails} why the ``less than'' and ``greater than'' alternatives are called one-tailed hypotheses and the ``not equals'' alternative is called a two-tailed hypothesis.

<<HOtails, echo=FALSE, fig.width=6, fig.height=2, out.width='.8\\linewidth', fig.cap="Depiction of the meaning of ``or more extreme'' in the calculation of p-values for the three possible alternative hypotheses.">>=
par(mfcol=c(1,3),mar=c(2,0,2,0),las=1,tcl=-0.2)
x0 <- seq(-4,4,by=0.001)
norm0 <- dnorm(x0,0,1)
plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu<mu[0])),3)
cv1 <- -1.5
xc1 <- c(cv1,x0[x0<=cv1],cv1)
yc1 <- c(0,norm0[x0<=cv1],0)
polygon(xc1,yc1,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv1,-0.03,expression(bar(x)),cex=2,xpd=TRUE)

plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu>mu[0])),3)
cv2 <- 1.5
xc2 <- c(cv2,x0[x0>=cv2],cv2)
yc2 <- c(0,norm0[x0>=cv2],0)
polygon(xc2,yc2,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv2,-0.03,expression(bar(x)),cex=2,xpd=TRUE)

plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu!=mu[0])),3)
polygon(xc1,yc1,col="red",border="red")
polygon(xc2,yc2,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv2,-0.03,expression(bar(x)),cex=2,xpd=TRUE)
@

The ``assuming that the null hypothesis is true'' phrase is used to define a $\mu$ for the sampling distribution on which the p-value will be calculated.  This sampling distribution is called the \textbf{null distribution} because it depends on the value of $\mu$ in the null hypothesis.  One must remember that the null distribution represents the distribution of all possible sample means assuming that the null hypothesis is true; it does NOT represent the actual sample means\footnote{Of course, unless the null hypothesis happens to be perfectly true.}.  The null distribution in the Square Lake example is thus $\bar{x}\sim N(105,4.45)$ because $n=50>30$, $H_{0}:\mu=105$, and SE=$\frac{31.49}{\sqrt{50}}$=$4.45$.

The p-value is computed with a ``forward'' normal distribution calculation on the null sampling distribution.  For example, suppose that a sample mean of 100 was observed in a sample of $n=50$ from Square Lake (as it was in \tabref{tab:SquareLakeSample1}).  The definition of the p-value in this particular instance would be ``the probability of observing $\bar{x}=100$ or a smaller value assuming that $\mu=105$.''  The answer to this definition is computed by finding the area to the left of 100 on a $N(105,4.45)$ null distribution.  This is the exact same type of calculation that was made on sampling distributions in \sectref{sect:sdprob}.  Thus, a p-value of \Sexpr{kPvalue(pnorm(100,mean=105,sd=31.49/sqrt(50)))} is computed and visualized \figrefp{fig:SLpvalue1} with

<<SLpvalue1, par1a=TRUE, fig.cap="Depiction of the p-value for the Square Lake example where $\\bar{x}=100$ and $H_{A}:\\mu<105$.">>=
( distrib(100,mean=105,sd=31.49/sqrt(50)) )
@

Interpreting the p-value requires critically thinking about the p-value definition and how it is calculated.  Small p-values appear when the observed statistic is ``far'' from the value expected from the null hypothesis.  In this case there is a small probability of seeing the observed statistic ASSUMING that $H_{0}$ is true.  Thus, the assumption is likely wrong and $H_{0}$ is likely incorrect.  In contrast, large p-values appear when the observed statistic is close to the null hypothesized value suggesting that the assumption about $H_{0}$ may be correct.

\warn{Small p-values are evidence against the null hypothesis.}

The p-value serves as a numerical measure on which to base a conclusion about $H_{0}$.  To do this objectively requires an objective definition of what it means to be a ``small'' or ``large'' p-value.  Statisticians use a cut-off value, called the rejection criterion which is symbolized as $\alpha$, such that p-values less than $\alpha$ are considered small and would result in the rejection of $H_{0}$ as a viable hypothesis.\index{alpha@{$\alpha$}}  The value of $\alpha$ is typically small, usually set at $0.05$, although $\alpha=0.01$ and $\alpha=0.10$ are also commonly used.

\defn{$\alpha$}{A predetermined rejection criterion value used in hypothesis testing.  This value sets the ``cutoff'' for determining whether it was reasonable to have seen the observed statistic or not assuming the null hypothesis is true.}

\vspace{-12pt}
\warn{Typical values of $\alpha$ are 0.01, 0.05, and 0.10.}

The choice of $\alpha$ is made by the person conducting the hypothesis test and is based on how much evidence a researcher demands before rejecting $H_{0}$.  Smaller values of $\alpha$ require a larger difference between the observed statistic and the null hypothesized value and, thus, require ``more evidence'' of a difference for the $H_{0}$ to be rejected.  For example, if a rejection of the null hypothesis will be heavily scrutinized by regulatory agencies, then the researcher may want to be very sure before claiming a difference and should then set $\alpha$ at a smaller value, say $\alpha=0.01$.  The actual choice for $\alpha$ MUST be made before collecting any data and canNOT be changed once the data has been collected.  In other words, once the data are in hand, a researcher cannot lower or raise $\alpha$ to achieve a desired outcome regarding $H_{0}$.

\warn{The value of the rejection criterion ($\alpha$) is set by the researcher BEFORE data is collected.}

\vspace{-12pt}
\warn{Set $\alpha$ to lower values to make it more difficult to reject $H_{0}$.}

The null hypothesis in the Square Lake example was not rejected because the calculated p-value (i.e., \Sexpr{kPvalue(pnorm(100,mean=105,sd=31.49/sqrt(50)))}) is larger than any of the common values of $\alpha$.  Thus, the conclusion in this example is that it is possible that the mean of the entire population is equal to 105 and it is not likely that the population mean is less than 105.  In other words, observing a sample mean of 100 is likely to happen based on random sampling variability alone and it is unlikely that the null hypothesized value is incorrect.

\begin{exsection}
  \item \label{revex:HypTCalc1} Compute the p-value and make a decision about $H_{0}$ with the following information -- $\alpha=0.10$, $H_{0}:\mu=10$, $H_{A}:\mu>10$, $\sigma=5$, $n=25$, and $\bar{x}=12.1$. \ansref{ans:HypTCalc1}
  \item \label{revex:HypTCalc2} Compute the p-value and make a decision about $H_{0}$ with the following information -- $\alpha=0.05$, $H_{0}:\mu=50$, $H_{A}:\mu<50$, $\sigma=20$, $n=50$, and $\bar{x}=43.8$. \ansref{ans:HypTCalc2}
  \item \label{revex:HypTCalc3}  Compute the p-value and make a decision about $H_{0}$ with the following information -- $\alpha=0.01$, $H_{0}:\mu=100$, $H_{A}:\mu\neq100$, $\sigma=15$, $n=100$, and $\bar{x}=98$. \ansref{ans:HypTCalc3}
  \item \label{revex:HypTWhy} Describe why we must formally go through the steps of a hypothesis test to conclude that $\mu>11$ when we observe $\bar{x}=12.1$. \ansref{ans:HypTWhy}
\end{exsection}

\subsection{Effect Size Type Calculations}
Instead of just reporting the observed statistic and the resulting p-value, it may be of interest to a researcher to know how ``far'' the observed statistic was from the hypothesized value of the parameter.  This simplistic value is easily calculated with
\[ \text{Observed Statistic}-\text{Hypothesized Parameter} \]
where ``Hypothesized Parameter'' represents the specific value in $H_{0}$.  However, the meaning of this value is difficult to interpret without an understanding of the standard error of the statistic.  For example, a difference of 10 between the observed statistic and the hypothesized parameter seems ``very different'' if the standard error is 1 but does not seem ``different'' if the standard error is 100.  Thus, it is common practice to standardize this difference by dividing by the standard error of the statistic.  This measure of distance is called a \emph{test statistic} and is generalized with

\begin{equation}  \label{eqn:zTestStatGeneral}
  \text{Test Statistic} = \frac{\text{Observed Statistic}-\text{Hypothesized Parameter}}{SE_{\text{Statistic}}}
\end{equation}

Thus, the test statistic \eqref{eqn:zTestStatGeneral} measures how many standard errors the observed statistic is away from the hypothesized parameter.  Relatively large values are indicative of a difference that is likely not due to randomness (i.e., sampling variability) and suggest a rejection of the null hypothesis.  There are other forms for calculating test statistics\footnote{A few of which we will see in this class.}, but all test statistics retain the general idea of scaling the difference between what was observed and what was expected from the null hypothesis in terms of sampling variability.  Even though there is a one-to-one relationship between a test statistic and a p-value, a test statistic is often reported with a hypothesis test to give another feel for the magnitude of the difference between what was observed and what was predicted.

\warn{A test statistic measures how many standard errors the observed statistic is away from the hypothesized parameter.}


\subsection{Summary of Concept}\index{Hypothesis Testing!Concept}\index{p-value}\index{alpha@{$\alpha$}}
In summary, hypothesis tests are statistically examined with the following procedure.
\begin{Enumerate}
  \item Construct null and alternative statistical hypotheses from the research hypothesis.
  \item Construct an expected value of the statistic based on the null hypothesis (i.e., assume that the null hypothesis is true).
  \item Calculate an observed statistic from the individuals in a sample.
  \item Compare the difference between the observed statistic and the expected statistic based on the null hypothesis in relation to sampling variability (i.e., calculate a test statistic and p-value).
  \item Use the $p-value$ to determine if this difference is ``large'' or not .
  \begin{Itemize}
    \item If this difference is ``large'' (i.e., $p-value<\alpha$), then reject the null hypothesis.
    \item If this difference is not ``large'' (i.e., $p-value>\alpha$), then ``Do Not Reject'' the null hypothesis.
  \end{Itemize}
\end{Enumerate}

When the p-value $>\alpha$, suggesting little evidence against the null hypothesis, you must be very careful to say ``do not reject H$_{0}$'' rather than ``accept H$_{0}$ as true.'' There are two reasons for this very specific choice of words.

First, there are several other possible values, besides the specific value of the null hypothesis, that would lead to ``do not reject'' conclusions.  For example, if a null hypothesized value of 105 was not rejected, then values of 104.99, 104.98, etc. would also likely not be rejected\footnote{In fact, for example, the values in a 95\% confidence interval -- see next section -- represent all possible hypothesized values that would not be rejected with a two-tailed alternative hypothesis using $\alpha=0.05$.}  So, we don't want to say that we ``accept'' a particular hypothesized value when we know many other values would also be ``accepted.''

Second, the null hypothesis is almost always not true.  Consider the null hypothesis of the Square Lake example (i.e., ``that the mean length is 105'').  The mean length of fish in Square Lake is undoubtedly not exactly equal to 105.  It may be 104.9, 105.01, or some other more disparate value.  The point is that the specific value of the hypothesis is likely never true, especially for a continuous variable.  The problem is that it takes large amounts of data to be able to distinguish means that are very close to the true population mean (i.e., it is difficult to distinguish between 104.9 and 105 when sampling variability is present).  Very often we will not take a sample size large enough to distinguish these subtle differences.  Thus, we will say that we ``do not reject H$_{0}$'' because there simply was not enough data to reject it.

\vspace{-18pt}
\begin{exsection}
\vspace{-12pt}
  \item \label{revex:HypTEffluent} The managers of a wastewater treatment plant monitored the amount of biological oxygen demand (BOD; lbs/day) in the effluent of the plant each month from January 1991 to October 2000.  The managers would need to take corrective actions if the average BOD over this time period was significantly greater than 2200 lbs/day at a 10\% rejection level.  Previous studies indicated that the standard deviation was 1200 lbs/day.  Summary statistics from their sample of days is given below.  Use this information to answer the questions below. \ansref{ans:HypTEffluent}
  \begin{Verbatim}
    n  Min. 1st Qu.  Median   Mean  3rd Qu.    Max.
  118   630    1600    2240   2504     3193    6023
  \end{Verbatim}

  \begin{Enumerate}
    \item What are the null and alternative hypotheses?
    \item What is the test statistic?
    \item \rhw{} Compute the p-value.
    \item Use the p-value to make a decision about $H_{0}$.
    \item What does this mean for the managers of the plant (i.e., will they need to take action)? Explain!
  \end{Enumerate}

  \item \label{revex:HypTMedSchool} Admissions representatives at the University of Minnesota medical school were concerned that the average grade point average of applicants in non-science courses had dropped below 3.7.  A sample of 40 of the most recent applicants indicated that the mean was 3.60.  Information from the Association of American Medical Colleges suggested that the overall standard deviation was 0.35.  Use this information, and an $\alpha=0.05$, to answer the questions below. \ansref{ans:HypTMedSchool}
  \begin{Enumerate}
    \item What are the null and alternative hypotheses?
    \item What is the test statistic?
    \item \rhw{} Compute the p-value.
    \item Use the p-value to make a decision about $H_{0}$.
    \item Was the representatives concern about the average gpa of applicants warranted?  Explain!
  \end{Enumerate}

\end{exsection}

\subsection{Errors and Power}\index{Hypothesis Testing!Errors}\index{Power}
The goal of hypothesis testing is to make a decision about $H_{0}$.  Unfortunately, because of sampling variability, there is always a risk of making an incorrect decision.  Two types of incorrect decisions can be made \tabrefp{tab:DMerrs}.  A Type I error occurs when a true $H_{0}$ is falsely rejected.  In other words, even if $H_{0}$ is true, there is a chance that a rare sample will occur and $H_{0}$ will be deemed incorrect.  The probability of making a Type I error is set when $\alpha$ is chosen.\index{alpha@{$\alpha$}}  A Type II error occurs when a false $H_{0}$ is not rejected.  The probability of a Type II error is denoted by $\beta$.\index{beta@{$\beta$}}

\begin{table}[htbp]
  \caption{Types of decisions that can be made from a hypothesis test.}
  \label{tab:DMerrs}
  \centering
  \begin{tabular}{cc|c|c|}
    \multicolumn{1}{c}{\widen{-2}{7}{}} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Decision from Data} \\
    \cline{3-4}
    \multicolumn{1}{c}{\widen{-2}{7}{}} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Reject} & \multicolumn{1}{c|}{Not Reject} \\
    \cline{2-4}
    \multicolumn{1}{c|}{\widen{-2}{7}{Truth About}} & \multicolumn{1}{c|}{$H_{0}$} & \multicolumn{1}{c|}{Type I} & \multicolumn{1}{c|}{Correct} \\
    \cline{2-4}
    \multicolumn{1}{c|}{\widen{-2}{7}{Population}} & \multicolumn{1}{c|}{$H_{A}$} & \multicolumn{1}{c|}{Correct} & \multicolumn{1}{c|}{Type II} \\
    \cline{2-4}
  \end{tabular}
\end{table}

\defn{Type I error}{Rejecting $H_{0}$ when $H_{0}$ was actually true.  Probability of Type I error is $\alpha$.}

\vspace{-12pt}
\defn{Type II error}{Not rejecting $H_{0}$ when $H_{0}$ was actually false.  Probability of Type II error is $\beta$.}

The decision in the Square Lake example above produced a Type II error because $H_{0}:\mu=105$ was not rejected even though we know that $\mu=98.06$ \tabrefp{tab:SquareLakePopn}.  Unfortunately, in real life, it will never be known exactly when a Type I or a Type II error has been made because the true $\mu$ is not known.  However, it is known that a Type I error will be made $100\alpha$\% of the time.  The probability of a type II error ($\beta$), though, is never known because this probability depends on the true $\mu$.  Decisions can be made, however, that affect the magnitude of $\beta$ (discussed below with power).

A concept that is very closely related to decision-making errors is the idea of \textbf{power}.\index{Power}  Power is the probability of correctly rejecting a false $H_{0}$.  In other words, it is the probability of detecting a difference from the hypothesized value if a difference really exists.  That is, power is used to demonstrate how sensitive a hypothesis test is for identifying a difference.  High power related to a $H_{0}$ that is not rejected implies that the $H_{0}$ really should not have been rejected.  Conversely, low power related to a $H_{0}$ that was not rejected implies that the test was very unlikely to detect a difference, so not rejecting $H_{0}$ is not surprising nor particularly conclusive.

\defn{Power}{The probability of correctly rejecting $H_{0}$ when $H_{0}$ was actually false.}

Power is equal to $1-\beta$ and, thus, like $\beta$ it cannot be computed directly.  However, a researcher can make decisions that will positively affect power (\figref{fig:SLPowerRelations}, \appref{app:PowerAnim}).  For example, a researcher can positively impact power by increasing $\alpha$ or by increasing $n$.  Increasing $n$ is a more beneficial alternative as it does not result in an increase in Type I errors as increasing $\alpha$ would.  In addition, power decreases as the difference between the hypothesized mean ($\mu_{0}$) and the actual mean ($\mu_{A}$) decreases.  This means that the ability to detect increasingly smaller differences decreases.  In addition, power decreases with an increasing amount of natural variability (i.e., $\sigma$).  In other words, the ability to detect a difference decreases with increasing amounts of variability among individuals.  A researcher cannot control the difference between $\mu_{0}$ and $\mu_{A}$ or the value of $\sigma$.  However, it is important to know that if a situation with a ``large'' amount of variability is encountered or the difference to be detected is small, the researcher will need to increase $n$ to gain power.

\warn{Power = 1-$\beta$.}

\vspace{-12pt}
\warn{Power will increase as the difference between the actual and hypothesized value of the parameter increases.}

\vspace{-12pt}
\warn{Power will increase as the standard error of the statistic decreases.  Thus, power increases as the sample size increases.}

\vspace{-12pt}
\warn{Power will increase as the $\alpha$ level increases.}

<<SLPowerRelations, echo=FALSE, fig.width=7, fig.height=7, out.width='.6\\linewidth', fig.cap="The relationship between one-tailed (lower) power and $\\alpha$, $n$, actual mean ($\\mu_{A}$), and $\\sigma$.  In all situations where the variable does not vary, $\\mu_{0}=105$, $\\mu_{A}=98.06$, $\\sigma=31.49$, $n=50$, and $\\alpha=0.05$. ">>=
par(mfcol=c(2,2),mar=c(3.5,3.5,0.5,0.5),mgp=c(2.1,0.4,0),tcl=-0.2,cex.lab=1.5,las=1)
power <- function(mu0,mua,sigma,n,alpha,uptail=FALSE){
  SE <- sigma/sqrt(n)
  if (uptail) 1-pnorm(qnorm(1-alpha,mu0,SE),mua,SE)
  else pnorm(qnorm(alpha,mu0,SE),mua,SE)
}

mu0 <- 105                                    # Set the Square Lake values
muA <- 98.06
sigma <- 31.49
n <- 50
alpha <- 0.05
ylmts <- c(0,1)
alphas <- seq(0.002,0.1,by=0.002)            # Cycle through alphas
palpha <- power(mu0,muA,sigma,n,alphas,FALSE)
plot(alphas,palpha,type="l",xlab=expression(alpha),ylab="Power",lwd=3,xlim=c(0,0.1),ylim=ylmts,cex=1.5)
muas <- seq(85,105,by=1)                      # Cycle through muas
pmua <- power(mu0,muas,sigma,n,alpha,FALSE)
plot(muas,pmua,type="l",xlab=expression(mu[A]),ylab="Power",lwd=3,ylim=ylmts)
ns <- seq(2,250)                             # Cycle through sample sizes
pn <- power(mu0,muA,sigma,ns,alpha,FALSE)
plot(ns,pn,type="l",xlab="n",ylab="Power",lwd=3,xlim=c(0,max(ns)),ylim=ylmts)
sigmas <- seq(1,60,by=0.5)                   # Cycle through sigmas
psigma <- power(mu0,muA,sigmas,n,alpha,FALSE)
plot(sigmas,psigma,type="l",xlab=expression(sigma),ylab="Power",lwd=3,xlim=c(0,max(sigmas)),ylim=ylmts)
@

% Power cannot usually be calculated because the actual mean ($\mu_{A}$) is usually not known.  However, in the Square Lake example, $\mu_{A}$ is known and power can be calculated as shown in \figref{fig:SLPowerCalcEx}.  There are four steps (each with multiple parts) to this calculation:
% \begin{Enumerate}
%   \item Draw the sampling distribution assuming the $H_{0}$ is true (called the null distribution).
%   \begin{itemize}
%     \item The null distribution is $N(105,\frac{31.49}{\sqrt{50}})$ because $H_{0}:\mu=105$, $\sigma=31.49$, and $n=50$.
%   \end{itemize}
%   \item Find the rejection region borders (based on $\alpha$ and $H_{A}$) in terms of the value of the statistic (a ``reverse'' calculation on the null distribution).
%   \begin{itemize}
%     \item The rejection region is delineated by the $\bar{x}$ that has $\alpha=0.10$ to the left (because $H_{A}$ is a ``less than'') of it.  The Z with 0.10 to the left of it is -1.282.  Thus, the $\bar{x}$ on the null distribution with 0.10 to the left of it is $-1.282\frac{31.49}{\sqrt{50}}+105 = 99.2908$.
%   \end{itemize}
%   \item Draw the sampling distribution corresponding to the ``actual'' parameter value (SE is the same as that for the null distribution).
%   \begin{itemize}
%     \item The actual $\mu$ is 98.06.  Thus, the actual sampling distribution is $N(98.06,\frac{31.49}{\sqrt{50}})$.
%   \end{itemize}
%   \item Compute the portion of the ``actual'' sampling distribution in the REJECTION region of null distribution (i.e., a ``forward'' calculation on the actual distribution).
%   \begin{itemize}
%     \item This computation is to find the area to the left of 99.2908 on $N(98.06,\frac{31.49}{\sqrt{50}})$.  The corresponding z is $\frac{99.2908-98.06}{\frac{31.49}{\sqrt{50}}}=0.29$.  The area to the left of this z is 0.6141.
%   \end{itemize}
% \end{Enumerate}

% Thus, the power to detect a $\mu_{A}=98.06$ was 0.6141 (the value in \figref{fig:SLPowerCalcEx} is slightly more accurate because the approximate Z table was not used).  This means that only about 61\% of the time will the false $H_{0}:\mu=105$ be correctly rejected.  Thus, it is not too surprising that we did not reject $H_{0}$ in this example.  If $n$ could be doubled to 100, however, the power to correctly reject $H_{0}:\mu=105$ would increase to approximately 0.82 \figrefp{fig:SLPowerRelations}.

% \begin{figure}[htbp]
%   \centering
%     \includegraphics[width=4in]{Figs/SLPower.png}
%   \caption{The calculation of the power to detect $\mu_{A}=98.06$ in the Square Lake example.}
%   \label{fig:SLPowerCalcEx}
% \end{figure}

\begin{exsection}
  \item \label{revex:HypTPower1} What is $\beta$ if power=0.875? \ansref{ans:HypTPower1}
  \item \label{revex:HypTPowerAlpha} For a constant sample size, $\sigma$, and difference between the hypothesized and actual means, what happens to power, if $\alpha$ is increased?  \ansref{ans:HypTPowerAlpha}
  \item \label{revex:HypTPowern} For a constant $\alpha$, $\sigma$, and difference between the hypothesized and actual means, what happens to power, if the sample size increases? \ansref{ans:HypTPowern}
  \item \label{revex:HypTPowerMu} For a constant $\alpha$, $\sigma$, and sample size, what happens to power if the difference between the hypothesized and actual means increases? \ansref{ans:HypTPowerMu}
  \item \label{revex:HypTBetaAlpha} For a constant sample size, $\sigma$, and difference between the hypothesized and actual means, what happens to $\beta$, if $\alpha$ is increased? \ansref{ans:HypTBetaAlpha}
  \item \label{revex:HypTBetan} For a constant $\alpha$, $\sigma$, and difference between the hypothesized and actual means, what happens to $\beta$, if the sample size is increased? \ansref{ans:HypTBetan}
  \item \label{revex:HypTBetaMu} For a constant $\alpha$, $\sigma$, and sample size, what happens to $\beta$ if the difference between the hypothesized and actual means increases? \ansref{ans:HypTBetaMu}
  \item \label{revex:HypTRealLife} Describe a real-life situation where you think that making a Type II error would be much more ``costly'' than making a Type I error.  Completely describe the situation at hand and what Type I and a Type II errors mean in terms of the situation you describe. \ansref{ans:HypTRealLife}
%  \item \label{revex:HypTCalcPwr1} Compute power given the following information: $\alpha=0.05$, $H_{0}:\mu=50$, $H_{A}:\mu<50$, $\sigma=20$, $n=50$, and $\mu_{A}=45$. \ansref{ans:HypTCalcPwr1}
%  \item \label{revex:HypTCalcPwr2} Compute power given the following information: $\alpha=0.10$, $H_{0}:\mu=10$, $H_{A}:\mu>10$, $\sigma=5$, $n=25$, and $\mu_{A}=12$. \ansref{ans:HypTCalcPwr2}
%  \item \label{revex:HypTCalcPwr3} Compute power given the following information: $\alpha=0.01$, $H_{0}:\mu=75$, $H_{A}:\mu>75$, $\sigma=15$, $n=30$, and $\mu_{A}=82$. \ansref{ans:HypTCalcPwr3}
\end{exsection}



\section{Confidence Regions} \label{sect:CI}
The final result from a hypothesis test can be somewhat uneventful -- i.e., the conclusion is either that the parameter may be equal to or that the parameter is different from the hypothesized value\footnote{Depending on the $H_{A}$ it may be known if the parameter is more or less than the hypothesized value.}.  If the parameter is thought to be different from the hypothesized value we might go as far as to say that our best guess at the parameter is the value of our observed statistic.  However, as has been seen many times, a statistic is, because of sampling variability, an imperfect estimate of the unknown parameter.  Thus, this imperfectness can be recognized by computing, from the results of a sample, a range of values that is likely to contain the parameter.  This range is called a confidence region for the unknown parameter.  For example, we may make a statement such as this -- ``Our best guess for the true population mean length of fish in Square lake is the sample mean of 98.5 mm; however, we are 95\% confident that the mean of ALL fish in the lake is between 95.9 and 101.1 mm.''  This last statement is the interpretation of a confidence interval and is important because it acknowledges sampling variability in the inferential statement.  In this section, the concept, calculation, and interpretation of confidence regions will be explored.

\subsection{Concept}\index{Confidence!Concept} \label{sect:CIconcept}
A complete understanding of what it means to be ``95\% confident'' requires examining multiple samples from a population in much the same way as how the concept of sampling variability was explored in \chapref{chap:SamplingDist}.  For the sake of simplicity in this exploration, the discussion here will be restricted to a confidence interval (CI) where a range, bounded on both ends, is computed.  In addition, a 95\%, rather than a more general value, CI will be used.  General methods for constructing confidence regions of different types with different levels of confidence will be discussed thoroughly in the next section.  These simplifying restrictions and the unrealistic idea that population values are known are made here only so that the \textbf{concept} of confidence intervals can be explored more easily.

Define a 95\% CI for $\mu$ as $\bar{x}\pm2SE_{\bar{x}}$.  In addition, as concern rests with whether a CI contains $\mu$ or not, recall that $\mu$=98.06 and $\sigma$=31.49 for the Square Lake population \tabrefp{tab:SquareLakePopn}.  Further recall from \tabref{tab:SquareLakeSample1} that the first sample of $n$=50 from the Square Lake population resulted in $\bar{x}=100.04$.  Using the CI formula above, the associated 95\% CI is $100.04\pm2\frac{31.49}{\sqrt{50}}$, $100.04\pm8.91$, or (91.13,108.95).  In this exploratory example $\mu$ is known and, thus, it can be said that this interval does indeed contain $\mu$.  In other words, this particular CI accomplishes what it was intended to do, i.e., provide a range that contains $\mu$.

Despite the success observed in this first sample, not all confidence intervals will contain $\mu$.  For example, four out of 100 95\% confidence intervals shown in \figref{fig:CIex100} did not contain $\mu$.  Thus, four times in these 100 samples the researcher would have concluded that $\mu$ was in an incorrect interval.  The concept of ``confidence'' in confidence regions is related to determining how often these types of mistakes are made.

<<CIex100, echo=FALSE, fig.width=4, fig.height=5, out.width='.6\\linewidth', fig.cap="Sampling distribution of the sample mean (top) and 100 random 95\\% confidence intervals (horizontal lines) from samples of $n$=50 from the Square Lake population.  Confidence intervals that do NOT contain $\\mu$=98.06 are shown in red.">>=
make.smplingdist <- function(mu,SE,C=0.95) {
  op <- par(mar=(c(0,0,0,0)),las=1,tcl=-0.2)                              # set all graph margins to zero
  z <- qnorm(C+(1-C)/2,0,1)                                               # find z for the given level of confidence
  x <- seq(mu-4.5*SE,mu+4.5*SE,by=0.1)                                    # x-coords for sampling distribution
  plot(x,dnorm(x,mu,SE),type="l",lwd=3,xlab="",ylab="",axes=F)            # plot the normal sampling distribution
  lines(c(min(x),max(x)),c(0,0),lwd=2)                                    # put a pseudo-x-axis
  lpi <- mu-z*SE; upi <- mu+z*SE                                          # find lower & upper value of expected sample mns
  pi.ht <- dnorm(mu-2.5*SE,mu,SE)                                         # set height for bar of expected sample means
  polygon(c(lpi,lpi,upi,upi,lpi),c(0,pi.ht,pi.ht,0,0),col="light green")  # make horiz bar that covers expected sample means
  lines(c(mu,mu),c(0,dnorm(mu,mu,SE)),lwd=3,lty=3)                        # put vert line at mu
  par(op)
} # end make.smplingdist

make.CI<-function(mu,SE,C=0.95,num=100) {
  op <- par(mar=(c(0,0,0,0)),las=1,tcl=-0.2)                              # set all graph margins to zero
  z <- qnorm(C+(1-C)/2,0,1)                                               # find z for the given level of confidence
  xlmts <- c(mu-4.5*SE,mu+4.5*SE); ylmts <- c(1,num)                      # find limits of x- and y-coord over which to plot
  plot(0,0,type="l",xlim=xlmts,ylim=ylmts,axes=FALSE)                     # make a schematic graphic -- basically empty
  for (i in 1:num) {                                                      # cycle through the number of resamples
    xbar <- rnorm(1,mu,SE)                                                #   find the random sample mean
    lci <- xbar-z*SE; uci <- xbar+z*SE                                    #   compute the CI values
    clr <- ifelse(uci<mu,"red",ifelse(lci>mu,"red","blue"))               #   CI line will be red if missed mu, otherwise blue
    lines(c(lci,uci),c(i,i),col=clr)                                      #   plot the CI line
    points(xbar,i,cex=0.5,pch=19,col=clr)                                 #   put point at sample mean
  }  #i
  lines(c(mu,mu),c(ylmts[1]-10,ylmts[2]+10),lwd=3,lty=3)                  # plot a vertical line at mu
  lpi <- mu-z*SE; upi <- mu+z*SE                                          # find lower & upper value of expected sample mns
  lines(c(lpi,lpi),c(ylmts[1]-10,ylmts[2]+10),lwd=2,lty=3,col="green")    # plot vert line at lower expected sample mn value
  lines(c(upi,upi),c(ylmts[1]-10,ylmts[2]+10),lwd=2,lty=3,col="green")    # plot vert line at upper expected sample mn value
  par(op)
} # end make.CI

set.seed(20)
layout(c(1,2),1,.3)                                                       # Create new window w/ 2 panes - top is smaller
n <- 50; mu <- 98.06; sigma <- 31.49; SE <- sigma/sqrt(n)
make.smplingdist(mu,SE)                                                   # Calls fnx to make the sampling distribution
make.CI(mu,SE)                                                            # Calls fnx to take samples and make CIs
@

\vspace{12pt}  % to handle overlap by floating figure

From the Central Limit Theorem, the sampling distribution of $\bar{x}$ for samples of $n$=50 is $N(98.06,\frac{31.49}{\sqrt{50}})$ or $N(98.06,4.45)$ for this known population.  According to the 68-95-99.7\% Rule, it is known that 95\% of the sample means in this sampling distribution will be between $\mu\pm2SE$ or, in this specific case, between $98.06\pm2(4.45)$.  The sampling distribution and this range of expected sample means is shown at the top of \figref{fig:CIex100}.  In addition, the range of expected sample means is extended down through all of the CI lines in \figref{fig:CIex100}.  Note that any sample that produced a sample mean (solid dot on the CI line) inside of the expected range of sample means also produced a 95\% CI that contained $\mu$ (i.e., blue CI line).  Thus, because 95\% of the sample means will be within the expected range of sample means, 95\% of the 95\% CIs will contain $\mu$.  So, ``95\% confident'' means that 95\% of all 95\% CIs will contain the parameter and 5\% will not.  In other words, the mistake identified above will be made with 5\% of all 95\% confidence intervals.

The specifics for constructing confidence regions with different levels of confidence will be described below.  However, at this point, it should be noted that the number of CIs expected to contain the parameter of interest is set by the level of confidence used to construct the CI.  For example, 80\% of 80\% CIs and 90\% of 90\% CIs will contain the parameter of interest.  In either case, a particular CI either does or does not contain the interval and, in real-life, we will never know whether it does or does not (i.e., we won't know the value of the parameter).  However, we do know that the technique (i.e., the construction of the CI) will ``work'' (i.e., contain the parameter) a set percentage of the time.  To reiterate this point, examine the 100 90\% CIs (\figref{fig:CI9080Ex}-Left) and 100 80\% CIs (\figref{fig:CI9080Ex}-Right) for the Square Lake fish length data.

<<CI9080Ex, echo=FALSE, fig.width=6, fig.height=4.5, out.width='.95\\linewidth', fig.cap="Sampling distribution of the sample mean (\\textbf{tops}) and 100 random 90\\% (\\textbf{Left}) and 80\\% (\\textbf{Right}) confidence intervals (horizontal lines) from samples of $n$=50 from the Square Lake population.  Confidence intervals that do NOT contain $\\mu$ are shown in red.">>=
par(mfrow=c(1,2),mar=c(0,0,0,0),las=1,tcl=-0.2)
set.seed(21)
layout(matrix(c(1,2,3,4),nrow=2,byrow=FALSE),heights=c(0.3,1))
n <- 50; mu <- 98.06; sigma <- 31.49; SE <- sigma/sqrt(n)
make.smplingdist(mu,SE,C=0.9)
make.CI(mu,SE,C=0.9)
make.smplingdist(mu,SE,C=0.8)
make.CI(mu,SE,C=0.8)
@

\warn{The number of confidence intervals expected to contain the parameter of interest is set by the level of confidence used to construct the confidence interval.}

\begin{minipage}{\textwidth}
One should consider the following subtleties when considering the concept of a confidence region,
\begin{Itemize}
  \item A CI is a random variable just like any other statistic. That is, each sample results in a different 95\% CI (observe the CI lines on \figref{fig:CIex100}) just like each sample results in a different $\bar{x}$ (observe the dot on each CI line of \figref{fig:CIex100}).
  \item Any one CI will either contain the parameter, $\mu$ in this case, or it will not.  However, on average, 95\% of 95\% CIs will contain the parameter of interest and 5\% will not.  That is, if we could construct all possible 95\% CIs, then 95\% of all of those CIs would contain the parameter.
  \item The 95\% CI is a technique that ``works correctly'' 95\% of the time.  In other words, 95\% of all 95\% CI ``capture'' the unknown value of the parameter.
\end{Itemize}
\end{minipage}

Because of these subtleties confidence regions are often misinterpreted by novice (and even advanced) users of statistics.\index{Confidence!Common Misinterpretations}  Some common misinterpretations are listed below with an explanation of the misinterpretation in parentheses.  These misinterpretations should be studied, compared to the interpretations discussed above, and avoided.
\begin{Enumerate}
  \item ``There is a 95\% probability that the population mean is between the endpoints of the computed confidence interval.'' [\textit{This is incorrect because the population mean is constant (not random) and it either is or is not in a particular computed interval, and it will never change whether it is or is not in that interval.  The key point is that the confidence interval is random and the parameter is not.}]
  \item ``95\% of all 95\% confidence intervals will fall between the endpoints of the computed confidence interval.'' [\textit{First, this is physically impossible at this point (i.e., using $Z^{*}$) because each confidence interval is the same width (if $n$ and the level of confidence stay constant).  Second, it is not important how many confidence intervals are contained in a confidence interval; interest is in whether the parameter is in the interval or not.}]
  \item ``There is a 95\% probability that the sample mean is between the endpoints of the computed confidence interval.`` [\textit{This is incorrect for the simple fact that confidence intervals are not used to estimate sample means (or, generally, statistics); they are used to estimate population means (or parameters).  Furthermore, the sample mean has to be exactly in the middle of the confidence interval (see next section).}]
\end{Enumerate}

\vspace{-12pt}
\warn{Care and specificity must be used when interpreting and describing confidence intervals.}

\vspace{-12pt}
\warn{Confidence intervals are constructed for parameters, not statistics.}

\begin{exsection}
  \item \label{revex:CIstat1TF} True or False -- A 95\% confidence region can be constructed for $\bar{x}$? \ansref{ans:CIstat1TF}
  \item \label{revex:CIparam1TF} True or False -- A 95\% confidence region can be constructed for the population median? \ansref{ans:CIparam1TF}
  \item \label{revex:CIparam2TF} True or False -- A 95\% confidence region can be constructed for $\sigma$? \ansref{ans:CIparam2TF}
  \item \label{revex:CIparam3YNC} Yes, No, Can't tell -- I computed the following CI: (111.12, 123.32). Is the estimated parameter in this interval? \ansref{ans:CIparam3YNC}
  \item \label{revex:CIparam4C} Make this statement correct by replacing the ``XXX'' with a word -- ``I am 99\% confident that the XXX of interest is within my confidence interval?'' \ansref{ans:CIparam4C}
\end{exsection}


\subsection{Construction} \label{sec:CIConstruct}\index{Confidence!Intervals}
As alluded to previously, not all confidence regions are designed to contain the parameter 95\% ``of the time,'' are intervals, or are computed to contain $\mu$.  Confidence regions can be constructed for any level of confidence, intervals or bounds, and for nearly all \textbf{parameters}.

The level of confidence (C) to use will be determined by the level of $\alpha$ chosen for the hypothesis test.  Specifically, the level of confidence will be $100(1-\alpha)$\%.  For example, if one sets $\alpha$ at 0.05, then the level of confidence should be 95\% or if $\alpha$ is set at 0.01, then a 99\% level of confidence should be used.  From this, one can see that if $\alpha$ is decreased such that fewer Type I errors are made, then the confidence level will increase and more of the confidence regions will contain the parameter of interest (i.e., fewer errors).  In this manner the proportion of Type I errors in the hypothesis testing framework is linked to the proportion of errors made from interpreting confidence regions.

\warn{The level of confidence (C) is determined from $\alpha$; i.e., $C=100(1-\alpha)$\%.}

The type of confidence region to be computed depends on the type of alternative hypothesis.  If the alternative hypothesis is two-tailed (i.e., $\neq$), then the confidence region will be a bounded interval.  In other words, two values will be computed such that the parameter of interest is expected, given a level of confidence, to be contained between those two values.  These are the intervals discussed previously in \sectref{sect:CIconcept}.  However, if the alternative hypothesis is one-tailed, then a so-called confidence bound is used.\index{Confidence!Bounds}  For example, if the alternative hypothesis is a ``less than'', then interest lies in determining what is the ``largest possible value'' for the parameter rather than what is the range of possible values for the parameter (as would be obtained with a confidence interval).  In other words, if the alternative hypothesis is a ``less than'', then an upper confidence bound for the parameter is constructed.  In contrast, if the alternative hypothesis is a ``greater than'', then a lower confidence bound is constructed to estimate the ``smallest possible value'' for the parameter.

\warn{A confidence interval should be constructed when a two-tailed $H_{A}$ is used.}

\warn{A confidence bound should be constructed when a one-tailed $H_{A}$ is used.  If $H_{A}$ is a ``greater than'', then the smallest possible value of the parameter is sought and a lower bound is constructed.  If $H_{A}$ is a ``less than'', then the largest possible value of the parameter is sought and an upper bound is constructed.}

Fortunately, most\footnote{All that we will see in this class.} confidence regions follow the same basic form of, \index{Margin-of-Error}

  \[ \text{``Statistic''} (\pm \text{``margin of error''}) \]

where ``Statistic'' represents whatever statistic is used to estimate the parameter and the $\pm$ sign represents either $+$, $-$, or $\pm$ (described below).  For example, $\bar{x}$ was used as the statistic in the previous example when confidence intervals were constructed to estimate $\mu$.  The margin of error generally has the form,

  \[ (\pm \text{``scaling factor''}) * SE_{statistic} \]

which makes the generic confidence interval formula,

  \[ \text{``Statistic''} (\pm \text{``scaling factor''}) * SE_{statistic} \]

The scaling factor serves a dual purpose -- controls the width and type of the confidence region.  The relative magnitude of the value controls the relative width of the region such that the parameter is contained in the region at a rate according to the level of confidence.  For example, in 99\% confidence regions the scaling factor will be set such that 99\% of the confidence regions will contain the parameter.  The actual value of the scaling factor is computed from known sampling distributions.  In the case, where $\sigma$ is known (the situation considered here), the scaling factor is computed from a $N(0,1)$ and is called a $Z^{*}$.

The sign of the scaling factor controls whether an interval, upper bound, or lower bound is computed.  For example, if the alternative hypothesis is two-tailed, then two values of $Z^{*}$ should be found such that an area equal to the level of confidence is contained between them (\figref{fig:CIboundsZ}-Left).  The two values that delineate these boundaries will be the exact same value but with different signs because the $N(0,1)$ distribution is symmetric about zero.  Thus, a confidence interval is computed with a scaling factor of $\pm Z^{*}$.

<<CIboundsZ, echo=FALSE, fig.width=6, fig.height=2, out.width='.8\\linewidth', fig.cap="Depiction of the areas that define the $z^{*}$ for creating confidence bounds of a parameter in a hypothesis test.">>=
par(mfcol=c(1,3),mar=c(2,0,2,0),las=1,tcl=-0.2)
x0<-seq(-4,4,by=0.001); norm0<-dnorm(x0,0,1)

plot(x0,norm0,type="l",xlab="",ylab="",axes=F,lwd=3)
mtext(expression(bold(H[A]:mu!=mu[0])),3)
cv1<- 1.96
xc1<-c(cv1,x0[x0>=cv1],cv1); yc1<-c(0,norm0[x0>=cv1],0)
polygon(xc1,yc1,col="red",border="red")
cv2<- -1.96
xc2<-c(cv2,x0[x0<=cv2],cv2);  yc2<-c(0,norm0[x0<=cv2],0)
polygon(xc2,yc2,col="red",border="red")
xc3<-c(cv2,x0[(x0<=cv1 & x0>=cv2)],cv1,cv2); yc3<-c(0,norm0[(x0<=cv1 & x0>=cv2)],0,0)
polygon(xc3,yc3,col="yellow",border="yellow")
lines(x0,norm0,lwd=3)  #redraw lines to cover borders
text(cv2,-0.03,expression(-z),cex=2,xpd=TRUE)
text(cv1,-0.03,expression(+z),cex=2,xpd=TRUE)
text(3.2,0.10,expression(frac(alpha,2)),col="blue",cex=1.5)
arrows(2.8,0.10,2.2,0.01,col="blue",length=0.1,lwd=2)
text(-3.2,0.10,expression(frac(alpha,2)),col="blue",cex=1.5)
arrows(-2.8,0.10,-2.2,0.01,col="blue",length=0.1,lwd=2)
text(0,0.15,expression(C),col="blue",cex=2)

plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu<mu[0])),3)
cv1<- 1.645
xc1<-c(cv1,x0[x0>=cv1],cv1); yc1<-c(0,norm0[x0>=cv1],0)
polygon(xc1,yc1,col="red",border="red")
xc3<-c(cv1,x0[x0<=cv1],cv1); yc3<-c(0,norm0[x0<=cv1],0)
polygon(xc3,yc3,col="yellow",border="yellow")
lines(x0,norm0,lwd=3)   #redraw lines to cover borders
text(cv1,-0.03,expression(+z),cex=2,xpd=TRUE)
text(2.8,0.12,expression(alpha),col="blue",cex=2)
arrows(2.8,0.10,2.2,0.01,col="blue",length=0.1,lwd=2)
text(0,0.15,expression(C),col="blue",cex=2)

plot(x0,norm0,type="l",xlab="",ylab="",axes=F,lwd=3)
mtext(expression(bold(H[A]:mu>mu[0])),3)
cv2<- -1.645
xc2<-c(cv2,x0[x0<=cv2],cv2); yc2<-c(0,norm0[x0<=cv2],0)
polygon(xc2,yc2,col="red",border="red")
xc3<-c(cv2,x0[x0>=cv2],cv2); yc3<-c(0,norm0[x0>=cv2],0)
polygon(xc3,yc3,col="yellow",border="yellow")
lines(x0,norm0,lwd=3)   #redraw lines to cover borders
text(cv2,-0.03,expression(-z),cex=2,xpd=TRUE)
text(-2.8,0.12,expression(alpha),col="blue",cex=2)
arrows(-2.8,0.10,-2.2,0.01,col="blue",length=0.1,lwd=2)
text(0,0.15,expression(C),col="blue",cex=2)
@

In contrast, if the alternative hypothesis is a ``less than'', then an upper confidence bound is desired.  In this case the $Z^{*}$ is found such that it has an area equal to the level of confidence LESS THAN it (\figref{fig:CIboundsZ}-Middle).  As the level of confidence will always be greater than 50\%, this definition will produce a positive value of $Z^{*}$ so that the scaling factor will be $+Z^{*}$.  Similarly, if the alternative hypothesis is a ``greater than'', then a lower confidence bound is desired and a value of $Z^{*}$ with an area equal to the level of confidence GREATER THAN it should be found (\figref{fig:CIboundsZ}-Right).  This will produce a negative value of $Z^{*}$ so that the scaling factor will be $-Z^{*}$.

\warn{Confidence intervals can be constructed for any level of confidence and for nearly every parameter.}

\vspace{-12pt}
\warn{When finding $Z^{*}$ for a confidence bound, the level of confidence always represents an area shaded in the same direction as the sign in $H_{A}$.}

The following are three examples for calculating confidence regions.
\begin{enumerate}
  \item For the Square Lake example, with $H_{A}:\mu < 105$ and $\alpha=0.05$, a 95\% upper confidence bound should be constructed.  The corresponding $Z^{*}=$\Sexpr{formatC(qnorm(0.95),format="f",digits=3)} is found with
<<fig.show='hide'>>=
( distrib(0.95,type="q") )
@
Thus, with the summary information for a single sample of $n=50$ shown in \tabref{tab:SquareLakeSample1}, the 95\% upper confidence bound is $100.04+1.645\frac{31.49}{\sqrt{50}}$, $100.04+7.33$, or $107.37)$.  Thus, one is 95\% confident that the true mean total length of all fish in Square Lake is less than 107.4 mm.  By confident, it is meant that 95\% of all 95\% confidence regions will contain the true $\mu$.

  \item Suppose that the mouse water consumption data from \tabref{tab:MouseData} was tested with $H_{A}:\mu \neq 10$ and $\alpha=0.01$.  In this case a 99\% confidence interval should be constructed.  The corresponding $Z^{*}=\pm$\Sexpr{formatC(qnorm(0.995),format="f",digits=3)} is found with
<<fig.show='hide'>>=
( distrib(0.995,type="q") )
@
Thus, assuming that $\sigma=$2 ml and using the summary information computed in \sectref{sec:quEDACenter} the 99\% confidence bound is $14.04\pm2.576\frac{2}{\sqrt{30}}$, $14.04\pm0.94$, or $(13.10,14.98)$.  Thus, one is 99\% confident that the true mean level of water consumption by all mice is between 13.1 and 15.0 ml.  By confident, it is meant that 99\% of all 99\% confidence regions will contain the true $\mu$.

  \item Suppose the third example hypothesis that started this chapter is being tested with an $\alpha=0.10$.  In this case a 90\% upper confidence bound should be constructed.  The corresponding $Z^{*}=+$\Sexpr{formatC(qnorm(0.90),format="f",digits=3)} is found with
<<fig.show='hide'>>=
( distrib(0.90,type="q") )
@
Thus, assuming that $\sigma=$15, $\bar{x}=$75, and $n=$40, the 90\% confidence bound is $75+1.282\frac{15}{\sqrt{40}}$, $75+3.04$, or $78.04$.  Thus, one is 90\% confident that the true mean monthly heating bill for all houses is less than \$78.04.  By confident, it is meant that 90\% of all 90\% confidence regions will contain the true $\mu$.
\end{enumerate}

\begin{exsection}
  \item \label{revex:CI99zstar} \rhw{} What is $Z^{*}$ for a 99\% confidence interval? \ansref{ans:CI99zstar}
  \item \label{revex:CI92zstar} \rhw{} What is $Z^{*}$ for a 92\% lower confidence bound? \ansref{ans:CI92zstar}
  \item \label{revex:CI90zstar} \rhw{} What is $Z^{*}$ for a 90\% upper confidence bound? \ansref{ans:CI90zstar}
  \item \label{revex:CI98zstar} \rhw{} What is $Z^{*}$ for a 98\% confidence interval? \ansref{ans:CI98zstar}
  \item \label{revex:CI95zstar} \rhw{} What is $Z^{*}$ for a 95\% lower confidence bound? \ansref{ans:CI95zstar}
  \item \label{revex:CI70zstar} \rhw{} What is $Z^{*}$ for a 70\% upper confidence bound? \ansref{ans:CI70zstar}
  \item \label{revex:CIEffluent} \rhw{} Construct and interpret (including describing what is meant by ``confidence'') a proper confidence region for the mean BOD level presented in Review Exercise \ref{revex:HypTEffluent}.  \ansref{ans:CIEffluent}
  \item \label{revex:CIMedSchool} \rhw{} Construct and interpret (including describing what is meant by ``confidence'') a proper confidence region for the mean grade point average presented in Review Exercise \ref{revex:HypTMedSchool}. \ansref{ans:CIMedSchool}
  \item \label{revex:CIBrule} \rhw{} Construct and interpret (including describing what is meant by ``confidence'') a proper confidence region if $H_{A}$ is a ``not equals'' and $\alpha$=0.05 for the population mean gage height on the Bois Brule River presented in Review Exercise \ref{revex:quEDABrule} assuming that the population standard deviation is 0.20 feet and the sampling distribution is approximately normal. \ansref{ans:CIBrule}
  \item \label{revex:CIWIc} \rhw{} Construct and interpret (including describing what is meant by ``confidence'') a proper confidence region if $H_{A}$ is a ``less than'' and $\alpha$=0.10 for the mean population density of all counties in Wisconsin using the data presented in Review Exercise \ref{revex:quEDAWIc} assuming that $\sigma=125$ people/land acre and the sampling distribution is approximately normal. \ansref{ans:CIWIc}
  \item \label{revex:CICreatPhosph} \rhw{} Construct and interpret (including describing what is meant by ``confidence'') a proper confidence region if $H_{A}$ is a ``greater than'' and $\alpha$=0.05 for the population mean creatine phosphokinase value using the data presented in Review Exercise \ref{revex:quEDACreatPhosph} assuming that $\sigma=40$. \ansref{ans:CICreatPhosph}
  \item \label{revex:CIsnow} \rhw{} \cite{Hebblewhite2000} reported the mean snow pack height (in cm) for Banff (data are below).  These data were strongly right-skewed with a possible outlier at the maximum.  Assume that it is known that $\sigma$=15 cm.  (A) Compute a 99\% confidence interval for the mean snow pack height.  (B) In addition, comment on whether or not a confidence interval should be computed for these data (note: compute the CI in (A) regardless of your answer here). \ansref{ans:CIsnow}
  \begin{Verbatim}
29.00,45.51,30.18,45.83,39.54,80.39,32.64,32.89,
46.84,45.79,62.92,67.24,30.96,46.08,33.28
  \end{Verbatim}
\end{exsection}


\section[Inference Type Relationship]{Hypothesis Tests and Confidence Region Relationship}\index{Confidence!Concept}
The concept of confidence intervals can be visualized differently.  This alternative view does not obfuscate the previous or subsequent discussions and, in fact, will strongly augment the hypothesis testing discussion of \sectref{sect:HypTest}.  This visualization, however, begins with a rather non-standard graphic where sample mean values that would be ``reasonable to see'' from a population with various possible values of $\mu$ are constructed.  The construction and utility of this graphic will be illustrated below with the Square Lake fish example.  With this example, consider that $\mu$ is unknown but that $\sigma$ is known (=31.49), that samples of $n=50$ are still used, and that 95\% CIs will be computed.

As a first step, compute the most common 95\% of sample means from a population assuming that $\mu=70$.  This is easily computed with $70 \pm 1.960\frac{31.49}{\sqrt{50}}$, $70 \pm 8.73$, or $(61.27,78.73)$.  This range is then plotted as a vertically oriented rectangle centered horizontally on $\mu=70$ (left-most rectangle on \figref{fig:CIAlt1}-Left).  Then compute and plot the same range for a slightly larger assumed value of $\mu$, say $\mu=71$ (i.e., plot $(62.27,78.73)$).  Repeat these steps for sequentially larger values of $\mu$ until a plot similar to \figref{fig:CIAlt1} is constructed.

<<CIAlt1, echo=FALSE, fig.cap="Range (95\\%) of sample means that would be produced by particular population means in the Square Lake fish length example (\\textbf{Left}) and an illustration of the ranges intercepted by $\\bar{x}=100.0$ mm (\\textbf{Right}).">>=
sigma <- 31.49; n <- 50; SE <- sigma/sqrt(n)
z <- qnorm(0.975); me <- z*SE
int <- 1; mu <- seq(70,130,int)
res <- matrix(0,nrow=length(mu),ncol=2)
for (i in 1:length(mu)) {
  res[i,1] <- mu[i]-me
  res[i,2] <- mu[i]+me
}

wid <- int/3
plot(0,0,xlim=c(min(mu)-int,max(mu)+int),ylim=range(res),xlab="Population Mean",ylab="Sample Mean")
for (i in 1:length(mu)) {
  px <- c(rep(mu[i]-wid,2),rep(mu[i]+wid,2),mu[i])
  py <- c(res[i,1],res[i,2],res[i,2],res[i,1],res[i,1])
  polygon(px,py,col="gray")
}

xbar <- 100
plot(0,0,xlim=c(min(mu)-int,max(mu)+int),ylim=range(res),xlab="Population Mean",ylab="Sample Mean")
for (i in 1:length(mu)) {
  px <- c(rep(mu[i]-wid,2),rep(mu[i]+wid,2),mu[i])
  py <- c(res[i,1],res[i,2],res[i,2],res[i,1],res[i,1])
  if ((xbar>res[i,1]) & (xbar<res[i,2])) {
    polygon(px,py,col="green")
  } else {
    polygon(px,py,col="red")
  }
  abline(h=xbar,lwd=1,col="green")
}

arrows(mu[23],res[23,2],mu[23],65,length=0.1,angle=20,col="green")
text(mu[23],67,round(mu[23],1),pos=1)
arrows(mu[39],res[39,2],mu[39],65,length=0.1,angle=20,col="green")
text(mu[39],67,round(mu[39],1),pos=1)
@

Before describing how this graphic is useful for understanding a confidence interval, consider very carefully what this graphic represents.  The vertical rectangles represent the range of the most common 95\% of sample means (values read from the y-axis) that will be produced for a particular population mean (value read from the x-axis).  In a nutshell, each vertical line represents the sample means that are likely to be observed (y-axis values) from a population with a given population mean (x-axis).

Now suppose that the sample mean of 100.04 is observed (as in \tabref{tab:SquareLakeSample1}).  Locate this value on the y-axis of \figref{fig:CIAlt1}, draw a horizontal line across the graph at this value, and draw vertical lines down from where the horizontal line first enters and then leaves the band of possible sample means (\figref{fig:CIAlt1}-Right).  The values along the x-axis that the vertical lines intercept are approximations to the 95\% confidence interval.  The approximations are only as close as the intervals used to construct the rectangles (i.e., in this example intervals of 1.0 mm were used).  However, the results from this graphical approach (i.e., $(92,108)$) compare favorably to the results from using the CI formula (i.e., $(91.27,108.73)$).

Surely, the CI formula discussed in \sectref{sec:CIConstruct} is a much quicker way to construct a 95\% confidence interval.  However, this graph illustrates a critical interpretation of confidence intervals.  The confidence interval (or region, more generally) consists of the population means which would likely produce the observed sample mean.  Thus, the values in a confidence interval represent population means that would be likely to produce the sample mean that was actually observed.  Thus, the confidence region represents possible hypothesized population means that WOULD NOT BE rejected during hypothesis testing.

\warn{The values in a confidence interval represent population means that were likely to have produced the sample mean that we actually observed.}

\section{Precision and Sample Size}\index{Sample Size!Estimation} \index{Confidence!Making narrower}\index{Precision}
The width of confidence intervals explains how precisely the parameter is estimated.  For example, relatively narrow intervals represent relatively precise estimates of the parameter.  From the general construction of confidence intervals it is seen that the width of a confidence interval is twice the margin of error.  Thus, the width of a confidence interval depends on the margin of error which, in turn, depends on (1) the standard error and (2) the scaling factor.  As either of these two items gets smaller (while holding the other constant), the width of the CI will get smaller.

\warn{The width of a confidence interval is a measure of the precision of our estimate of the parameter.}

\vspace{-12pt}
\warn{The width of a confidence interval depends on the standard error of the statistic and the scaling factor used.}

A smaller standard error means that the estimate is more precise.  More precise estimates are obtained only by increasing the sample size.\index{Confidence!Effect of n}  A smaller standard deviation would also result in a smaller SE, but for most purposes the standard deviation is constant (i.e., a population has a standard deviation, we cannot make it smaller).

\warn{Confidence intervals can be made narrower by increasing the sample size.}

A smaller scaling factor is obtained by reducing the level of confidence.\index{Confidence!Effect of C}  For example, a 90\% confidence interval uses a $Z^{*}=1.645$ whereas a 95\% confidence interval uses a $Z^{*}=1.960$ (as shown previously).  Thus, narrower CIs can be constructed by decreasing the confidence level.  However, there is a trade-off in reducing the level of confidence to make a narrower confidence interval because the number of confidence intervals not containing the parameter of interest will increase.

\warn{Confidence intervals can be made (but should not be made) narrower by decreasing the confidence level of the interval.}

The relationship between the precision of an estimate as reflected in the width of the confidence interval and the sample size provides a means for computing the same size required to estimate $\mu$ within $\pm m.e.$ units (i.e., margin-of-error) with C\% confidence assuming that $\sigma$ is known.  A formula for determining the sample size given these constraints is derived by algebraically solving for $n$ in the margin-of-error formula for the construction of a confidence interval for $\mu$, i.e.,
\[
  \begin{split}
    m.e. &= z*\frac{\sigma}{\sqrt{n}} \\
    \sqrt{n} &= \frac{z*\sigma}{m.e.} \\
    n &= \left(\frac{z*\sigma}{m.e}\right)^{2} \\
  \end{split}
\]

For example, suppose one wants to compute the sample size required to estimate the mean length of fish in Square Lake to within 5 mm with 90\% confidence knowing that the population standard deviation is 34.91.  First, define the symbols as $m.e.$=5, $\sigma$=34.91, and $Z^{*}$=1.645 (found previously for 90\% confidence).  Thus, $n = \left(\frac{1.645*34.91}{5}\right)^{2} = 131.91$.  Therefore, a sample of at least 132 fish from Square Lake should be taken.  Note that sample size calculations are always rounded up to the next integer because rounding down will produce a sample size that does not meet the desired criteria (i.e., you need at least some fraction more to meet the desired criteria).

\warn{Always round up to the next integer in sample size calculations.}

The margin-of-error and confidence level in these calculations need to come from the researcher's beliefs in how much error they can live with (i.e., chance that a confidence interval does not contain the parameter) and how precise their estimate of the mean needs to be.  Values for $\sigma$ are rarely known in practice (because it is a parameter) and estimates from preliminary studies, previous similar studies, similar populations, or wild guesses are often used instead.  In practice, a researcher will often prepare a graph with varying values of $\sigma$ \figrefp{fig:SampleSizeSigma} to make an informed decision of what sample size to choose.

<<SampleSizeSigma, echo=FALSE, fig.cap='Desired sample size versus margin-of-error for constant values of $\\sigma$ (shown to the left of each line) and $C=90$.  The desired sample size for m.e.=5, $\\sigma=35$, and $C=90$ is illustrated with the black dotted lines.'>>=
sigma <- seq(25,50,5); me <- seq(3,10,0.25); z <- qnorm(0.95)
res <- matrix(0,nrow=length(me),ncol=length(sigma))
for (i in 1:length(sigma)) res[,i] <- (z*sigma[i]/me)^2
matplot(me,res,xlab="Margin-of-Error",ylab="Sample Size",type="l",lwd=3,lty=1,xlim=c(2.5,10),yaxt="n",xaxt="n")
axis(1,seq(3,10)); axis(2,seq(0,700,100))
for (i in 1:length(sigma)) { text(2.5,res[1,i],sigma[i],col=i)  }
lines(c(5,5),c(0,res[9,3]),lty=3,lwd=2)
lines(c(5,0),c(res[9,3],res[9,3]),lty=3,lwd=2)
@

\begin{minipage}{\textwidth}
\begin{exsection}
  \item \label{revex:CIABn} If two populations have the same standard deviation and a sample of size 30 is taken from population A and a sample of size 50 from population B, which will have a narrower CI? \ansref{ans:CIABn}
  \item \label{revex:CIABs} If the same size of sample is taken from two populations, but Population C has a smaller standard deviation than Population D, which will have a narrower CI? \ansref{ans:CIABs}
  \item \label{revex:CIC} From the same data, is a 95\% or a 99\% CI narrower? \ansref{ans:CIC}
  \item \label{revex:CIdescribe} Describe how the margin of error will change as each of the following change (all others held constant): confidence level (C), z*, $n$, $\sigma$, $\mu$, and $\bar{x}$ (in the case of CIs for $\mu$).  Make sure to explain your reasoning for each. \ansref{ans:CIdescribe}
  \item \label{revex:CIPebbles} Geographers measure the longest axis of pebbles to determine ``grain'' sizes.  If the standard deviation of pebble long-axis length for a particular site is known to be 4 mm, how many pebbles must be measured in order to determine the average pebble length within 0.1 mm with 99\% confidence? \ansref{ans:CIPebbles}
  \item \label{revex:CIISP} An investment group wants to start an Internet Service Provider (ISP) and, for their business plan and model, needs to estimate the average Internet usage of households.  How many households must be randomly selected to be 95\% sure that the sample mean is within 1 minute of the population mean?  Assume that a previous survey of household usage had a standard deviation of 6.95 minutes. \ansref{ans:CIISP}
\end{exsection}
\end{minipage}

\section{11-Steps of Hypothesis Testing}\index{Hypothesis Testing!Steps}  \label{sec:11Steps}
I have created an 11-step process to make sure that you complete all aspects important to statistical hypothesis testing.  These steps are listed below and should be used for all hypothesis tests in ensuing chapters.
\begin{Enumerate}
  \item State the rejection criterion ($\alpha$),
  \item State the null and alternative hypotheses to be tested and define the parameter(s),
  \item Identify the hypothesis test to use (e.g., one-sample t, 2-sample t, etc.) and explain why it is the test of choice,
  \item Collect the data (describe how the data were collected and if randomization occurred),
  \item Check all necessary assumptions (describe how you tested the validity),
  \item Calculate the appropriate statistic(s),
  \item Calculate the appropriate test statistic,
  \item Calculate the p-value,
  \item Reject/DNR $H_{0}$,
  \item Summarize your findings in terms of the problem (do not use the word ``reject''),
  \item \textbf{If $H_{0}$ was rejected}, compute and interpret an appropriate confidence region for the parameter.
\end{Enumerate}

Two of these steps require amplifying discussion.  First, the ``collect the data'' step (step 4) should be highlighted because the most important order in these 11 steps is that steps 1-3 \textbf{MUST} be completed before collecting the data and the remaining steps are performed after collecting the data.  Second, when a null hypothesis is rejected it is implied that some difference exists between what was observed and what was expected.  Following the detection of a difference it is important to clearly articulate the direction and magnitude of that difference.  This is accomplished by computing an appropriate confidence region for the parameter (Step 11).

\warn{The $\alpha$, hypotheses, and test to use must be declared before data are collected.}

\vspace{-12pt}
\warn{Confidence regions for a parameter are an appropriate component of an hypothesis testing procedure when the null hypothesis is rejected, because the confidence region clearly articulates the direction and magnitude of the difference.}


\section{One-Sample Z-test} \label{sect:ZTest}\index{Z-test}
A one-sample Z-test is the name for the procedure developed previously in this chapter.  A one-sample Z-test tests the null hypothesis that the population mean is equal to a specific value or, symbolically, $H_{0}:\mu=\mu_{0}$ where $\mu_{0}$ represents any specific value of the population mean.  In this section, the specifics of a one-sample Z-test are summarized and, in doing so, a framework to be used for subsequent hypothesis tests is developed.  In addition, two full examples, using the 11-steps of any hypothesis test, will be completed.

\subsection{Specifics}
A one-sample Z-test is characterized by testing $H_{0}:\mu=\mu_{0}$ in the situation when $\sigma$ is known.  The only test that can possibly be confused with a one-sample Z-test is a one-sample t-test \chaprefp{chap:tTest}, which tests the same null hypothesis but in the situation where $\sigma$ is unknown.  The specifics of a one-sample Z-test are identified in \tabref{tab:1Zspec}.  The conceptual underpinnings of the one-sample Z-test were discussed in great detail in previous sections of this chapter and in \chapref{chap:SamplingDist}.

\begin{table}[htbp]
  \caption{Characteristics of a one-sample Z-test.}
  \label{tab:1Zspec}
    \begin{Itemize}
      \item \textbf{Hypothesis:} $H_{0}:\mu=\mu_{0}$
      \item \textbf{Statistic:} $\bar{x}$
      \vspace{6pt}
      \item \textbf{Test Statistic:} $z=\frac{\bar{x}-\mu_{0}}{\frac{\sigma}{\sqrt{n}}}$
      \vspace{6pt}
      \item \textbf{Confidence Region:} $\bar{x} (\pm Z^{*})\frac{\sigma}{\sqrt{n}}$
      \vspace{6pt}
      \item \textbf{Assumptions:}
        \begin{Enumerate}
          \item $\sigma$ is known
          \item $n>30$, $n>15$ and the \textbf{population} is not strongly skewed, OR the \textbf{population} is normally distributed.
        \end{Enumerate}
    \end{Itemize}
\end{table}

\subsubsection{Example - Intra-class Travel}
Consider the following situation,
\begin{quote}
\textsl{A dean is interested in the average amount of time it takes to get from one class to another.  In particular, she wants to determine if it takes more than 10 minutes, on average, to go between classes.  In an effort to test this hypothesis, she collects a random sample of 100 intra-class travel times and finds the mean to be 10.12 mins.  Assume that it is known from previous studies that the distribution of intra-class times is symmetric with a standard deviation of 1.60 minutes.  Use appropriate methods to test the dean's hypothesis with an $\alpha=0.10$.}
\end{quote}

The 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for this example are as follows:
\begin{Enumerate}
  \item As stated, $\alpha$ should be set at 0.10.
  \item The null hypothesis will be about $\mu$ (mean time for all intra-class travel events) and it will be tested against a specific value, namely $\mu_{0}=10$.  Thus, $H_{0}:\mu=10$ mins.  The $H_{A}:\mu>10$ mins (the dean is interested in seeing if the mean intra-class time is \textbf{more than} 10 mins).
  \item A one-sample Z-test is required because a quantitative variable (intra-class travel time) was measured on individuals from one population, the population mean is compared to a specific value in the null hypothesis, and $\sigma$ is known (given in the background).
  \item The data appear to be part of an observational study (the dean did not impart any conditions on the students) with a random selection of individuals.
  \item The sample size (=100) is much greater than 30, thus the test statistic computed below should reasonably follow a standard normal distribution.  In addition, $\sigma$ is known (=1.60 mins).
  \item The $\bar{x}$ is the statistic of choice because the hypothesis is about $\mu$.  From the background information, the $\bar{x}$=10.12.
  \item The z test statistic is $\frac{10.12-10}{\frac{1.60}{\sqrt{100}}} = \frac{0.12}{0.16} = 0.75$.
  \item The p-value for this statistic is \Sexpr{kPvalue(pnorm(10.12,mean=10,sd=0.16,lower.tail=FALSE))} \figrefp{fig:Ex1Zpvalue} as computed with
<<Ex1Zpvalue, par1a=TRUE, out.width='.3\\linewidth', fig.cap="Depiction of the p-value for the intra-class travel example.",fig.pos="!h">>=
( distrib(10.12,mean=10,sd=1.60/sqrt(100),lower.tail=FALSE) )
@
  \item $H_{0}$ is not rejected because the $p-value >\alpha=0.10$.
  \item It appears that the mean for \textbf{all} intra-class travel events is not more than 10 minutes.
\end{Enumerate}

\begin{exsection}
  \item \label{revex:HypTestZCactus} A researcher is investigating the growth of a certain cactus under a variety of environmental conditions.  He knows from previous research that the growth of this particular type of cactus is approximately normally distributed with a standard deviation of 1.40 cm.  Under the current environmental conditions that he is investigating, however, he does not know the mean.  He does hypothesize that it is no more than 4 cm.  To test this hypothesis he used a preliminary sample of 10 randomly-selected cacti.  He found the sample mean for these cacti to be 3.26 cm.  Use this information to test his hypothesis with $\alpha=0.05$. \ansref{ans:HypTestZCactus}
  \item \label{revex:HypTestZPike} \cite{OwensPronin2000} studied the age and growth of pike in Chivyrkui Bay on Lake Baikal.  They found that the length of the sample of 30 pike in Lake Baikal was slightly right-skewed with a mean of 656.1 mm.  Suppose that a recent article in an outdoor magazine reported the average length of all pike in this lake to be 600 mm long.  It is known from previous studies that the standard deviation of pike length is about 130 mm.  Perform a test, using a 95\% confidence level, to determine if the mean length of pike reported by the researchers significantly differs from that reported in the outdoor magazine.  \ansref{ans:HypTestZPike}
\end{exsection}

\subsection{One-Sample Z-test in R}
The p-value in a one-sample Z-test is computed from summary information using \R{distrib()} as shown in the previous discussion and example.  However, if raw data exists it is more efficient to use \R{z.test()}\footnote{From the \R{TeachingDemos} package which is loaded with \R{NCStats}.}.  This function requires a vector of the quantitative data as the first argument, the hypothesized value for $\mu$ in the \R{mu=} argument, and a value of the known $\sigma$ in the \R{sd=} argument.  In addition, the type of alternative hypothesis is declared in the \R{alt=} argument.  This argument requires a string (i.e., contained in quotes) of either \R{"two.sided"} (the default), \R{"less"}, or \R{"greater"} corresponding to the ``not equals'', ``less than'', and ``greater than'' alternatives, respectively.  Finally, a level of confidence is declared in the \R{conf.level=} argument.  This value must be a proportion (between 0 and 1) and defaults to 0.95.  You should take note of the default values for the \R{alt=} and \R{conf.level=} arguments as these are what \R{z.test()} will use if these arguments are not specifically declared by you.

The results of \R{z.test()} should be assigned to an object.  Typing the name of this object will produce output that shows, among other things, the calculated statistic ($\bar{x}$), test statistic ($Z$), p-value, and confidence region.  In addition, the saved object is submitted to \R{plot()} to produce a visual representation of the test statistic and p-value.  While the graphic from \R{plot()} does not provide any new information for the hypothesis test, it is highly recommended that you make the plot as a check of the p-value and your choice for the alternative hypothesis.

Use of \R{z.test()} and \R{plot()} are illustrated in the following example.

\subsubsection{Body Temperature}
Consider the following situation\footnote{There is an interesting discussion of studies of body temperature at \href{http://hypertextbook.com/facts/LenaWong.shtml}{The Physics Factbook}.},
\begin{quote}
\textsl{\cite{Machowiaketal1992} critically examined the belief that the mean body temperature is 98.6$^{0}$F by measuring the body temperatures in a sample of healthy humans.  Their data are found in \href{https://raw.githubusercontent.com/droglenc/NCData/master/BodyTemp.csv}{BodyTemp.csv}.  Use these data, with a supposedly known $\sigma=0.63^{0}$F, and an $\alpha=0.01$ to determine if the mean body temperature differs from 98.6$^{0}$F.}
\end{quote}

The 11-steps \sectrefp{sec:11Steps} for completing a full hypothesis test for this example are as follows:
\begin{Enumerate}
  \item As stated, $\alpha$ should be set at 0.01.
  \item The null hypothesis will be about $\mu$ and it will be tested against a specific value, namely $\mu_{0}=98.6^{0}$F.  Thus, $H_{0}:\mu=98.6^{0}$F.  The $H_{A}:\mu\neq98.6^{0}$F (the researchers want to determine if the temperature is \textbf{different from} $98.6^{0}$F).
  \item A one-sample Z-test is required because a quantitative variable (i.e., body temperature) was measured on individuals from one population, the population mean is compared to a specific value in the null hypothesis, and $\sigma$ is known (given in the background).
  \item The data appear to be part of an observational study although this is not made clear in the background information.  There is also no evidence that randomization was used.  The data were loaded into R from \dfile{BodyTemp.csv} and the results of the one-sample Z-test were computed as follows,
<<Ex2Zpvalue, par1a=TRUE, out.width='.3\\linewidth',fig.cap="Depiction of the p-value for the body temperature example.",fig.pos="!h">>=
bt <- read.csv("data/BodyTemp.csv")
headtail(bt)
( bt.z <- z.test(bt$temp,mu=98.6,sd=0.63,conf.level=0.99) )
plot(bt.z)
@
  \item The sample size (\Sexpr{round(bt.z$parameter["n"],0)}) is much greater than 30, thus the test statistic computed below should reasonably follow a standard normal distribution.  In addition, $\sigma$ is known ($=0.63^{0}$F).
  \item The hypothesis is about $\mu$.  Therefore, we want to calculate $\bar{x}$, which from the output above, is \Sexpr{round(bt.z$estimate,2)}$^{0}$F.
  \item The z test statistic is \Sexpr{round(bt.z$statistic,2)}.
  \item The p-value for this value of the test statistic is \Sexpr{kPvalue(bt.z$p.value)} \figrefp{fig:Ex2Zpvalue}.
  \item Reject $H_{0}$ because $p-value<\alpha=0.01$.
  \item It appears that the mean body temperature of all humans is different from 98.6$^{0}$F.
  \item A \textbf{99\%} confidence \textbf{interval} is warranted for this situation and is (\Sexpr{round(bt.z$conf.int[1],2)},\Sexpr{round(bt.z$conf.int[2],2)}).  Thus, one is 99\% confident that the mean body temperature ($\mu$) is actually between \Sexpr{round(bt.z$conf.int[1],2)} and \Sexpr{round(bt.z$conf.int[2],2)}$^{0}$F.
\end{Enumerate}

\begin{exsection}
  \item \label{revex:HypTestZPain} \rhw{} A study by \cite{Cheshireetal1994} reported on six patients with chronic myofascial pain syndrome (introduced in Review Exercise \ref{revex:HypTPain}).  The researchers determined the duration of pain for the six patients were 2.5, 2.7, 2.8, 2.8, 2.8, and 3.0.  Test the hypothesis that the mean pain length was greater than 2.5 years at the 10\% significance level.  Assume that it is known that the distribution of duration of pain is normal with a standard deviation of 0.5 years. \ansref{ans:HypTestZPain}

  \item \label{revex:HypTestAsianCholest} \rhw{} Suppose that it is known that cholesterol levels in women aged 21-40 in the U.S. has a mean of 190 mg/dl and standard deviation of 40 mg/dl.  Suppose that we want to determine, at the 10\% significance level, if the cholesterol level of Asian women is different from U.S. women as determined from 40 randomly selected Asian women aged 21-40 who had recently immigrated to the U.S.  Assume that the Asian women have the same standard deviation as the U.S. women population.  The data from this sample are found in \href{https://raw.githubusercontent.com/droglenc/NCData/master/Cholesterol.csv}{Cholesterol.csv}.  \ansref{ans:HypTestZAsianCholest}
\end{exsection}

\newpage
\begin{hwsection}{All questions below should be typed and answered following the expectations identified in \sectref{sect:ReportWriting}.  All work must be shown.  Questions marked with the R logo must include R output with your R commands in an attached appendix.}

  \item \label{hwprob:HypTestZHawks} \rhw{} The Duluth, MN touristry board would like to advertise that, on average, more than 50,000 raptors are seen at Hawk's Ridge\footnote{Information about Hawk Ridge is found \href{http://www.hawkridge.org/}{here}.} per year.  Data was recorded for a number of raptor species from 1971-2003 and recorded in \href{https://raw.githubusercontent.com/droglenc/NCData/master/HawksRidge.csv}{HawksRidge.csv}.  Note that the \var{Total} variable should be used from this data file as the board is focused on the total number of raptors seen in a year.  Further, assume that it is known that the population standard deviation is 37,000 raptors per year.  The board wants there to be strong, if any, evidence to support their claim (i.e., test at the 1\% level).  Use these data to determine if there is support in these data for the board's claim.

  \item \label{hwprob:HypTestZCredit} Credit card companies use a regression model (includes such factors as income, employment, credit history) to determine the credit worthiness of a prospective card holder. In the past, the companies used a threshold (cutoff) limit of 630 to receive a card.  It is also assumed that the standard deviation of all potential credit card holders is 5.  Recent information suggests that delinquencies have been increasing and it is hypothesized that credit card companies will raise the ``cutoff score'' in response.  Examine the following results concerning ``cutoff scores'' from 44 credit card issuers to see if there is evidence that the ``cutoff score'' has been increased significantly from 630.  Use $\alpha=0.10$.
  \begin{Verbatim}
  Variable  Mean   Median  StDev  Min Max   Q1    Q3
  CCards   636.86  636.50   4.42  629 647 633.25 640.00
  \end{Verbatim}

  \item \label{hwprob:CIelk} \rhw{} \cite{Hebblewhite2000} recorded the density (number per square km) of elk (\textit{Cervus elaphus}) in Banff National Park, Alberta, CA from 1986 to 2000.  The raw data from his study are shown below.  Further assume that it is known from previous studies that the standard deviation of density estimates for all years is 2 elk per square kilometer and the distribution is approximately normal.  Use this information to construct and fully interpret a test, at the 10\% significance level, of whether the mean density of elk is greater than 8 per square km.
  \begin{Verbatim}
 5.20, 7.79, 6.46, 8.60, 8.97,8.65, 9.60,9.09,
12.42,10.70,11.59,10.68,10.61,9.04,10.89
  \end{Verbatim}

  \item \label{hwprob:CInPlants} Suppose that a plant ecologist is to examine a very large tract of land that has been subdivided into 1400 plots of 10 $m^{2}$ (10 square meters).  The researcher wants to determine the mean density of plants per 10 $m^{2}$ plots for the entire tract of land to within 10 plants per 10 $m^{2}$ plot with 90\% confidence.  A pilot study indicated that the standard deviation was approximately 50 plants per 10 $m^{2}$ plot.  Determine how many 10 $m^{2}$ plots the researcher should examine to reach her stated goals.

\end{hwsection}
